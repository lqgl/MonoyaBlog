[{"categories":["Computer Science"],"content":"Golang面试问题汇总 通常我们去面试肯定会有些不错的Golang的面试题目的，所以总结下，让其他Golang开发者也可以查看到，同时也用来检测自己的能力和提醒自己的不足之处,欢迎大家补充和提交新的面试题目. Golang面试问题汇总, 这里主要分为 Golang, Mysql, Redis, Network Protocol(网络协议), Linux,以及 Algorithm 和 Structures. ","date":"2022-10-09","objectID":"/go-interview-summary/:1:0","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Golang基础 题号 题目 1 Golang中除了加Mutex锁以外还有哪些方式安全读写共享变量 2 无缓冲Chan的发送和接收是否同步 3 Golang并发机制以及它所使用的CSP并发模型 4 Golang中常用的并发模型 5 Go中对nil的Slice和空Slice的处理是一致的吗 6 协程和线程和进程的区别 7 Golang的内存模型中为什么小对象多了会造成GC压力 8 Go中数据竞争问题怎么解决 9 什么是Channel，为什么它可以做到线程安全 10 Golang垃圾回收算法 11 GC的触发条件 12 Go的GPM如何调度 13 并发编程概念是什么 14 Go语言的栈空间管理是怎么样的 15 Goroutine和Channel的作用分别是什么 16 怎么查看Goroutine的数量 17 Go中的锁有哪些 18 怎么限制Goroutine的数量 19 Channel是同步的还是异步的 20 Goroutine和线程的区别 21 Go的Struct能不能比较 22 Go的defer原理是什么 23 Go的select可以用于什么 24 Go的Context包的用途是什么 25 Go主协程如何等其余协程完再操作 26 Go的Slice如何扩容 27 Go中的map如何实现顺序读取 28 Go中CAS是怎么回事 29 Go中的逃逸分析是什么 30 Go值接收者和指针接收者的区别 31 Go的对象在内存中是怎样分配的 32 栈的内存是怎么分配的 33 堆内存管理怎么分配的 34 Go中的defer函数使用下面的两种情况下结果是什么 35 在Go函数中为什么会发生内存泄露 36 Go中new和make的区别 37 G0的作用 38 Go中的锁如何实现 39 Go中的channel的实现 40 Go中的map的实现 41 Go中的http包的实现原理 42 Goroutine发生了泄漏如何检测 43 Go函数返回局部变量的指针是否安全 44 Go中两个Nil可能不相等吗 45 Goroutine和KernelThread之间是什么关系 46 为何GPM调度要有P 47 如何在goroutine执行一半就退出协程 ","date":"2022-10-09","objectID":"/go-interview-summary/:1:1","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql基础 题号 题目 1 Mysql索引用的是什么算法 2 Mysql事务的基本要素 3 Mysql的存储引擎 4 Mysql事务隔离级别 5 Mysql高可用方案有哪些 6 Mysql中utf8和utf8mb4区别 7 Mysql中乐观锁和悲观锁区别 8 Mysql索引主要是哪些 9 Mysql联合索引最左匹配原则 10 聚簇索引和非聚簇索引区别 11 如何查询一个字段是否命中了索引 12 Mysql中查询数据什么情况下不会命中索引 13 Mysql中的MVCC是什么 14 Mvcc和Redolog和Undolog以及Binlog有什么不同 15 Mysql读写分离以及主从同步 16 InnoDB的关键特性 17 Mysql如何保证一致性和持久性 18 为什么选择B+树作为索引结构 19 InnoDB的行锁模式 20 哈希(hash)比树(tree)更快，索引结构为什么要设计成树型 21 为什么索引的key长度不能太长 22 Mysql的数据如何恢复到任意时间点 23 Mysql为什么加了索引可以加快查询 24 Explain命令有什么用 ","date":"2022-10-09","objectID":"/go-interview-summary/:1:2","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Redis基础 题号 题目 1 Redis的数据结构及使用场景 2 Redis持久化的几种方式 3 Redis的LRU具体实现 4 单线程的Redis为什么快 5 Redis的数据过期策略 6 如何解决Redis缓存雪崩问题 7 如何解决Redis缓存穿透问题 8 Redis并发竞争key如何解决 9 Redis的主从模式和哨兵模式和集群模式区别 10 Redis有序集合zset底层怎么实现的 11 跳表的查询过程是怎么样的，查询和插入的时间复杂度 12 redis如何分片 ","date":"2022-10-09","objectID":"/go-interview-summary/:1:3","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"网络协议基础 题号 题目 1 TCP和UDP有什么区别 2 TCP中三次握手和四次挥手 3 TCP的LISTEN状态是什么 4 常见的HTTP状态码有哪些 5 301和302有什么区别 6 504和500有什么区别 7 HTTPS和HTTP有什么区别 8 Quic有什么优点相比Http2 9 Grpc的优缺点 10 Get和Post区别 11 Unicode和ASCII以及Utf8的区别 12 Cookie与Session异同 13 Client如何实现长连接 14 Http1和Http2和Grpc之间的区别是什么 15 Tcp中的拆包和粘包是怎么回事 16 TFO的原理是什么 17 TIME_WAIT的作用 18 网络的性能指标有哪些 ","date":"2022-10-09","objectID":"/go-interview-summary/:1:4","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Linux基础 题号 题目 1 异步和非阻塞的区别 2 虚拟内存作用是什么 3 Linux查看端口占用和cpu负载 4 Linux如何发送信号给一个进程 5 如何避免死锁 6 孤儿进程和僵尸进程区别 7 滑动窗口的概念以及应用 8 Epoll和Select的区别 9 进程之间为什么要进行通信呢 10 输入PingIP后敲回车,发包前会发生什么 11 进程和进程间的通信方式区别和不同 12 如何查看二进制可执行文件引用的动态链接库 ","date":"2022-10-09","objectID":"/go-interview-summary/:1:5","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Algorithm和Structures 题号 题目 1 哪些排序算法是稳定的 2 给定一个二叉树,判断其是否是一个有效的二叉搜索树 3 排序算法 4 如何通过递归反转单链表 5 链表和数组相比有什么优缺点 6 通常一般会用到哪些数据结构 云原生 题号 题目 1 prometheus架构中有哪些组件以及各个组件有什么用 ","date":"2022-10-09","objectID":"/go-interview-summary/:1:6","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"其他基础 题号 题目 1 中间件原理 2 Hash冲突有什么解决办法 3 微服务架构是什么样子的 4 分布式锁实现 5 负载均衡原理是什么 6 互斥锁和读写锁和死锁问题是怎么解决 7 Etcd中的Raft一致性算法原理 8 Git的merge跟rebase的区别 9 如何对一个20GB的文件进行排序 10 LVS原理是什么 11 为什么需要消息队列 12 高并发系统的设计与实现 13 Kafka的文件存储机制 14 Kafka如何保证可靠性 15 Kafka是如何实现高吞吐率的 16 分布式事务有哪几种 ","date":"2022-10-09","objectID":"/go-interview-summary/:1:7","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Golang基础知识 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:0","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Golang中除了加Mutex锁以外还有哪些方式安全读写共享变量 Golang中Goroutine 可以通过 Channel 进行安全读写共享变量,还可以通过原子性操作进行. ","date":"2022-10-09","objectID":"/go-interview-summary/:2:1","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"无缓冲Chan的发送和接收是否同步 ch := make(chan int) 无缓冲的channel由于没有缓冲发送和接收需要同步. ch := make(chan int, 2) 有缓冲channel不要求发送和接收操作同步. channel无缓冲时,无缓冲chan是指在接收前没有能力保存任何值得通道。 这种类型的通道要求发送goroutine和接收goroutine同时准备好，才能完成发送和接收操作。如果两个goroutine没有同时准备好，通道会导致先执行发送或接收操作的goroutine阻塞等待。 channel有缓冲时,当缓冲满时发送阻塞，当缓冲空时接收阻塞。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:2","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Golang并发机制以及它所使用的CSP并发模型． 在计算机科学中，通信顺序过程（communicating sequential processes，CSP）是一种描述并发系统中交互模式的正式语言，它是并发数学理论家族中的一个成员，被称为过程算法（process algebras），或者说过程计算（process calculate），是基于消息的通道传递的数学理论。 CSP模型是上个世纪七十年代提出的,不同于传统的多线程通过共享内存来通信，CSP讲究的是“以通信的方式来共享内存”。用于描述两个独立的并发实体通过共享的通讯 channel(管道)进行通信的并发模型。 CSP中channel是第一类对象，它不关注发送消息的实体，而关注与发送消息时使用的channel。 Golang中 channel是被单独创建并且可以在进程之间传递，它的通信模式类似于boss-worker模式的，一个实体通过将消息发送到channel 中，然后又监听这个 channel 的实体处理，两个实体之间是匿名的，这个就实现实体中间的解耦，其中 channel 是同步的一个消息被发送到 channel 中，最终是一定要被另外的实体消费掉的，在实现原理上其实类似一个阻塞的消息队列。 Goroutine 是Golang实际并发执行的实体，它底层是使用协程(coroutine)实现并发，coroutine是一种运行在用户态的用户线程，类似于green thread，go底层选择使用coroutine的出发点是因为， 它具有以下特点: 用户空间 避免了内核态和用户态的切换导致的成本. 可以由语言和框架层进行调度. 更小的栈空间允许创建大量的实例. Golang中的Goroutine的特性: Golang内部有三个对象: P对象(processor) 代表上下文（或者可以认为是cpu），M(work thread)代表工作线程，G对象(goroutine). 正常情况下一个CPU对象启一个工作线程对象，线程去检查并执行goroutine对象。碰到goroutine对象阻塞的时候，会启动一个新的工作线程，以充分利用cpu资源。所以有时候线程对象会比处理器对象多很多. 我们用如下图分别表示P、M、G: G（Goroutine）: 我们所说的协程，为用户级的轻量级线程，每个Goroutine对象中的sched保存着其上下文信息。 M（Machine）: 对OS内核级线程的封装，数量对应真实的CPU数(真正干活的对象). P (Processor): 逻辑处理器,即为G和M的调度对象，用来调度G和M之间的关联关系，其数量可通过 GOMAXPROCS()来设置，默认为核心数。 在单核情况下，所有Goroutine运行在同一个线程（M0）中，每一个线程维护一个上下文（P），任何时刻，一个上下文中只有一个Goroutine，其他Goroutine在runqueue中等待。 一个Goroutine运行完自己的时间片后，让出上下文，自己回到runqueue中（如下图所示）。 当正在运行的G0阻塞的时候（可以需要IO），会再创建一个线程（M1），P转到新的线程中去运行。 当M0返回时，它会尝试从其他线程中“偷”一个上下文过来，如果没有偷到，会把Goroutine放到Global runqueue中去，然后把自己放入线程缓存中。 上下文会定时检查Global runqueue。 Golang是为并发而生的语言，Go语言是为数不多的在语言层面实现并发的语言；也正是Go语言的并发特性，吸引了全球无数的开发者。 Golang的CSP并发模型，是通过Goroutine和Channel来实现的。 Goroutine 是Go语言中并发的执行单位。有点抽象，其实就是和传统概念上的”线程“类似，可以理解为”线程“。Channel是Go语言中各个并发结构体(Goroutine)之前的通信机制。通常Channel，是各个Goroutine之间通信的”管道“，有点类似于Linux中的管道。 通信机制channel也很方便，传数据用channel \u003c- data，取数据用 \u003c-channel。 在通信过程中，传数据 channel \u003c- data 和取数据 \u003c-channel 必然会成对出现，因为这边传，那边取，两个goroutine之间才会实现通信。而且不管是传还是取，肯定阻塞，直到另外的goroutine传或者取为止。因此GPM的简要概括即为:事件循环,线程池,工作队列。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:3","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Golang中常用的并发模型 Golang中常用的并发模型有三种: 通过channel通知实现并发控制 无缓冲的通道指的是通道的大小为0，也就是说，这种类型的通道在接收前没有能力保存任何值，它要求发送 goroutine 和接收 goroutine 同时准备好，才可以完成发送和接收操作。 从上面无缓冲的通道定义来看，发送 goroutine 和接收 goroutine 必须是同步的，同时准备后，如果没有同时准备好的话，先执行的操作就会阻塞等待，直到另一个相对应的操作准备好为止。这种无缓冲的通道我们也称之为同步通道。 func main() { ch := make(chan struct{}) go func() { fmt.Println(\"start working\") time.Sleep(time.Second * 1) ch \u003c- struct{}{} }() \u003c-ch fmt.Println(\"finished\") } 当主 goroutine 运行到 \u003c-ch 接受 channel 的值的时候，如果该 channel 中没有数据，就会一直阻塞等待，直到有值。 这样就可以简单实现并发控制 通过sync包中的WaitGroup实现并发控制 Goroutine是异步执行的，有的时候为了防止在结束main函数的时候结束掉Goroutine，所以需要同步等待，这个时候就需要用 WaitGroup了，在Sync包中，提供了 WaitGroup,它会等待它收集的所有 goroutine 任务全部完成。 在WaitGroup里主要有三个方法: Add, 可以添加或减少 goroutine的数量. Done, 相当于Add(-1). Wait, 执行后会堵塞主线程，直到WaitGroup 里的值减至0. 在主goroutine 中 Add(delta int) 索要等待goroutine 的数量。在每一个goroutine 完成后Done()表示这一个goroutine 已经完成，当所有的 goroutine 都完成后，在主 goroutine 中 WaitGroup 返回。 func main(){ var wg sync.WaitGroup var urls = []string{ \"http://www.golang.org/\", \"http://www.google.com/\", } for _, url := range urls { wg.Add(1) go func(url string) { defer wg.Done() http.Get(url) }(url) } wg.Wait() } 在Golang官网中对于WaitGroup介绍是A WaitGroup must not be copied after first use,在 WaitGroup 第一次使用后，不能被拷贝。 应用示例: func main(){ wg := sync.WaitGroup{} for i := 0; i \u003c 5; i++ { wg.Add(1) go func(wg sync.WaitGroup, i int) { fmt.Printf(\"i:%d\", i) wg.Done() }(wg, i) } wg.Wait() fmt.Println(\"exit\") } 运行: i:1i:3i:2i:0i:4fatal error: all goroutines are asleep - deadlock! goroutine 1 [semacquire]: sync.runtime_Semacquire(0xc000094018) /home/keke/soft/go/src/runtime/sema.go:56 +0x39 sync.(*WaitGroup).Wait(0xc000094010) /home/keke/soft/go/src/sync/waitgroup.go:130 +0x64 main.main() /home/keke/go/Test/wait.go:17 +0xab exit status 2 它提示所有的 goroutine 都已经睡眠了，出现了死锁。这是因为 wg 给拷贝传递到了 goroutine 中，导致只有 Add 操作，其实 Done操作是在 wg 的副本执行的。 因此 Wait 就会死锁。 这个第一个修改方式: 将匿名函数中 wg 的传入类型改为 *sync.WaitGroup,这样就能引用到正确的WaitGroup了。 这个第二个修改方式: 将匿名函数中的 wg 的传入参数去掉，因为Go支持闭包类型，在匿名函数中可以直接使用外面的 wg 变量. 在Go 1.7以后引进的强大的Context上下文，实现并发控制. 通常,在一些简单场景下使用 channel 和 WaitGroup 已经足够了，但是当面临一些复杂多变的网络并发场景下 channel 和 WaitGroup 显得有些力不从心了。 比如一个网络请求 Request，每个 Request 都需要开启一个 goroutine 做一些事情，这些 goroutine 又可能会开启其他的 goroutine，比如数据库和RPC服务。 所以我们需要一种可以跟踪 goroutine 的方案，才可以达到控制他们的目的，这就是Go语言为我们提供的 Context，称之为上下文非常贴切，它就是goroutine 的上下文。 它是包括一个程序的运行环境、现场和快照等。每个程序要运行时，都需要知道当前程序的运行状态，通常Go 将这些封装在一个 Context 里，再将它传给要执行的 goroutine 。 context 包主要是用来处理多个 goroutine 之间共享数据，及多个 goroutine 的管理。 context 包的核心是 struct Context，接口声明如下： // A Context carries a deadline, cancelation signal, and request-scoped values // across API boundaries. Its methods are safe for simultaneous use by multiple // goroutines. type Context interface { // Done returns a channel that is closed when this `Context` is canceled // or times out. // Done() 返回一个只能接受数据的channel类型，当该context关闭或者超时时间到了的时候，该channel就会有一个取消信号 Done() \u003c-chan struct{} // Err indicates why this Context was canceled, after the Done channel // is closed. // Err() 在Done() 之后，返回context 取消的原因。 Err() error // Deadline returns the time when this Context will be canceled, if any. // Deadline() 设置该context cancel的时间点 Deadline() (deadline time.Time, ok bool) // Value returns the value associated with key or nil if none. // Value() 方法允许 Context 对象携带request作用域的数据，该数据必须是线程安全的。 Value(key interface{}) interface{} } Context 对象是线程安全的，你可以把一个 Context 对象传递给任意个数的 gorotuine，对它执行取消操作时，所有 goroutine 都会接收到取消信号。 一个 Context 不能拥有 Cancel 方法，同时我们也只能 Done channel 接收数据。其中的原因是一致的：接收取消信号的函数和发送信号的函数通常不是一个。 典型的场景是：父操作为子操作操作启动 goroutine，子操作也就不能取消父操作。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:4","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中对nil的Slice和空Slice的处理是一致的吗 首先Go的JSON 标准库对 nil slice 和 空 slice 的处理是不一致. 通常错误的用法，会报数组越界的错误，因为只是声明了slice，却没有给实例化的对象。 var slice []int slice[1] = 0 此时slice的值是nil，这种情况可以用于需要返回slice的函数，当函数出现异常的时候，保证函数依然会有nil的返回值。 empty slice 是指slice不为nil，但是slice没有值，slice的底层的空间是空的，此时的定义如下： slice := make([]int,0) slice := []int{} 当我们查询或者处理一个空的列表的时候，这非常有用，它会告诉我们返回的是一个列表，但是列表内没有任何值。 总之，nil slice 和 empty slice是不同的东西,需要我们加以区分的. ","date":"2022-10-09","objectID":"/go-interview-summary/:2:5","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"协程和线程和进程的区别 进程 进程是程序的一次执行过程，是程序在执行过程中的分配和管理资源的基本单位，每个进程都有自己的地址空间,进程是系统进行资源分配和调度的一个独立单位。 每个进程都有自己的独立内存空间，不同进程通过IPC（Inter-Process Communication）进程间通信来通信。由于进程比较重量，占据独立的内存，所以上下文进程间的切换开销（栈、寄存器、虚拟内存、文件句柄等）比较大，但相对比较稳定安全。 线程 线程是进程的一个实体,线程是内核态,而且是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位.线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器,一组寄存器和栈),但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。 线程间通信主要通过共享内存，上下文切换很快，资源开销较少，但相比进程不够稳定容易丢失数据。 协程 协程是一种用户态的轻量级线程，协程的调度完全由用户控制。协程拥有自己的寄存器上下文和栈。 协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:6","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Golang的内存模型中为什么小对象多了会造成GC压力 通常小对象过多会导致GC三色法消耗过多的CPU。优化思路是，减少对象分配. ","date":"2022-10-09","objectID":"/go-interview-summary/:2:7","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中数据竞争问题怎么解决 Data Race问题可以使用互斥锁sync.Mutex, 或者也可以通过CAS无锁并发解决.其中使用同步访问共享数据或者CAS无锁并发是处理数据竞争的一种有效的方法. golang在1.1之后引入了竞争检测机制，可以使用 go run -race 或者 go build -race来进行静态检测。 其在内部的实现是,开启多个协程执行同一个命令， 并且记录下每个变量的状态. 竞争检测器基于C/C++的ThreadSanitizer运行时库，该库在Google内部代码基地和Chromium找到许多错误。这个技术在2012年九月集成到Go中，从那时开始，它已经在标准库中检测到42个竞争条件。现在，它已经是我们持续构建过程的一部分，当竞争条件出现时，它会继续捕捉到这些错误。 竞争检测器已经完全集成到Go工具链中，仅仅添加-race标志到命令行就使用了检测器。 $ go test -race mypkg // 测试包 $ go run -race mysrc.go // 编译和运行程序 $ go build -race mycmd // 构建程序 $ go install -race mypkg // 安装程序 要想解决数据竞争的问题可以使用互斥锁sync.Mutex,解决数据竞争(Data race),也可以使用管道解决,使用管道的效率要比互斥锁高. ","date":"2022-10-09","objectID":"/go-interview-summary/:2:8","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"什么是Channel，为什么它可以做到线程安全 Channel是Go中的一个核心类型，可以把它看成一个管道，通过它并发核心单元就可以发送或者接收数据进行通讯(communication),Channel也可以理解是一个先进先出的队列，通过管道进行通信。 Golang的Channel, 发送一个数据到Channel和从Channel接收一个数据都是原子性的。 Go的设计思想就是, 不要通过共享内存来通信，而是通过通信来共享内存，前者就是传统的加锁，后者就是Channel。也就是说，设计Channel的主要目的就是在多任务间传递数据的，本身就是安全的。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:9","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Golang垃圾回收算法 首先我们先来了解下什么是垃圾回收。 什么是垃圾回收？ 内存管理是程序员开发应用的一大难题。传统的系统级编程语言（主要指C/C++）中，程序开发者必须对内存小心的进行管理操作，控制内存的申请及释放。因为稍有不慎，就可能产生内存泄露问题，这种问题不易发现并且难以定位，一直成为困扰程序开发者的噩梦。 如何解决这个头疼的问题呢？ 过去一般采用两种办法: 内存泄露检测工具。这种工具的原理一般是静态代码扫描，通过扫描程序检测可能出现内存泄露的代码段。然而检测工具难免有疏漏和不足，只能起到辅助作用。 智能指针。这是 c++ 中引入的自动内存管理方法，通过拥有自动内存管理功能的指针对象来引用对象，是程序员不用太关注内存的释放，而达到内存自动释放的目的。这种方法是采用最广泛的做法，但是对程序开发者有一定的学习成本（并非语言层面的原生支持），而且一旦有忘记使用的场景依然无法避免内存泄露。 为了解决这个问题，后来开发出来的几乎所有新语言（java，python，php等等）都引入了语言层面的自动内存管理 – 也就是语言的使用者只用关注内存的申请而不必关心内存的释放，内存释放由虚拟机（virtual machine）或运行时（runtime）来自动进行管理。而这种对不再使用的内存资源进行自动回收的行为就被称为垃圾回收。 常用的垃圾回收的方法: 引用计数（reference counting） 这是最简单的一种垃圾回收算法，和之前提到的智能指针异曲同工。对每个对象维护一个引用计数，当引用该对象的对象被销毁或更新时被引用对象的引用计数自动减一，当被引用对象被创建或被赋值给其他对象时引用计数自动加一。当引用计数为0时则立即回收对象。 这种方法的优点是实现简单，并且内存的回收很及时。这种算法在内存比较紧张和实时性比较高的系统中使用的比较广泛，如ios cocoa框架，php，python等。 但是简单引用计数算法也有明显的缺点： 频繁更新引用计数降低了性能。 一种简单的解决方法就是编译器将相邻的引用计数更新操作合并到一次更新；还有一种方法是针对频繁发生的临时变量引用不进行计数，而是在引用达到0时通过扫描堆栈确认是否还有临时对象引用而决定是否释放。等等还有很多其他方法，具体可以参考这里。 循环引用。 当对象间发生循环引用时引用链中的对象都无法得到释放。最明显的解决办法是避免产生循环引用，如cocoa引入了strong指针和weak指针两种指针类型。或者系统检测循环引用并主动打破循环链。当然这也增加了垃圾回收的复杂度。 标记-清除（mark and sweep） 标记-清除（mark and sweep）分为两步，标记从根变量开始迭代得遍历所有被引用的对象，对能够通过应用遍历访问到的对象都进行标记为“被引用”；标记完成后进行清除操作，对没有标记过的内存进行回收（回收同时可能伴有碎片整理操作）。 这种方法解决了引用计数的不足，但是也有比较明显的问题：每次启动垃圾回收都会暂停当前所有的正常代码执行，回收时,系统响应能力大大降低！当然后续也出现了很多mark\u0026sweep算法的变种（如三色标记法）优化了这个问题。 分代搜集（generation） java的jvm 就使用的分代回收的思路。在面向对象编程语言中，绝大多数对象的生命周期都非常短。分代收集的基本思想是，将堆划分为两个或多个称为代（generation）的空间。 新创建的对象存放在称为新生代（young generation）中（一般来说，新生代的大小会比 老年代小很多），随着垃圾回收的重复执行，生命周期较长的对象会被提升（promotion）到老年代中（这里用到了一个分类的思路，这个是也是科学思考的一个基本思路）。 因此，新生代垃圾回收和老年代垃圾回收两种不同的垃圾回收方式应运而生，分别用于对各自空间中的对象执行垃圾回收。新生代垃圾回收的速度非常快，比老年代快几个数量级，即使新生代垃圾回收的频率更高，执行效率也仍然比老年代垃圾回收强，这是因为大多数对象的生命周期都很短，根本无需提升到老年代。 Golang GC 时会发生什么? Golang 1.5后，采取的是“非分代的、非移动的、并发的、三色的”标记清除垃圾回收算法。 golang 中的 gc 基本上是标记清除的过程： golang 的垃圾回收是基于标记清扫算法，这种算法需要进行 STW（stop the world)，这个过程就会导致程序是卡顿的，频繁的 GC 会严重影响程序性能. golang 在此基础上进行了改进，通过三色标记清扫法与写屏障来减少 STW 的时间. gc的过程一共分为四个阶段： 栈扫描（开始时STW），所有对象最开始都是白色. 从 root开始找到所有可达对象（所有可以找到的对象)，标记为灰色，放入待处理队列。 遍历灰色对象队列，将其引用对象标记为灰色放入待处理队列，自身标记为黑色。 清除（并发） 循环步骤3直到灰色队列为空为止，此时所有引用对象都被标记为黑色，所有不可达的对象依然为白色，白色的就是需要进行回收的对象。 三色标记法相对于普通标记清扫，减少了 STW 时间. 这主要得益于标记过程是 “on-the-fly” 的，在标记过程中是不需要 STW 的，它与程序是并发执行的，这就大大缩短了STW的时间. Golang gc 优化的核心就是尽量使得 STW(Stop The World) 的时间越来越短。 详细的Golang的GC介绍可以参看Golang垃圾回收. 写屏障: 当标记和程序是并发执行的，这就会造成一个问题. 在标记过程中，有新的引用产生，可能会导致误清扫. 清扫开始前，标记为黑色的对象引用了一个新申请的对象，它肯定是白色的，而黑色对象不会被再次扫描，那么这个白色对象无法被扫描变成灰色、黑色，它就会最终被清扫，而实际它不应该被清扫. 这就需要用到屏障技术，golang采用了写屏障，其作用就是为了避免这类误清扫问题. 写屏障即在内存写操作前，维护一个约束，从而确保清扫开始前，黑色的对象不能引用白色对象. ","date":"2022-10-09","objectID":"/go-interview-summary/:2:10","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"GC的触发条件 Go中对 GC 的触发时机存在两种形式： 主动触发(手动触发)，通过调用runtime.GC 来触发GC，此调用阻塞式地等待当前GC运行完毕. 被动触发，分为两种方式： a. 使用系统监控，当超过两分钟没有产生任何GC时，强制触发 GC. b. 使用步调（Pacing）算法，其核心思想是控制内存增长的比例,当前内存分配达到一定比例则触发. ","date":"2022-10-09","objectID":"/go-interview-summary/:2:11","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go的GPM如何调度 Goroutine协程: 协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈。 因此，协程能保留上一次调用时的状态（即所有局部状态的一个特定组合），每次过程重入时，就相当于进入上一次调用的状态，换种说法：进入上一次离开时所处逻辑流的位置。 线程和进程的操作是由程序触发系统接口，最后的执行者是系统；协程的操作执行者则是用户自身程序，goroutine也是协程。 goroutine能拥有强大的并发实现是通过GPM调度模型实现. Go的调度器内部有四个重要的结构：M，P，S，Sched，如上图所示（Sched未给出）. M: M代表内核级线程，一个M就是一个线程，goroutine就是跑在M之上的；M是一个很大的结构，里面维护小对象内存cache（mcache）、当前执行的goroutine、随机数发生器等等非常多的信息. G: 代表一个goroutine，它有自己的栈，instruction pointer和其他信息（正在等待的channel等等），用于调度. P: P全称是Processor，逻辑处理器，它的主要用途就是用来执行goroutine的，所以它也维护了一个goroutine队列，里面存储了所有需要它来执行的goroutine. Sched：代表调度器，它维护有存储M和G的队列以及调度器的一些状态信息等. Go中的GPM调度: 新创建的G 会先保存在 P 的本地队列中，如果 P 的本地队列已经满了就会保存在全局的队列中，最终等待被逻辑处理器P执行即可。 在M与P绑定后，M会不断从P的Local队列中无锁地取出G，并切换到G的堆栈执行，当P的Local队列中没有G时，再从Global队列中获取一个G，当Global队列中也没有待运行的G时，则尝试从其它的P窃取部分G来执行相当于P之间的负载均衡。 从上图中可以看到，有2个物理线程M，每一个M都拥有一个处理器P，每一个也都有一个正在运行的goroutine。P的数量可以通过GOMAXPROCS()来设置，它其实也就代表了真正的并发度，即有多少个goroutine可以同时运行。 图中灰色的那些goroutine并没有运行，而是出于ready的就绪态，正在等待被调度。P维护着这个队列（称之为runqueue），Go语言里，启动一个goroutine很容易：go function 就行，所以每有一个go语句被执行，runqueue队列就在其末尾加入一个goroutine，在下一个调度点，就从runqueue中取出（如何决定取哪个goroutine？）一个goroutine执行。 当一个OS线程M0陷入阻塞时，P转而在运行M1，图中的M1可能是正被创建，或者从线程缓存中取出。 当M0返回时，它必须尝试取得一个P来运行goroutine，一般情况下，它会从其他的OS线程那里拿一个P过来，如果没有拿到的话，它就把goroutine放在一个global runqueue里，然后自己睡眠（放入线程缓存里）。所有的P也会周期性的检查global runqueue并运行其中的goroutine，否则global runqueue上的goroutine永远无法执行。 另一种情况是P所分配的任务G很快就执行完了（分配不均），这就导致了这个处理器P处于空闲的状态，但是此时其他的P还有任务，此时如果global runqueue没有任务G了，那么这个P就会从其他的P里偷取一些G来执行。 通常来说，如果P从其他的P那里要拿任务的话，一般就拿run queue的一半，这就确保了每个OS线程都能充分的使用。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:12","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"并发编程概念是什么 并行是指两个或者多个事件在同一时刻发生；并发是指两个或多个事件在同一时间间隔发生。 并行是在不同实体上的多个事件，并发是在同一实体上的多个事件。在一台处理器上“同时”处理多个任务，在多台处理器上同时处理多个任务。如hadoop分布式集群 并发偏重于多个任务交替执行，而多个任务之间有可能还是串行的。而并行是真正意义上的“同时执行”。 并发编程是指在一台处理器上“同时”处理多个任务。并发是在同一实体上的多个事件。多个事件在同一时间间隔发生。并发编程的目标是充分的利用处理器的每一个核，以达到最高的处理性能。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:13","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go语言的栈空间管理是怎么样的 Go语言的运行环境（runtime）会在goroutine需要的时候动态地分配栈空间，而不是给每个goroutine分配固定大小的内存空间。这样就避免了需要程序员来决定栈的大小。 分块式的栈是最初Go语言组织栈的方式。当创建一个goroutine的时候，它会分配一个8KB的内存空间来给goroutine的栈使用。我们可能会考虑当这8KB的栈空间被用完的时候该怎么办? 为了处理这种情况，每个Go函数的开头都有一小段检测代码。这段代码会检查我们是否已经用完了分配的栈空间。如果是的话，它会调用morestack函数。morestack函数分配一块新的内存作为栈空间，并且在这块栈空间的底部填入各种信息（包括之前的那块栈地址）。在分配了这块新的栈空间之后，它会重试刚才造成栈空间不足的函数。这个过程叫做栈分裂（stack split）。 在新分配的栈底部，还插入了一个叫做lessstack的函数指针。这个函数还没有被调用。这样设置是为了从刚才造成栈空间不足的那个函数返回时做准备的。当我们从那个函数返回时，它会跳转到lessstack。lessstack函数会查看在栈底部存放的数据结构里的信息，然后调整栈指针（stack pointer）。这样就完成了从新的栈块到老的栈块的跳转。接下来，新分配的这个块栈空间就可以被释放掉了。 分块式的栈让我们能够按照需求来扩展和收缩栈的大小。 Go开发者不需要花精力去估计goroutine会用到多大的栈。创建一个新的goroutine的开销也不大。当 Go开发者不知道栈会扩展到多少大时，它也能很好的处理这种情况。 这一直是之前Go语言管理栈的的方法。但这个方法有一个问题。缩减栈空间是一个开销相对较大的操作。如果在一个循环里有栈分裂，那么它的开销就变得不可忽略了。一个函数会扩展，然后分裂栈。当它返回的时候又会释放之前分配的内存块。如果这些都发生在一个循环里的话，代价是相当大的。 这就是所谓的热分裂问题（hot split problem）。它是Go语言开发者选择新的栈管理方法的主要原因。新的方法叫做栈复制法（stack copying）。 栈复制法一开始和分块式的栈很像。当goroutine运行并用完栈空间的时候，与之前的方法一样，栈溢出检查会被触发。但是，不像之前的方法那样分配一个新的内存块并链接到老的栈内存块，新的方法会分配一个两倍大的内存块并把老的内存块内容复制到新的内存块里。这样做意味着当栈缩减回之前大小时，我们不需要做任何事情。栈的缩减没有任何代价。而且，当栈再次扩展时，运行环境也不需要再做任何事。它可以重用之前分配的空间。 栈的复制听起来很容易，但实际操作并非那么简单。存储在栈上的变量的地址可能已经被使用到。也就是说程序使用到了一些指向栈的指针。当移动栈的时候，所有指向栈里内容的指针都会变得无效。然而，指向栈内容的指针自身也必定是保存在栈上的。这是为了保证内存安全的必要条件。否则一个程序就有可能访问一段已经无效的栈空间了。 因为垃圾回收的需要，我们必须知道栈的哪些部分是被用作指针了。当我们移动栈的时候，我们可以更新栈里的指针让它们指向新的地址。所有相关的指针都会被更新。我们使用了垃圾回收的信息来复制栈，但并不是任何使用栈的函数都有这些信息。因为很大一部分运行环境是用C语言写的，很多被调用的运行环境里的函数并没有指针的信息，所以也就不能够被复制了。当遇到这种情况时，我们只能退回到分块式的栈并支付相应的开销。 这也是为什么现在运行环境的开发者正在用Go语言重写运行环境的大部分代码。无法用Go语言重写的部分（比如调度器的核心代码和垃圾回收器）会在特殊的栈上运行。这个特殊栈的大小由运行环境的开发者设置。 这些改变除了使栈复制成为可能，它也允许我们在将来实现并行垃圾回收。 另外一种不同的栈处理方式就是在虚拟内存中分配大内存段。由于物理内存只是在真正使用时才会被分配，因此看起来好似你可以分配一个大内存段并让操 作系统处理它。下面是这种方法的一些问题 首先，32位系统只能支持4G字节虚拟内存，并且应用只能用到其中的3G空间。由于同时运行百万goroutines的情况并不少见，因此你很可 能用光虚拟内存，即便我们假设每个goroutine的stack只有8K。 第二，然而我们可以在64位系统中分配大内存，它依赖于过量内存使用。所谓过量使用是指当你分配的内存大小超出物理内存大小时，依赖操作系统保证 在需要时能够分配出物理内存。然而，允许过量使用可能会导致一些风险。由于一些进程分配了超出机器物理内存大小的内存，如果这些进程使用更多内存 时，操作系统将不得不为它们补充分配内存。这会导致操作系统将一些内存段放入磁盘缓存，这常常会增加不可预测的处理延迟。正是考虑到这个原因，一 些新系统关闭了对过量使用的支持。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:14","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Goroutine和Channel的作用分别是什么 进程是内存资源管理和cpu调度的执行单元。为了有效利用多核处理器的优势，将进程进一步细分，允许一个进程里存在多个线程，这多个线程还是共享同一片内存空间，但cpu调度的最小单元变成了线程。 那协程又是什么呢，以及与线程的差异性? 协程，可以看作是轻量级的线程。但与线程不同的是，线程的切换是由操作系统控制的，而协程的切换则是由用户控制的。 最早支持协程的程序语言应该是lisp方言scheme里的continuation（续延）, 续延允许scheme保存任意函数调用的现场，保存起来并重新执行。Lua,C#,python等语言也有自己的协程实现。 Go中的goroutine就是协程,可以实现并行，多个协程可以在多个处理器同时跑。而协程同一时刻只能在一个处理器上跑（可以把宿主语言想象成单线程的就好了）。 然而,多个goroutine之间的通信是通过channel，而协程的通信是通过yield和resume()操作。 goroutine非常简单，只需要在函数的调用前面加关键字go即可，例如: go elegance() 我们也可以启动5个goroutines分别打印索引: func main() { for i:=1;i\u003c5;i++ { go func(i int) { fmt.Println(i) }(i) } // 停歇5s，保证打印全部结束 time.Sleep(5*time.Second) } 在分析goroutine执行的随机性和并发性，启动了5个goroutine，再加上main函数的主goroutine，总共有6个goroutines。由于goroutine类似于”守护线程“，异步执行的,如果主goroutine不等待片刻，可能程序就没有输出打印了。 在Golang中channel则是goroutines之间进行通信的渠道。 可以把channel形象比喻为工厂里的传送带,一头的生产者goroutine往传输带放东西,另一头的消费者goroutine则从输送带取东西。channel实际上是一个有类型的消息队列,遵循先进先出的特点。 channel的操作符号 ch \u003c- data 表示data被发送给channel ch； data \u003c- ch 表示从channel ch取一个值，然后赋给data。 阻塞式channel channel默认是没有缓冲区的，也就是说，通信是阻塞的。send操作必须等到有消费者accept才算完成。 应用示例: func main() { ch1 := make(chan int) go pump(ch1) // pump hangs fmt.Println(\u003c-ch1) // prints only 1 } func pump(ch chan int) { for i:= 1; ; i++ { ch \u003c- i } } 在函数pump()里的channel在接受到第一个元素后就被阻塞了，直到主goroutine取走了数据。最终channel阻塞在接受第二个元素，程序只打印 1。 没有缓冲(buffer)的channel只能容纳一个元素，而带有缓冲(buffer)channel则可以非阻塞容纳N个元素。发送数据到缓冲(buffer) channel不会被阻塞，除非channel已满；同样的，从缓冲(buffer) channel取数据也不会被阻塞，除非channel空了。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:15","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"怎么查看Goroutine的数量 在Golang中,GOMAXPROCS中控制的是未被阻塞的所有Goroutine,可以被 Multiplex 到多少个线程上运行,通过GOMAXPROCS可以查看Goroutine的数量。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:16","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中的锁有哪些 Go中的三种锁包括:互斥锁,读写锁,sync.Map的安全的锁. 互斥锁 Go并发程序对共享资源进行访问控制的主要手段，由标准库代码包中sync中的Mutex结构体表示。 // Mutex 是互斥锁， 零值是解锁的互斥锁， 首次使用后不得复制互斥锁。 type Mutex struct { state int32 sema uint32 } sync.Mutex包中的类型只有两个公开的指针方法Lock和Unlock。 // Locker表示可以锁定和解锁的对象。 type Locker interface { Lock() Unlock() } // 锁定当前的互斥量 // 如果锁已被使用，则调用goroutine // 阻塞直到互斥锁可用。 func (m *Mutex) Lock() // 对当前互斥量进行解锁 // 如果在进入解锁时未锁定m，则为运行时错误。 // 锁定的互斥锁与特定的goroutine无关。 // 允许一个goroutine锁定Mutex然后安排另一个goroutine来解锁它。 func (m *Mutex) Unlock() 声明一个互斥锁： var mutex sync.Mutex 不像C或Java的锁类工具，我们可能会犯一个错误：忘记及时解开已被锁住的锁，从而导致流程异常。但Go由于存在defer，所以此类问题出现的概率极低。关于defer解锁的方式如下： var mutex sync.Mutex func Write() { mutex.Lock() defer mutex.Unlock() } 如果对一个已经上锁的对象再次上锁，那么就会导致该锁定操作被阻塞，直到该互斥锁回到被解锁状态. package main import ( \"fmt\" \"sync\" \"time\" ) func main() { var mutex sync.Mutex fmt.Println(\"begin lock\") mutex.Lock() fmt.Println(\"get locked\") for i := 1; i \u003c= 3; i++ { go func(i int) { fmt.Println(\"begin lock \", i) mutex.Lock() fmt.Println(\"get locked \", i) }(i) } time.Sleep(time.Second) fmt.Println(\"Unlock the lock\") mutex.Unlock() fmt.Println(\"get unlocked\") time.Sleep(time.Second) } 我们在for循环之前开始加锁，然后在每一次循环中创建一个协程，并对其加锁，但是由于之前已经加锁了，所以这个for循环中的加锁会陷入阻塞直到main中的锁被解锁， time.Sleep(time.Second) 是为了能让系统有足够的时间运行for循环，输出结果如下： \u003e go run mutex.go begin lock get locked begin lock 3 begin lock 1 begin lock 2 Unlock the lock get unlocked get locked 3 这里可以看到解锁后，三个协程会重新抢夺互斥锁权，最终协程3获胜。 互斥锁锁定操作的逆操作并不会导致协程阻塞，但是有可能导致引发一个无法恢复的运行时的panic，比如对一个未锁定的互斥锁进行解锁时就会发生panic。避免这种情况的最有效方式就是使用defer。 我们知道如果遇到panic，可以使用recover方法进行恢复，但是如果对重复解锁互斥锁引发的panic却是无用的（Go 1.8及以后）。 package main import ( \"fmt\" \"sync\" ) func main() { defer func() { fmt.Println(\"Try to recover the panic\") if p := recover(); p != nil { fmt.Println(\"recover the panic : \", p) } }() var mutex sync.Mutex fmt.Println(\"begin lock\") mutex.Lock() fmt.Println(\"get locked\") fmt.Println(\"unlock lock\") mutex.Unlock() fmt.Println(\"lock is unlocked\") fmt.Println(\"unlock lock again\") mutex.Unlock() } 运行: \u003e go run mutex.go begin lock get locked unlock lock lock is unlocked unlock lock again fatal error: sync: unlock of unlocked mutex goroutine 1 [running]: runtime.throw(0x4bc1a8, 0x1e) /home/keke/soft/go/src/runtime/panic.go:617 +0x72 fp=0xc000084ea8 sp=0xc000084e78 pc=0x427ba2 sync.throw(0x4bc1a8, 0x1e) /home/keke/soft/go/src/runtime/panic.go:603 +0x35 fp=0xc000084ec8 sp=0xc000084ea8 pc=0x427b25 sync.(*Mutex).Unlock(0xc00001a0c8) /home/keke/soft/go/src/sync/mutex.go:184 +0xc1 fp=0xc000084ef0 sp=0xc000084ec8 pc=0x45f821 main.main() /home/keke/go/Test/mutex.go:25 +0x25f fp=0xc000084f98 sp=0xc000084ef0 pc=0x486c1f runtime.main() /home/keke/soft/go/src/runtime/proc.go:200 +0x20c fp=0xc000084fe0 sp=0xc000084f98 pc=0x4294ec runtime.goexit() /home/keke/soft/go/src/runtime/asm_amd64.s:1337 +0x1 fp=0xc000084fe8 sp=0xc000084fe0 pc=0x450ad1 exit status 2 这里试图对重复解锁引发的panic进行recover，但是我们发现操作失败，虽然互斥锁可以被多个协程共享，但还是建议将对同一个互斥锁的加锁解锁操作放在同一个层次的代码中。 读写锁 读写锁是针对读写操作的互斥锁，可以分别针对读操作与写操作进行锁定和解锁操作 。 读写锁的访问控制规则如下： 多个写操作之间是互斥的. 写操作与读操作之间也是互斥的. 多个读操作之间不是互斥的. 在这样的控制规则下，读写锁可以大大降低性能损耗。 在Go的标准库代码包中sync中的RWMutex结构体表示为: // RWMutex是一个读/写互斥锁，可以由任意数量的读操作或单个写操作持有。 // RWMutex的零值是未锁定的互斥锁。 // 首次使用后，不得复制RWMutex。 // 如果goroutine持有RWMutex进行读取而另一个goroutine可能会调用Lock，那么在释放初始读锁之前，goroutine不应该期望能够获取读锁定。 // 特别是，这种禁止递归读锁定。 这是为了确保锁最终变得可用; 阻止的锁定会阻止新读操作获取锁定。 type RWMutex struct { w Mutex //如果有待处理的写操作就持有 writerSem uint32 // 写操作等待读操作完成的信号量 readerSem uint32 //读操作等待写操作完成的信号量 readerCount int32 // 待处理的读操作数量 readerWait int32 // number of departing readers } sync中的RWMutex有以下几种方法： //对读操作的锁定 func (rw *RWMutex) RLock() //对读操作的解锁 func (rw *RWMutex) RUnlock() //对写操作的锁定 func (rw *RWMutex) Lock() //对写操作的解锁 func (rw *RWMutex) Unlock() //返回一个实现了sync.Locker接口类型的值，实际上是回调rw.RLock and rw.RUnlock. func (rw *RWMutex) RLocker() Locker Unlock方法会试图唤醒所有想进行读锁定而被阻塞的协程，而RUnlock方法只会在已无任何读锁定的情况下，试图唤醒一个因欲进行写锁定而被阻塞的协程。 若对一个未被写锁定的读写锁进行写解锁，就会引发一个不可恢复的panic，同理对一个未被读锁定的读写锁进行读写锁也会如此。 由于读写锁控制下的多个读操作之间不是互斥的，因此对于读解锁更容易被忽视。对于同一个读写锁，添加多少个读锁定，就必要有等量的读解锁，这样才能其他协程有机会进行操作。 因此Go中读写锁，在多个读线程可以同时访问共享数","date":"2022-10-09","objectID":"/go-interview-summary/:2:17","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"怎么限制Goroutine的数量 在Golang中，Goroutine虽然很好，但是数量太多了，往往会带来很多麻烦，比如耗尽系统资源导致程序崩溃，或者CPU使用率过高导致系统忙不过来。 所以我们可以限制下Goroutine的数量,这样就需要在每一次执行go之前判断goroutine的数量，如果数量超了，就要阻塞go的执行。 所以通常我们第一时间想到的就是使用通道。每次执行的go之前向通道写入值，直到通道满的时候就阻塞了， package main import \"fmt\" var ch chan int func elegance(){ \u003c-ch fmt.Println(\"the ch value receive\",ch) } func main(){ ch = make(chan int,5) for i:=0;i\u003c10;i++{ ch \u003c-1 fmt.Println(\"the ch value send\",ch) go elegance() fmt.Println(\"the result i\",i) } } 运行: \u003e go run goroutine.go the ch value send 0xc00009c000 the result i 0 the ch value send 0xc00009c000 the result i 1 the ch value send 0xc00009c000 the result i 2 the ch value send 0xc00009c000 the result i 3 the ch value send 0xc00009c000 the result i 4 the ch value send 0xc00009c000 the result i 5 the ch value send 0xc00009c000 the ch value receive 0xc00009c000 the result i 6 the ch value receive 0xc00009c000 the ch value send 0xc00009c000 the result i 7 the ch value send 0xc00009c000 the result i 8 the ch value send 0xc00009c000 the result i 9 the ch value send 0xc00009c000 the ch value receive 0xc00009c000 the ch value receive 0xc00009c000 the ch value receive 0xc00009c000 the result i 10 the ch value send 0xc00009c000 the result i 11 the ch value send 0xc00009c000 the result i 12 the ch value send 0xc00009c000 the result i 13 the ch value send 0xc00009c000 the ch value receive 0xc00009c000 the ch value receive 0xc00009c000 the ch value receive 0xc00009c000 the ch value receive 0xc00009c000 the result i 14 the ch value receive 0xc00009c000 \u003e go run goroutine.go the ch value send 0xc00007e000 the result i 0 the ch value send 0xc00007e000 the result i 1 the ch value send 0xc00007e000 the result i 2 the ch value send 0xc00007e000 the result i 3 the ch value send 0xc00007e000 the ch value receive 0xc00007e000 the result i 4 the ch value send 0xc00007e000 the ch value receive 0xc00007e000 the result i 5 the ch value send 0xc00007e000 the ch value receive 0xc00007e000 the result i 6 the ch value send 0xc00007e000 the result i 7 the ch value send 0xc00007e000 the ch value receive 0xc00007e000 the ch value receive 0xc00007e000 the ch value receive 0xc00007e000 the result i 8 the ch value send 0xc00007e000 the result i 9 这样每次同时运行的goroutine就被限制为5个了。但是新的问题于是就出现了，因为并不是所有的goroutine都执行完了，在main函数退出之后，还有一些goroutine没有执行完就被强制结束了。这个时候我们就需要用到sync.WaitGroup。使用WaitGroup等待所有的goroutine退出。 package main import ( \"fmt\" \"runtime\" \"sync\" \"time\" ) // Pool Goroutine Pool type Pool struct { queue chan int wg *sync.WaitGroup } // New 新建一个协程池 func NewPool(size int) *Pool{ if size \u003c=0{ size = 1 } return \u0026Pool{ queue:make(chan int,size), wg:\u0026sync.WaitGroup{}, } } // Add 新增一个执行 func (p *Pool)Add(delta int){ // delta为正数就添加 for i :=0;i\u003cdelta;i++{ p.queue \u003c-1 } // delta为负数就减少 for i:=0;i\u003edelta;i--{ \u003c-p.queue } p.wg.Add(delta) } // Done 执行完成减一 func (p *Pool) Done(){ \u003c-p.queue p.wg.Done() } // Wait 等待Goroutine执行完毕 func (p *Pool) Wait(){ p.wg.Wait() } func main(){ // 这里限制5个并发 pool := NewPool(5) fmt.Println(\"the NumGoroutine begin is:\",runtime.NumGoroutine()) for i:=0;i\u003c20;i++{ pool.Add(1) go func(i int) { time.Sleep(time.Second) fmt.Println(\"the NumGoroutine continue is:\",runtime.NumGoroutine()) pool.Done() }(i) } pool.Wait() fmt.Println(\"the NumGoroutine done is:\",runtime.NumGoroutine()) } 运行: the NumGoroutine begin is: 1 the NumGoroutine continue is: 6 the NumGoroutine continue is: 7 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 6 the NumGoroutine continue is: 3 the NumGoroutine continue is: 2 the NumGoroutine done is: 1 其中，Go的GOMAXPROCS默认值已经设置为CPU的核数， 这","date":"2022-10-09","objectID":"/go-interview-summary/:2:18","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Channel是同步的还是异步的 Channel是异步进行的, channel存在3种状态： nil，未初始化的状态，只进行了声明，或者手动赋值为nil active，正常的channel，可读或者可写 closed，已关闭，千万不要误认为关闭channel后，channel的值是nil 下面我们对channel的三种操作解析: 零值（nil）通道； 非零值但已关闭的通道； 非零值并且尚未关闭的通道。 操作 一个零值nil通道 一个非零值但已关闭的通道 一个非零值且尚未关闭的通道 关闭 产生恐慌 产生恐慌 成功关闭 发送数据 永久阻塞 产生恐慌 阻塞或者成功发送 接收数据 永久阻塞 永不阻塞 阻塞或者成功接收 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:19","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Goroutine和线程的区别 从调度上看，goroutine的调度开销远远小于线程调度开销。 OS的线程由OS内核调度，每隔几毫秒，一个硬件时钟中断发到CPU，CPU调用一个调度器内核函数。这个函数暂停当前正在运行的线程，把他的寄存器信息保存到内存中，查看线程列表并决定接下来运行哪一个线程，再从内存中恢复线程的注册表信息，最后继续执行选中的线程。这种线程切换需要一个完整的上下文切换：即保存一个线程的状态到内存，再恢复另外一个线程的状态，最后更新调度器的数据结构。某种意义上，这种操作还是很慢的。 Go运行的时候包涵一个自己的调度器，这个调度器使用一个称为一个M:N调度技术，m个goroutine到n个os线程（可以用GOMAXPROCS来控制n的数量），Go的调度器不是由硬件时钟来定期触发的，而是由特定的go语言结构来触发的，他不需要切换到内核语境，所以调度一个goroutine比调度一个线程的成本低很多。 从栈空间上，goroutine的栈空间更加动态灵活。 每个OS的线程都有一个固定大小的栈内存，通常是2MB，栈内存用于保存在其他函数调用期间哪些正在执行或者临时暂停的函数的局部变量。这个固定的栈大小，如果对于goroutine来说，可能是一种巨大的浪费。作为对比goroutine在生命周期开始只有一个很小的栈，典型情况是2KB, 在go程序中，一次创建十万左右的goroutine也不罕见（2KB*100,000=200MB）。而且goroutine的栈不是固定大小，它可以按需增大和缩小，最大限制可以到1GB。 goroutine没有一个特定的标识。 在大部分支持多线程的操作系统和编程语言中，线程有一个独特的标识，通常是一个整数或者指针，这个特性可以让我们构建一个线程的局部存储，本质是一个全局的map，以线程的标识作为键，这样每个线程可以独立使用这个map存储和获取值，不受其他线程干扰。 goroutine中没有可供程序员访问的标识，原因是一种纯函数的理念，不希望滥用线程局部存储导致一个不健康的超距作用，即函数的行为不仅取决于它的参数，还取决于运行它的线程标识。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:20","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go的Struct能不能比较 相同struct类型的可以比较 不同struct类型的不可以比较,编译都不过，类型不匹配 package main import \"fmt\" func main() { type A struct { a int } type B struct { a int } a := A{1} //b := A{1} b := B{1} if a == b { fmt.Println(\"a == b\") }else{ fmt.Println(\"a != b\") } } output: \u003e command-line-arguments [command-line-arguments.test] \u003e ./.go:14:7: invalid operation: a == b (mismatched types A and B) ","date":"2022-10-09","objectID":"/go-interview-summary/:2:21","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go的defer原理是什么 什么是defer？如何理解 defer 关键字？Go 中使用 defer 的一些坑。 defer 意为延迟，在 golang 中用于延迟执行一个函数。它可以帮助我们处理容易忽略的问题，如资源释放、连接关闭等。但在实际使用过程中，有一些需要注意的地方. 若函数中有多个 defer，其执行顺序为 先进后出，可以理解为栈。 package main import \"fmt\" func main() { for i := 0; i \u003c 5; i++ { defer fmt.Println(i) } } 运行: 4 3 2 1 0 return 会做什么呢? Go 的函数返回值是通过堆栈返回的, return 语句不是原子操作，而是被拆成了两步. 给返回值赋值 (rval) 调用 defer 表达式 返回给调用函数(ret) package main import \"fmt\" func main() { fmt.Println(increase(1)) } func increase(d int) (ret int) { defer func() { ret++ }() return d } 运行输出: 2 若 defer 表达式有返回值，将会被丢弃。 闭包与匿名函数. 匿名函数:没有函数名的函数。 闭包:可以使用另外一个函数作用域中的变量的函数。 在实际开发中，defer 的使用经常伴随着闭包与匿名函数的使用。 package main import \"fmt\" func main() { for i := 0; i \u003c 5; i++ { defer func() { fmt.Println(i) }() } } 运行输出: 5 5 5 5 5 之所以这样是因为,defer 表达式中的 i 是对 for 循环中 i 的引用。到最后，i 加到 5，故最后全部打印 5。 如果将 i 作为参数传入 defer 表达式中，在传入最初就会进行求值保存，只是没有执行延迟函数而已。 应用示例: func f1() (result int) { defer func() { result++ }() return 0 } func f2() (r int) { t := 5 defer func() { t = t + 5 }() return t } func f3() (r int) { defer func(r int) { r = r + 5 }(r) return 1 } type Test struct { Max int } func (t *Test) Println() { fmt.Println(t.Max) } func deferExec(f func()) { f() } func call() { var t *Test defer deferExec(t.Println) t = new(Test) } 有没有得出结果？例1的答案不是 0，例2的答案不是 10，例3的答案也不是 6。 defer是在return之前执行的。这个在官方文档中是明确说明了的。要使用defer时不踩坑，最重要的一点就是要明白，return xxx这一条语句并不是一条原子指令! 函数返回的过程是这样的：先给返回值赋值，然后调用defer表达式，最后才是返回到调用函数中。 defer表达式可能会在设置函数返回值之后，在返回到调用函数之前，修改返回值，使最终的函数返回值与你想象的不一致。 其实使用defer时，用一个简单的转换规则改写一下，就不会迷糊了。改写规则是将return语句拆成两句写，return xxx会被改写成: 返回值 = xxx 调用defer函数 空的return f1: 比较简单，参考结论2，将 0 赋给 result，defer 延迟函数修改 result，最后返回给调用函数。正确答案是 1。 f1可以修改成长这样的: func f() (result int) { result = 0 // return语句不是一条原子调用，return xxx其实是赋值＋ret指令 func() { // defer被插入到return之前执行，也就是赋返回值和ret指令之间 result++ }() return } 所以这个返回值是1。 f2: defer 是在 t 赋值给 r 之后执行的，而 defer 延迟函数只改变了 t 的值，r 不变。正确答案 5。 f2可以修改成这样的: func f() (r int) { t := 5 r = t // 赋值指令 func() { // defer被插入到赋值与返回之间执行，这个例子中返回值r没被修改过 t = t + 5 } return // 空的return指令 } 所以这个的结果是5。 f3: 这里将 r 作为参数传入了 defer 表达式。故 func (r int) 中的 r 非 func f() (r int) 中的 r，只是参数命名相同而已。正确答案 1。 f3可以修改成这样的: func f() (r int) { r = 1 // 给返回值赋值 func(r int) { // 这里改的r是传值传进去的r，不会改变要返回的那个r值 r = r + 5 }(r) return // 空的return } 所以这个例子的结果是1。 f4: 这里将发生 panic。将方法传给 deferExec，实际上在传的过程中对方法求了值。而此时的 t 任然为 nil。 因此, defer确实是在return之前调用的。但表现形式上却可能不像。根本原因是 return xxx 语句并不是一条原子指令，defer被插入到了赋值 与 ret之间，因此可能有机会改变最终的返回值。 defer关键字的实现跟go关键字很类似，不同的是它调用的是runtime.deferproc而不是runtime.newproc。 在defer出现的地方，插入了指令call runtime.deferproc，然后在函数返回之前的地方，插入指令call runtime.deferreturn。 普通的函数返回时，汇编代码类似: add xx SP return 如果其中包含了defer语句，则汇编代码是： call runtime.deferreturn， add xx SP return goroutine的控制结构中，有一张表记录defer，调用runtime.deferproc时会将需要defer的表达式记录在表中，而在调用runtime.deferreturn的时候，则会依次从defer表中出栈并执行。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:22","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go的select可以用于什么 Golang 的 select 机制可以理解为是在语言层面实现了和select, poll, epoll 相似的功能：监听多个描述符的读/写等事件，一旦某个描述符就绪（一般是读或者写事件发生了），就能够将发生的事件通知给关心的应用程序去处理该事件。 golang 的 select 机制是，监听多个channel，每一个 case 是一个事件，可以是读事件也可以是写事件，随机选择一个执行，可以设置default. 它的作用是：当监听的多个事件都阻塞住会执行default的逻辑。 select的源码在 (runtime/select.go)[https://github.com/golang/go/blob/master/src/runtime/select.go] ，看的时候建议是重点关注 pollorder 和 lockorder. pollorder保存的是scase的序号，乱序是为了之后执行时的随机性。 lockorder保存了所有case中channel的地址，这里按照地址大小堆排了一下lockorder对应的这片连续内存。对chan排序是为了去重，保证之后对所有channel上锁时不会重复上锁。 goroutine作为Golang并发的核心，我们不仅要关注它们的创建和管理，当然还要关注如何合理的退出这些协程，不（合理）退出不然可能会造成阻塞、panic、程序行为异常、数据结果不正确等问题。goroutine在退出方面，不像线程和进程，不能通过某种手段强制关闭它们，只能等待goroutine主动退出。 goroutine的优雅退出方法有三种: 使用for-range退出 for-range是使用频率很高的结构，常用它来遍历数据，range能够感知channel的关闭，当channel被发送数据的协程关闭时，range就会结束，接着退出for循环。 它在并发中的使用场景是：当协程只从1个channel读取数据，然后进行处理，处理后协程退出。下面这个示例程序，当in通道被关闭时，协程可自动退出。 go func(in \u003c-chan int) { // Using for-range to exit goroutine // range has the ability to detect the close/end of a channel for x := range in { fmt.Printf(\"Process %d\\n\", x) } }(in) 使用select case ,ok退出 for-select也是使用频率很高的结构，select提供了多路复用的能力，所以for-select可以让函数具有持续多路处理多个channel的能力。但select没有感知channel的关闭，这引出了2个问题： 继续在关闭的通道上读，会读到通道传输数据类型的零值，如果是指针类型，读到nil，继续处理还会产生nil。 继续在关闭的通道上写，将会panic。 问题2可以这样解决，通道只由发送方关闭，接收方不可关闭，即某个写通道只由使用该select的协程关闭，select中就不存在继续在关闭的通道上写数据的问题。 问题1可以使用,ok来检测通道的关闭，使用情况有2种。 第一种：如果某个通道关闭后，需要退出协程，直接return即可。示例代码中，该协程需要从in通道读数据，还需要定时打印已经处理的数量，有2件事要做，所有不能使用for-range，需要使用for-select，当in关闭时，ok=false，我们直接返回。 go func() { // in for-select using ok to exit goroutine for { select { case x, ok := \u003c-in: if !ok { return } fmt.Printf(\"Process %d\\n\", x) processedCnt++ case \u003c-t.C: fmt.Printf(\"Working, processedCnt = %d\\n\", processedCnt) } } }() 第二种：如果某个通道关闭了，不再处理该通道，而是继续处理其他case，退出是等待所有的可读通道关闭。我们需要使用select的一个特征：select不会在nil的通道上进行等待。这种情况，把只读通道设置为nil即可解决。 go func() { // in for-select using ok to exit goroutine for { select { case x, ok := \u003c-in1: if !ok { in1 = nil } // Process case y, ok := \u003c-in2: if !ok { in2 = nil } // Process case \u003c-t.C: fmt.Printf(\"Working, processedCnt = %d\\n\", processedCnt) } // If both in channel are closed, goroutine exit if in1 == nil \u0026\u0026 in2 == nil { return } } }() 使用退出通道退出 使用,ok来退出使用for-select协程，解决是当读入数据的通道关闭时，没数据读时程序的正常结束。想想下面这2种场景，,ok还能适用吗？ 接收的协程要退出了，如果它直接退出，不告知发送协程，发送协程将阻塞。启动了一个工作协程处理数据，如何通知它退出？ 使用一个专门的通道，发送退出的信号，可以解决这类问题。以第2个场景为例，协程入参包含一个停止通道stopCh，当stopCh被关闭，case \u003c-stopCh会执行，直接返回即可。 当我启动了100个worker时，只要main()执行关闭stopCh，每一个worker都会都到信号，进而关闭。如果main()向stopCh发送100个数据，这种就低效了。 func worker(stopCh \u003c-chan struct{}) { go func() { defer fmt.Println(\"worker exit\") // Using stop channel explicit exit for { select { case \u003c-stopCh: fmt.Println(\"Recv stop signal\") return case \u003c-t.C: fmt.Println(\"Working .\") } } }() return } 通过channel控制子goroutine的方法可以总结为：循环监听一个channel，一般来说是for循环里放一个select监听channel以达到通知子goroutine的效果。再借助Waitgroup，主进程可以等待所有协程优雅退出后再结束自己的运行，这就通过channel实现了优雅控制goroutine并发的开始和结束。 因此在退出协程的时候需要注意: 发送协程主动关闭通道，接收协程不关闭通道。使用技巧：把接收方的通道入参声明为只读，如果接收协程关闭只读协程，编译时就会报错。 协程处理1个通道，并且是读时，协程优先使用for-range，因为range可以关闭通道的关闭自动退出协程。 ok可以处理多个读通道关闭，需要关闭当前使用for-select的协程。 显式关闭通道stopCh可以处理主动通知协程退出的场景。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:23","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go的Context包的用途是什么 在 Go http包的Server中，每一个请求在都有一个对应的 goroutine 去处理。请求处理函数通常会启动额外的 goroutine 用来访问后端服务，比如数据库和RPC服务。用来处理一个请求的 goroutine 通常需要访问一些与请求特定的数据，比如终端用户的身份认证信息、验证相关的token、请求的截止时间。 当一个请求被取消或超时时，所有用来处理该请求的 goroutine 都应该迅速退出，然后系统才能释放这些 goroutine 占用的资源。 在Google 内部，我们开发了 Context 包，专门用来简化 对于处理单个请求的多个 goroutine 之间与请求域的数据、取消信号、截止时间等相关操作，这些操作可能涉及多个 API 调用。 context的数据结构是: // A Context carries a deadline, cancelation signal, and request-scoped values // across API boundaries. Its methods are safe for simultaneous use by multiple // goroutines. type Context interface { // Done returns a channel that is closed when this `Context` is canceled // or times out. Done() \u003c-chan struct{} // Err indicates why this Context was canceled, after the Done channel // is closed. Err() error // Deadline returns the time when this Context will be canceled, if any. Deadline() (deadline time.Time, ok bool) // Value returns the value associated with key or nil if none. Value(key interface{}) interface{} } Context中的方法: Done会返回一个channel，当该context被取消的时候，该channel会被关闭，同时对应的使用该context的routine也应该结束并返回。 Context中的方法是协程安全的，这也就代表了在父routine中创建的context，可以传递给任意数量的routine并让他们同时访问。 Deadline会返回一个超时时间，routine获得了超时时间后，可以对某些io操作设定超时时间。 Value可以让routine共享一些数据，当然获得数据是协程安全的。 这里需要注意一点的是在goroutine中使用context包的时候,通常我们需要在goroutine中新创建一个上下文的context,原因是:如果直接传递外部context到协层中,一个请求可能在主函数中已经结束,在goroutine中如果还没有结束的话,会直接导致goroutine中的运行的被取消. go func() { _, ctx, _ := log.FromContextOrNew(context.Background(), nil) }() context.Background函数的返回值是一个空的context，经常作为树的根结点，它一般由接收请求的第一个routine创建，不能被取消、没有值、也没有过期时间。 Background函数的声明如下： // Background returns an empty Context. It is never canceled, has no deadline, // and has no values. Background is typically used in main, init, and tests, // and as the top-level `Context` for incoming requests. func Background() Context WithCancel 和 WithTimeout 函数 会返回继承的 Context 对象， 这些对象可以比它们的父 Context 更早地取消。 当请求处理函数返回时，与该请求关联的 Context 会被取消。 当使用多个副本发送请求时，可以使用 WithCancel取消多余的请求。 WithTimeout 在设置对后端服务器请求截止时间时非常有用。 下面是这三个函数的声明： // WithCancel returns a copy of parent whose Done channel is closed as soon as // parent.Done is closed or cancel is called. func WithCancel(parent Context) (ctx Context, cancel CancelFunc) // A CancelFunc cancels a Context. type CancelFunc func() // WithTimeout returns a copy of parent whose Done channel is closed as soon as // parent.Done is closed, cancel is called, or timeout elapses. The new // Context's Deadline is the sooner of now+timeout and the parent's deadline, if // any. If the timer is still running, the cancel function releases its // resources. func WithTimeout(parent Context, timeout time.Duration) (Context, CancelFunc) 调用CancelFunc对象将撤销对应的Context对象，这样父结点的所在的环境中，获得了撤销子节点context的权利，当触发某些条件时，可以调用CancelFunc对象来终止子结点树的所有routine。在子节点的routine中，需要判断何时退出routine： select { case \u003c-cxt.Done(): // do some cleaning and return } 根据cxt.Done()判断是否结束。当顶层的Request请求处理结束，或者外部取消了这次请求，就可以cancel掉顶层context，从而使整个请求的routine树得以退出。 WithDeadline和WithTimeout比WithCancel多了一个时间参数，它指示context存活的最长时间。如果超过了过期时间，会自动撤销它的子context。所以context的生命期是由父context的routine和deadline共同决定的。 WithValue 函数能够将请求作用域的数据与 Context 对象建立关系。声明如下： type valueCtx struct { Context key, val interface{} } func WithValue(parent Context, key, val interface{}) Context { if key == nil { panic(\"nil key\") } ...... return \u0026valueCtx{parent, key, val} } func (c *valueCtx) Value(key interface{}) interface{} { if c.key == key { return c.val } return c.Context.Value(key) } WithValue返回parent的一个副本，该副本保存了传入的key/value，而调用Context接口的Value(key)方法就可以得到val。注意在同一个context中设置key/value，若key相同，值会被覆盖。 Context上下文数据的存储就像一个树，每个结点只存储一个key/value对。WithValue()保存一个key/value对，它将父context嵌入到新的子context，并在节点中保存了key/value数据。Value()查询key对应的value数据，会从当前context中查询，如果查不到，会递归查询父context中的数据。 值得注意的是，context中的上下文数据并不是全局的，它只查询本节点及父节点们的数据，不能查询兄弟节点的数据。 Context 使用原则: 不要把Context放在结构体中，要以参数的方式传递。 以Context作为参数的函数方法，应该把Context作为第一个参数，放在第一位。 给一个函数方法传递Context的时候，不要传递nil，如果不知道传递什么，就使用context.TODO。 Context的Value相关方法应该传递必须的数据，不要什么数据都使用这个传递。 Context是线程安全的，可以放心的在多个goroutine中传","date":"2022-10-09","objectID":"/go-interview-summary/:2:24","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go主协程如何等其余协程完再操作 Go提供了更简单的方法——使用sync.WaitGroup。WaitGroup，就是用来等待一组操作完成的。WaitGroup内部实现了一个计数器，用来记录未完成的操作个数. 它提供了三个方法，Add()用来添加计数。Done()用来在操作结束时调用，使计数减一。Wait()用来等待所有的操作结束，即计数变为0，该函数会在计数不为0时等待，在计数为0时立即返回。 应用示例: package main import ( \"fmt\" \"sync\" ) func main() { var wg sync.WaitGroup wg.Add(2) // 因为有两个动作，所以增加2个计数 go func() { fmt.Println(\"Goroutine 1\") wg.Done() // 操作完成，减少一个计数 }() go func() { fmt.Println(\"Goroutine 2\") wg.Done() // 操作完成，减少一个计数 }() wg.Wait() // 等待，直到计数为0 } 运行输出: Goroutine 2 Goroutine 1 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:25","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go的Slice如何扩容 slice是 Go 中的一种基本的数据结构，使用这种结构可以用来管理数据集合。但是slice本身并不是动态数据或者数组指针。slice常见的操作有 reslice、append、copy。 slice自身并不是动态数组或者数组指针。它内部实现的数据结构通过指针引用底层数组，设定相关属性将数据读写操作限定在指定的区域内。slice本身是一个只读对象，其工作机制类似数组指针的一种封装。 slice是对数组一个连续片段的引用，所以切片是一个引用类型（因此更类似于 C/C++ 中的数组类型，或者 Python 中的 list类型）。这个片段可以是整个数组，或者是由起始和终止索引标识的一些项的子集。 这里需要注意的是，终止索引标识的项不包括在切片内。切片提供了一个与指向数组的动态窗口。 slice是可以看做是一个长度可变的数组。 slice数据结构如下: type slice struct { array unsafe.Pointer len int cap int } slice的结构体由3部分构成，Pointer 是指向一个数组的指针，len 代表当前切片的长度，cap 是当前切片的容量。cap 总是大于等于 len 的。 通常我们在对slice进行append等操作时，可能会造成slice的自动扩容。 其扩容时的大小增长规则是： 如果切片的容量小于1024个元素，那么扩容的时候slice的cap就翻番，乘以2；一旦元素个数超过1024个元素，增长因子就变成1.25，即每次增加原来容量的四分之一。 如果扩容之后，还没有触及原数组的容量，那么，切片中的指针指向的位置，就还是原数组，如果扩容之后，超过了原数组的容量，那么，Go就会开辟一块新的内存，把原来的值拷贝过来，这种情况丝毫不会影响到原数组。 通过slice源码可以看到,append的实现只是简单的在内存中将旧slice复制给新slice. newcap := old.cap if newcap+newcap \u003c cap { newcap = cap } else { for { if old.len \u003c 1024 { newcap += newcap } else { newcap += newcap / 4 } if newcap \u003e= cap { break } } } ","date":"2022-10-09","objectID":"/go-interview-summary/:2:26","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中的map如何实现顺序读取 Go中map如果要实现顺序读取的话,可以先把map中的key,通过sort包排序. 通过sort中的排序包进行对map中的key进行排序. package main import ( \"fmt\" \"sort\" ) func main() { var m = map[string]int{ \"hello\": 0, \"morning\": 1, \"jojo\": 2, \"jame\": 3, } var keys []string for k := range m { keys = append(keys, k) } sort.Strings(keys) for _, k := range keys { fmt.Println(\"Key:\", k, \"Value:\", m[k]) } } ","date":"2022-10-09","objectID":"/go-interview-summary/:2:27","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中CAS是怎么回事 CAS算法（Compare And Swap）,是原子操作的一种, CAS算法是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。可用于在多线程编程中实现不被打断的数据交换操作，从而避免多线程同时改写某一数据时由于执行顺序不确定性以及中断的不可预知性产生的数据不一致问题。 该操作通过将内存中的值与指定数据进行比较，当数值一样时将内存中的数据替换为新的值。 Go中的CAS操作是借用了CPU提供的原子性指令来实现。CAS操作修改共享变量时候不需要对共享变量加锁，而是通过类似乐观锁的方式进行检查，本质还是不断的占用CPU 资源换取加锁带来的开销（比如上下文切换开销）。 package main import ( \"fmt\" \"sync\" \"sync/atomic\" ) var ( counter int32 //计数器 wg sync.WaitGroup //信号量 ) func main() { threadNum := 5 wg.Add(threadNum) for i := 0; i \u003c threadNum; i++ { go incCounter(i) } wg.Wait() } func incCounter(index int) { defer wg.Done() spinNum := 0 for { // 原子操作 old := counter ok := atomic.CompareAndSwapInt32(\u0026counter, old, old+1) if ok { break } else { spinNum++ } } fmt.Printf(\"thread,%d,spinnum,%d\\n\", index, spinNum) } 当主函数main首先创建了5个信号量，然后开启五个线程执行incCounter方法,incCounter内部执行, 使用cas操作递增counter的值，atomic.CompareAndSwapInt32具有三个参数，第一个是变量的地址，第二个是变量当前值，第三个是要修改变量为多少，该函数如果发现传递的old值等于当前变量的值，则使用第三个变量替换变量的值并返回true，否则返回false。 这里之所以使用无限循环是因为在高并发下每个线程执行CAS并不是每次都成功，失败了的线程需要重写获取变量当前的值，然后重新执行CAS操作。读者可以把线程数改为10000或者更多就会发现输出thread,5329,spinnum,1 其中这个1就说明该线程尝试了两个CAS操作，第二次才成功。 因此呢, go中CAS操作可以有效的减少使用锁所带来的开销，但是需要注意在高并发下这是使用cpu资源做交换的。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:28","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中的逃逸分析是什么 在Go中逃逸分析是一种确定指针动态范围的方法，可以分析在程序的哪些地方可以访问到指针。它涉及到指针分析和形状分析。 当一个变量(或对象)在子程序中被分配时，一个指向变量的指针可能逃逸到其它执行线程中，或者去调用子程序。如果使用尾递归优化（通常在函数编程语言中是需要的），对象也可能逃逸到被调用的子程序中。 如果一个子程序分配一个对象并返回一个该对象的指针，该对象可能在程序中的任何一个地方被访问到——这样指针就成功“逃逸”了。 如果指针存储在全局变量或者其它数据结构中，它们也可能发生逃逸，这种情况是当前程序中的指针逃逸。 逃逸分析需要确定指针所有可以存储的地方，保证指针的生命周期只在当前进程或线程中。 导致内存逃逸的情况比较多，有些可能还是官方未能够实现精确的分析逃逸情况的 bug，通常来讲就是如果变量的作用域不会扩大并且其行为或者大小能够在编译的时候确定，一般情况下都是分配到栈上，否则就可能发生内存逃逸分配到堆上。 内存逃逸的五种情况: 发送指针的指针或值包含了指针到channel 中，由于在编译阶段无法确定其作用域与传递的路径，所以一般都会逃逸到堆上分配。 slices 中的值是指针的指针或包含指针字段。一个例子是类似[]*string 的类型。这总是导致 slice 的逃逸。即使切片的底层存储数组仍可能位于堆栈上，数据的引用也会转移到堆中。 slice 由于 append 操作超出其容量，因此会导致 slice 重新分配。这种情况下，由于在编译时 slice 的初始大小的已知情况下，将会在栈上分配。如果 slice 的底层存储必须基于仅在运行时数据进行扩展，则它将分配在堆上。 调用接口类型的方法。接口类型的方法调用是动态调度,实际使用的具体实现只能在运行时确定。考虑一个接口类型为 io.Reader 的变量 r。对 r.Read(b) 的调用将导致 r 的值和字节片b的后续转义并因此分配到堆上。 尽管能够符合分配到栈的场景，但是其大小不能够在在编译时候确定的情况，也会分配到堆上. 有效的避免上述的五种逃逸的情况,就可以避免内存逃逸. ","date":"2022-10-09","objectID":"/go-interview-summary/:2:29","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go值接收者和指针接收者的区别 Go中的方法能给用户自定义的类型添加新的行为。它和函数的区别在于方法有一个接收者，给一个函数添加一个接收者，那么它就变成了方法。接收者可以是值接收者，也可以是指针接收者。 在调用方法的时候，值类型既可以调用值接收者的方法，也可以调用指针接收者的方法；指针类型既可以调用指针接收者的方法，也可以调用值接收者的方法。 也就是说，不管方法的接收者是什么类型，该类型的值和指针都可以调用，不必严格符合接收者的类型。 package main import \"fmt\" type Person struct { age int } func (p Person) Elegance() int { return p.age } func (p *Person) GetAge() { p.age += 1 } func main() { // p1 是值类型 p := Person{age: 18} // 值类型 调用接收者也是值类型的方法 fmt.Println(p.howOld()) // 值类型 调用接收者是指针类型的方法 p.GetAge() fmt.Println(p.GetAge()) // ---------------------- // p2 是指针类型 p2 := \u0026Person{age: 100} // 指针类型 调用接收者是值类型的方法 fmt.Println(p2.GetAge()) // 指针类型 调用接收者也是指针类型的方法 p2.GetAge() fmt.Println(p2.GetAge()) } 运行 18 19 100 101 函数和方法 值接收者 指针接收者 值类型调用者 方法会使用调用者的一个副本，类似于“传值” 使用值的引用来调用方法，上例中，p1.GetAge() 实际上是 (\u0026p1).GetAge(). 指针类型调用者 指针被解引用为值，上例中，p2.GetAge()实际上是 (*p1).GetAge() 实际上也是“传值”，方法里的操作会影响到调用者，类似于指针传参，拷贝了一份指针 如果实现了接收者是值类型的方法，会隐含地也实现了接收者是指针类型的方法。 如果方法的接收者是值类型，无论调用者是对象还是对象指针，修改的都是对象的副本，不影响调用者；如果方法的接收者是指针类型，则调用者修改的是指针指向的对象本身。 通常我们使用指针作为方法的接收者的理由： 使用指针方法能够修改接收者指向的值。 可以避免在每次调用方法时复制该值，在值的类型为大型结构体时，这样做会更加高效。 因而呢,我们是使用值接收者还是指针接收者，不是由该方法是否修改了调用者（也就是接收者）来决定，而是应该基于该类型的本质。 如果类型具备“原始的本质”，也就是说它的成员都是由 Go 语言里内置的原始类型，如字符串，整型值等，那就定义值接收者类型的方法。像内置的引用类型，如 slice，map，interface，channel，这些类型比较特殊，声明他们的时候，实际上是创建了一个 header， 对于他们也是直接定义值接收者类型的方法。这样，调用函数时，是直接 copy 了这些类型的 header，而 header 本身就是为复制设计的。 如果类型具备非原始的本质，不能被安全地复制，这种类型总是应该被共享，那就定义指针接收者的方法。比如 go 源码里的文件结构体（struct File）就不应该被复制，应该只有一份实体。 接口值的零值是指动态类型和动态值都为 nil。当仅且当这两部分的值都为 nil 的情况下，这个接口值就才会被认为 接口值 == nil。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:30","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go的对象在内存中是怎样分配的 Go中的内存分类并不像TCMalloc那样分成小、中、大对象，但是它的小对象里又细分了一个Tiny对象，Tiny对象指大小在1Byte到16Byte之间并且不包含指针的对象。 小对象和大对象只用大小划定，无其他区分。 大对象指大小大于32kb.小对象是在mcache中分配的，而大对象是直接从mheap分配的，从小对象的内存分配看起。 Go的内存分配原则: Go在程序启动的时候，会先向操作系统申请一块内存（注意这时还只是一段虚拟的地址空间，并不会真正地分配内存），切成小块后自己进行管理。 申请到的内存块被分配了三个区域，在X64上分别是512MB，16GB，512GB大小。 arena区域就是我们所谓的堆区，Go动态分配的内存都是在这个区域，它把内存分割成8KB大小的页，一些页组合起来称为mspan。 bitmap区域标识arena区域哪些地址保存了对象，并且用4bit标志位表示对象是否包含指针、GC标记信息。bitmap中一个byte大小的内存对应arena区域中4个指针大小（指针大小为 8B ）的内存，所以bitmap区域的大小是512GB/(4*8B)=16GB。 此外我们还可以看到bitmap的高地址部分指向arena区域的低地址部分，这里bitmap的地址是由高地址向低地址增长的。 spans区域存放mspan（是一些arena分割的页组合起来的内存管理基本单元，后文会再讲）的指针，每个指针对应一页，所以spans区域的大小就是 512GB/8KB*8B=512MB。 除以8KB是计算arena区域的页数，而最后乘以8是计算spans区域所有指针的大小。创建mspan的时候，按页填充对应的spans区域，在回收object时，根据地址很容易就能找到它所属的mspan。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:31","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"栈的内存是怎么分配的 栈和堆只是虚拟内存上2块不同功能的内存区域： 栈在高地址，从高地址向低地址增长。 堆在低地址，从低地址向高地址增长。 栈和堆相比优势： 栈的内存管理简单，分配比堆上快。 栈的内存不需要回收，而堆需要，无论是主动free，还是被动的垃圾回收，这都需要花费额外的CPU。 栈上的内存有更好的局部性，堆上内存访问就不那么友好了，CPU访问的2块数据可能在不同的页上，CPU访问数据的时间可能就上去了。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:32","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"堆内存管理怎么分配的 通常在Golang中,当我们谈论内存管理的时候，主要是指堆内存的管理，因为栈的内存管理不需要程序去操心。 堆内存管理中主要是三部分, 1.分配内存块，2.回收内存块, 3.组织内存块。 一个内存块包含了3类信息，如下图所示，元数据、用户数据和对齐字段，内存对齐是为了提高访问效率。下图申请5Byte内存的时候，就需要进行内存对齐。 释放内存实质是把使用的内存块从链表中取出来，然后标记为未使用，当分配内存块的时候，可以从未使用内存块中有先查找大小相近的内存块，如果找不到，再从未分配的内存中分配内存。 上面这个简单的设计中还没考虑内存碎片的问题，因为随着内存不断的申请和释放，内存上会存在大量的碎片，降低内存的使用率。为了解决内存碎片，可以将2个连续的未使用的内存块合并，减少碎片。 想要深入了解可以看下这个文章,《Writing a Memory Allocator》. ","date":"2022-10-09","objectID":"/go-interview-summary/:2:33","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中的defer函数使用下面的两种情况下结果是什么 我们看看下面两种defer函数的返回的是什么: a := 1 defer fmt.Println(\"the value of a1:\",a) a++ defer func() { fmt.Println(\"the value of a2:\",a) }() 运行: the value of a2: 2 the value of a1:1 第一种情况： defer fmt.Println(\"the value of a1:\",a) defer延迟函数调用的fmt.Println(a)函数的参数值在defer语句出现时就已经确定了，所以无论后面如何修改a变量都不会影响延迟函数。 第二种情况: defer func() { fmt.Println(\"the value of a2:\",a) }() defer延迟函数调用的函数参数的值在defer定义时候就确定了，而defer延迟函数内部所使用的值需要在这个函数运行时候才确定。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:34","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"在Go函数中为什么会发生内存泄露 通常内存泄漏，指的是能够预期的能很快被释放的内存由于附着在了长期存活的内存上、或生命期意外地被延长，导致预计能够立即回收的内存而长时间得不到回收。 在 Go 中，由于 goroutine 的存在，因此,内存泄漏除了附着在长期对象上之外，还存在多种不同的形式。 预期能被快速释放的内存因被根对象引用而没有得到迅速释放. 当有一个全局对象时，可能不经意间将某个变量附着在其上，且忽略的将其进行释放，则该内存永远不会得到释放。 goroutine 泄漏 Goroutine 作为一种逻辑上理解的轻量级线程，需要维护执行用户代码的上下文信息。在运行过程中也需要消耗一定的内存来保存这类信息，而这些内存在目前版本的 Go 中是不会被释放的。 因此，如果一个程序持续不断地产生新的 goroutine、且不结束已经创建的 goroutine 并复用这部分内存，就会造成内存泄漏的现象. 例如: func main() { for i := 0; i \u003c 10000; i++ { go func() { select {} }() } } ","date":"2022-10-09","objectID":"/go-interview-summary/:2:35","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中new和make的区别 在Go中,的值类型和引用类型: 值类型：int，float，bool，string，struct和array. 变量直接存储值，分配栈区的内存空间，这些变量所占据的空间在函数被调用完后会自动释放。 引用类型：slice，map，chan和值类型对应的指针. 变量存储的是一个地址（或者理解为指针），指针指向内存中真正存储数据的首地址。内存通常在堆上分配，通过GC回收。 这里需要注意的是: 对于引用类型的变量，我们不仅要声明变量，更重要的是，我们得手动为它分配空间. 因此new该方法的参数要求传入一个类型，而不是一个值，它会申请一个该类型大小的内存空间，并会初始化为对应的零值，返回指向该内存空间的一个指针。 // The new built-in function allocates memory. The first argument is a type, // not a value, and the value returned is a pointer to a newly // allocated zero value of that type. func new(Type) *Type 而make也是用于内存分配，但是和new不同，只用来引用对象slice、map和channel的内存创建，它返回的类型就是类型本身，而不是它们的指针类型。 // The make built-in function allocates and initializes an object of type // slice, map, or chan (only). Like new, the first argument is a type, not a // value. Unlike new, make's return type is the same as the type of its // argument, not a pointer to it. The specification of the result depends on // the type: // Slice: The size specifies the length. The capacity of the slice is // equal to its length. A second integer argument may be provided to // specify a different capacity; it must be no smaller than the // length. For example, make([]int, 0, 10) allocates an underlying array // of size 10 and returns a slice of length 0 and capacity 10 that is // backed by this underlying array. // Map: An empty map is allocated with enough space to hold the // specified number of elements. The size may be omitted, in which case // a small starting size is allocated. // Channel: The channel's buffer is initialized with the specified // buffer capacity. If zero, or the size is omitted, the channel is // unbuffered. func make(t Type, size ...IntegerType) Type ","date":"2022-10-09","objectID":"/go-interview-summary/:2:36","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"G0的作用 在Go中 g0作为一个特殊的goroutine，为 scheduler 执行调度循环提供了场地（栈）。对于一个线程来说，g0 总是它第一个创建的 goroutine。 之后，它会不断地寻找其他普通的 goroutine 来执行，直到进程退出。 当需要执行一些任务，且不想扩栈时，就可以用到 g0 了，因为 g0 的栈比较大。 g0 其他的一些“职责”有：创建 goroutine、deferproc 函数里新建 _defer、垃圾回收相关的工作（例如 stw、扫描 goroutine 的执行栈、一些标识清扫的工作、栈增长）等等。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:37","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中的锁如何实现 锁是一种同步机制，用于在多任务环境中限制资源的访问，以满足互斥需求。 go源码sync包中经常用于同步操作的方式: 原子操作 互斥锁 读写锁 waitgroup 我们着重来分析下互斥锁和读写锁. 互斥锁: 下面是互斥锁的数据结构: // A Mutex is a mutual exclusion lock. // The zero value for a Mutex is an unlocked mutex. // // A Mutex must not be copied after first use. type Mutex struct { state int32 // 互斥锁上锁状态枚举值如下所示 sema uint32 // 信号量，向处于Gwaitting的G发送信号 } const ( mutexLocked = 1 \u003c\u003c iota // 值为1，表示在state中由低向高第1位，意义：锁是否可用,0可用，1不可用,锁定中 mutexWoken // 值为2，表示在state中由低向高第2位，意义：mutex是否被唤醒 mutexStarving // 当前的互斥锁进入饥饿状态； mutexWaiterShift = iota //值为2，表示state中统计阻塞在此mutex上goroutine的数目需要位移的偏移量 starvationThresholdNs = 1e6 state和sema两个加起来只占 8 字节空间的结构体表示了 Go 语言中的互斥锁。 互斥锁的状态比较复杂，如下图所示，最低三位分别表示 mutexLocked、mutexWoken 和 mutexStarving，剩下的位置用来表示当前有多少个 Goroutine 等待互斥锁的释放. 在默认情况下，互斥锁的所有状态位都是 0，int32 中的不同位分别表示了不同的状态： mutexLocked 表示互斥锁的锁定状态； mutexWoken 表示从正常模式被从唤醒； mutexStarving 当前的互斥锁进入饥饿状态； waitersCount 当前互斥锁上等待的 Goroutine 个数； sync.Mutex 有两种模式,正常模式和饥饿模式。 在正常模式下，锁的等待者会按照先进先出的顺序获取锁。 但是刚被唤起的 Goroutine 与新创建的 Goroutine 竞争时，大概率会获取不到锁，为了减少这种情况的出现，一旦 Goroutine 超过 1ms 没有获取到锁，它就会将当前互斥锁切换饥饿模式，防止部分 Goroutine 被饿死。 饥饿模式是在 Go 语言 1.9 版本引入的优化的，引入的目的是保证互斥锁的公平性（Fairness）。 在饥饿模式中，互斥锁会直接交给等待队列最前面的 Goroutine。新的 Goroutine 在该状态下不能获取锁、也不会进入自旋状态，它们只会在队列的末尾等待。 如果一个 Goroutine 获得了互斥锁并且它在队列的末尾或者它等待的时间少于 1ms，那么当前的互斥锁就会被切换回正常模式。 相比于饥饿模式，正常模式下的互斥锁能够提供更好地性能，饥饿模式的能避免 Goroutine 由于陷入等待无法获取锁而造成的高尾延时。 互斥锁的加锁是靠 sync.Mutex.Lock 方法完成的, 当锁的状态是 0 时，将 mutexLocked 位置成 1： // Lock locks m. // If the lock is already in use, the calling goroutine // blocks until the mutex is available. func (m *Mutex) Lock() { // Fast path: grab unlocked mutex. if atomic.CompareAndSwapInt32(\u0026m.state, 0, mutexLocked) { if race.Enabled { race.Acquire(unsafe.Pointer(m)) } return } // Slow path (outlined so that the fast path can be inlined) m.lockSlow() } 如果互斥锁的状态不是 0 时就会调用 sync.Mutex.lockSlow 尝试通过自旋（Spinnig）等方式等待锁的释放， 这个方法是一个非常大 for 循环,它获取锁的过程： 判断当前 Goroutine 能否进入自旋； 通过自旋等待互斥锁的释放； 计算互斥锁的最新状态； 更新互斥锁的状态并获取锁； 那么互斥锁是如何判断当前 Goroutine 能否进入自旋等互斥锁的释放,是通过它的lockSlow方法, 由于自旋是一种多线程同步机制，所以呢当前的进程在进入自旋的过程中会一直保持对 CPU 的占用，持续检查某个条件是否为真。 通常在多核的 CPU 上，自旋可以避免 Goroutine 的切换，使用得当会对性能带来很大的增益，但是往往使用的不得当就会拖慢整个程序. 所以 Goroutine 进入自旋的条件非常苛刻： 互斥锁只有在普通模式才能进入自旋； runtime.sync_runtime_canSpin 需要返回 true： a. 需要运行在多 CPU 的机器上； b. 当前的Goroutine 为了获取该锁进入自旋的次数小于四次； c. 当前机器上至少存在一个正在运行的处理器 P 并且处理的运行队列为空； 一旦当前 Goroutine 能够进入自旋就会调用runtime.sync_runtime_doSpin 和 runtime.procyield 并执行 30 次的 PAUSE 指令，该指令只会占用 CPU 并消耗 CPU 时间. 处理了自旋相关的特殊逻辑之后，互斥锁会根据上下文计算当前互斥锁最新的状态。 通过几个不同的条件分别会更新 state 字段中存储的不同信息,mutexLocked、mutexStarving、mutexWoken 和 mutexWaiterShift： new := old if old\u0026mutexStarving == 0 { new |= mutexLocked } if old\u0026(mutexLocked|mutexStarving) != 0 { new += 1 \u003c\u003c mutexWaiterShift } if starving \u0026\u0026 old\u0026mutexLocked != 0 { new |= mutexStarving } if awoke { new \u0026^= mutexWoken } 计算了新的互斥锁状态之后，就会使用 CAS 函数 sync/atomic.CompareAndSwapInt32 更新该状态： if atomic.CompareAndSwapInt32(\u0026m.state, old, new) { if old\u0026(mutexLocked|mutexStarving) == 0 { break // 通过 CAS 函数获取了锁 } ... runtime_SemacquireMutex(\u0026m.sema, queueLifo, 1) starving = starving || runtime_nanotime()-waitStartTime \u003e starvationThresholdNs old = m.state if old\u0026mutexStarving != 0 { delta := int32(mutexLocked - 1\u003c\u003cmutexWaiterShift) if !starving || old\u003e\u003emutexWaiterShift == 1 { delta -= mutexStarving } atomic.AddInt32(\u0026m.state, delta) break } awoke = true iter = 0 } else { old = m.state } } } 如果我们没有通过 CAS 获得锁，会调用 runtime.sync_runtime_SemacquireMutex 使用信号量保证资源不会被两个 Goroutine 获取。 runtime.sync_runtime_SemacquireMutex 会在方法中不断调用尝试获取锁并休眠当前 Goroutine 等待信号量的释放，一旦当前 Goroutine 可以获取信号量，它就会立刻返回，sync.Mutex.Lock 方法的剩余代码也会继续执行。 在正常模式下，这段代码会设置唤醒和饥饿标记、重置迭代次数并重新执行获取锁的循环. 在饥饿模式下，当前 Goroutine 会获得互斥锁，如果等待队列中只存在当前 Goroutine，互斥锁还会从饥饿模式中退出. 互斥锁的解锁过程 sync.Mutex.Unlock 与加锁过程相比就很简单，该过程会先使用 sync/atomic.AddInt32 函数快速解锁，这时会发生下面的两种情况： 如果该函数返回的新状态等于 0，当前 Goroutine 就成功解锁了互斥锁； 如果该函数返回的新状态不等于 0，这段代码会调用 sync.Mutex.unlockSlow 方法开始慢速解锁： func (m *Mutex) Unlock() { if race.Enabled { _ = m.state race.Release(unsafe.Pointer(m)) } // Fast path: drop lock bit. new := atomic.Ad","date":"2022-10-09","objectID":"/go-interview-summary/:2:38","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中的channel的实现 在Go中最常见的就是通信顺序进程（Communicating sequential processes，CSP）的并发模型,通过共享通信,来实现共享内存,这里就提到了channel. Goroutine 和 Channel 分别对应 CSP 中的实体和传递信息的媒介，Go 语言中的 Goroutine 会通过 Channel 传递数据。 Goroutine通过使用channel传递数据，一个会向 Channel 中发送数据，另一个会从 Channel 中接收数据，它们两者能够独立运行并不存在直接关联，但是能通过 Channel 间接完成通信。 Channel 收发操作均遵循了先入先出（FIFO）的设计，具体规则如下： 先从 Channel 读取数据的 Goroutine 会先接收到数据； 先向 Channel 发送数据的 Goroutine 会得到先发送数据的权利； Channel 通常会有以下三种类型： 同步 Channel — 不需要缓冲区，发送方会直接将数据交给（Handoff）接收方； 异步 Channel — 基于环形缓存的传统生产者消费者模型； chan struct{} 类型的异步 Channel 的 struct{} 类型不占用内存空间，不需要实现缓冲区和直接发送（Handoff）的语义； Channel 在运行时使用 runtime.hchan 结构体表示: type hchan struct { qcount uint // 当前队列里还剩余元素个数 dataqsiz uint // 环形队列长度，即缓冲区的大小，即make(chan T,N) 中的N buf unsafe.Pointer // 环形队列指针 elemsize uint16 // 每个元素的大小 closed uint32 // 标识当前通道是否处于关闭状态，创建通道后，该字段设置0，即打开通道；通道调用close将其设置为1，通道关闭 elemtype *_type // 元素类型，用于数据传递过程中的赋值 sendx uint // 环形缓冲区的状态字段，它只是缓冲区的当前索引-支持数组，它可以从中发送数据 recvx uint // 环形缓冲区的状态字段，它只是缓冲区当前索引-支持数组，它可以从中接受数据 recvq waitq // 等待读消息的goroutine队列 sendq waitq // 等待写消息的goroutine队列 // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel. // // Do not change another G's status while holding this lock // (in particular, do not ready a G), as this can deadlock // with stack shrinking. lock mutex // 互斥锁，为每个读写操作锁定通道，因为发送和接受必须是互斥操作 } type waitq struct { first *sudog last *sudog } 其中hchan结构体中有五个字段是构建底层的循环队列: * qcount — Channel 中的元素个数； * dataqsiz — Channel 中的循环队列的长度； * buf — Channel 的缓冲区数据指针； * sendx — Channel 的发送操作处理到的位置； * recvx — Channel 的接收操作处理到的位置； 通常, elemsize 和 elemtype 分别表示当前 Channel 能够收发的元素类型和大小. sendq 和 recvq 存储了当前 Channel 由于缓冲区空间不足而阻塞的 Goroutine 列表，这些等待队列使用双向链表runtime.waitq表示，链表中所有的元素都是runtime.sudog结构. waitq 表示一个在等待列表中的 Goroutine，该结构体中存储了阻塞的相关信息以及两个分别指向前后runtime.sudog的指针。 channel 在Go中是通过make关键字创建,编译器会将make(chan int,10). 创建管道: runtime.makechan 和 runtime.makechan64 会根据传入的参数类型和缓冲区大小创建一个新的 Channel 结构，其中后者用于处理缓冲区大小大于 2 的 32 次方的情况. 这里我们来详细看下makechan 函数: func makechan(t *chantype, size int) *hchan { elem := t.elem // compiler checks this but be safe. if elem.size \u003e= 1\u003c\u003c16 { throw(\"makechan: invalid channel element type\") } if hchanSize%maxAlign != 0 || elem.align \u003e maxAlign { throw(\"makechan: bad alignment\") } mem, overflow := math.MulUintptr(elem.size, uintptr(size)) if overflow || mem \u003e maxAlloc-hchanSize || size \u003c 0 { panic(plainError(\"makechan: size out of range\")) } // Hchan does not contain pointers interesting for GC when elements stored in buf do not contain pointers. // buf points into the same allocation, elemtype is persistent. // SudoG's are referenced from their owning thread so they can't be collected. // TODO(dvyukov,rlh): Rethink when collector can move allocated objects. var c *hchan switch { case mem == 0: // Queue or element size is zero. c = (*hchan)(mallocgc(hchanSize, nil, true)) // Race detector uses this location for synchronization. c.buf = c.raceaddr() case elem.ptrdata == 0: // Elements do not contain pointers. // Allocate hchan and buf in one call. c = (*hchan)(mallocgc(hchanSize+mem, nil, true)) c.buf = add(unsafe.Pointer(c), hchanSize) default: // Elements contain pointers. c = new(hchan) c.buf = mallocgc(mem, elem, true) } c.elemsize = uint16(elem.size) c.elemtype = elem c.dataqsiz = uint(size) lockInit(\u0026c.lock, lockRankHchan) if debugChan { print(\"makechan: chan=\", c, \"; elemsize=\", elem.size, \"; dataqsiz=\", size, \"\\n\") } return c } Channel 中根据收发元素的类型和缓冲区的大小初始化 runtime.hchan 结构体和缓冲区： arena区域就是我们所谓的堆区，Go动态分配的内存都是在这个区域，它把内存分割成8KB大小的页，一些页组合起来称为mspan。 bitmap区域标识arena区域哪些地址保存了对象，并且用4bit标志位表示对象是否包含指针、GC标记信息。bitmap中一个byte大小的内存对应arena区域中4个指针大小（指针大小为 8B ）的内存，所以bitmap区域的大小是512GB/(4*8B)=16GB。 此外我们还可以看到bitmap的高地址部分指向arena区域的低地址部分，这里bitmap的地址是由高地址向低地址增长的。 spans区域存放mspan（是一些arena分割的页组合起来的内存管理基本单元，后文会再讲）的指针，每个指针对应一页，所以spans区域的大小就是512GB/8KB*8B=512MB。 除以8KB是计算arena区域的页数，而最后乘以8是计算spans区域所有指针的大小。创建mspan的时候，按页填充对应的spans区域，在回收object时，根据地址很容易就能找到它所属的mspan。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:39","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"栈的内存是怎么分配的 栈和堆只是虚拟内存上2块不同功能的内存区域： 栈在高地址，从高地址向低地址增长。 堆在低地址，从低地址向高地址增长。 栈和堆相比优势： 栈的内存管理简单，分配比堆上快。 栈的内存不需要回收，而堆需要，无论是主动free，还是被动的垃圾回收，这都需要花费额外的CPU。 栈上的内存有更好的局部性，堆上内存访问就不那么友好了，CPU访问的2块数据可能在不同的页上，CPU访问数据的时间可能就上去了。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:40","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"堆内存管理怎么分配的 通常在Golang中,当我们谈论内存管理的时候，主要是指堆内存的管理，因为栈的内存管理不需要程序去操心。 堆内存管理中主要是三部分, 1.分配内存块，2.回收内存块, 3.组织内存块。 一个内存块包含了3类信息，如下图所示，元数据、用户数据和对齐字段，内存对齐是为了提高访问效率。下图申请5Byte内存的时候，就需要进行内存对齐。 释放内存实质是把使用的内存块从链表中取出来，然后标记为未使用，当分配内存块的时候，可以从未使用内存块中有先查找大小相近的内存块，如果找不到，再从未分配的内存中分配内存。 上面这个简单的设计中还没考虑内存碎片的问题，因为随着内存不断的申请和释放，内存上会存在大量的碎片，降低内存的使用率。为了解决内存碎片，可以将2个连续的未使用的内存块合并，减少碎片。 想要深入了解可以看下这个文章,《Writing a Memory Allocator》. ","date":"2022-10-09","objectID":"/go-interview-summary/:2:41","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中的defer函数使用下面的两种情况下结果是什么 我们看看下面两种defer函数的返回的是什么: a := 1 defer fmt.Println(\"the value of a1:\",a) a++ defer func() { fmt.Println(\"the value of a2:\",a) }() 运行: the value of a1: 1 the value of a1: 2 第一种情况： defer fmt.Println(\"the value of a1:\",a) defer延迟函数调用的fmt.Println(a)函数的参数值在defer语句出现时就已经确定了，所以无论后面如何修改a变量都不会影响延迟函数。 第二种情况: defer func() { fmt.Println(\"the value of a2:\",a) }() defer延迟函数调用的函数参数的值在defer定义时候就确定了，而defer延迟函数内部所使用的值需要在这个函数运行时候才确定。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:42","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"在Go函数中为什么会发生内存泄露 通常内存泄漏，指的是能够预期的能很快被释放的内存由于附着在了长期存活的内存上、或生命期意外地被延长，导致预计能够立即回收的内存而长时间得不到回收。 在 Go 中，由于 goroutine 的存在，因此,内存泄漏除了附着在长期对象上之外，还存在多种不同的形式。 预期能被快速释放的内存因被根对象引用而没有得到迅速释放. 当有一个全局对象时，可能不经意间将某个变量附着在其上，且忽略的将其进行释放，则该内存永远不会得到释放。 goroutine 泄漏 Goroutine 作为一种逻辑上理解的轻量级线程，需要维护执行用户代码的上下文信息。在运行过程中也需要消耗一定的内存来保存这类信息，而这些内存在目前版本的 Go 中是不会被释放的。 因此，如果一个程序持续不断地产生新的 goroutine、且不结束已经创建的 goroutine 并复用这部分内存，就会造成内存泄漏的现象. 例如: func main() { for i := 0; i \u003c 10000; i++ { go func() { select {} }() } } ","date":"2022-10-09","objectID":"/go-interview-summary/:2:43","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中new和make的区别 在Go中,的值类型和引用类型: 值类型：int，float，bool，string，struct和array. 变量直接存储值，分配栈区的内存空间，这些变量所占据的空间在函数被调用完后会自动释放。 引用类型：slice，map，chan和值类型对应的指针. 变量存储的是一个地址（或者理解为指针），指针指向内存中真正存储数据的首地址。内存通常在堆上分配，通过GC回收。 这里需要注意的是: 对于引用类型的变量，我们不仅要声明变量，更重要的是，我们得手动为它分配空间. 因此new该方法的参数要求传入一个类型，而不是一个值，它会申请一个该类型大小的内存空间，并会初始化为对应的零值，返回指向该内存空间的一个指针。 // The new built-in function allocates memory. The first argument is a type, // not a value, and the value returned is a pointer to a newly // allocated zero value of that type. func new(Type) *Type 而make也是用于内存分配，但是和new不同，只用来引用对象slice、map和channel的内存创建，它返回的类型就是类型本身，而不是它们的指针类型。 // The make built-in function allocates and initializes an object of type // slice, map, or chan (only). Like new, the first argument is a type, not a // value. Unlike new, make's return type is the same as the type of its // argument, not a pointer to it. The specification of the result depends on // the type: // Slice: The size specifies the length. The capacity of the slice is // equal to its length. A second integer argument may be provided to // specify a different capacity; it must be no smaller than the // length. For example, make([]int, 0, 10) allocates an underlying array // of size 10 and returns a slice of length 0 and capacity 10 that is // backed by this underlying array. // Map: An empty map is allocated with enough space to hold the // specified number of elements. The size may be omitted, in which case // a small starting size is allocated. // Channel: The channel's buffer is initialized with the specified // buffer capacity. If zero, or the size is omitted, the channel is // unbuffered. func make(t Type, size ...IntegerType) Type 如果当前 Channel 中不存在缓冲区，那么就只会为 hchan 分配一段内存空间. 如果当前 Channel 中存储的类型不是指针类型，就会为当前的 Channel 和底层的数组分配一块连续的内存空间. 在默认情况下会单独为 hchan 和缓冲区分配内存. 发送数据: 当我们想要向 Channel 发送数据时，就需要使用 ch \u003c- i 语句. runtime.chansend1 调用了 runtime.chansend 并传入 Channel 和需要发送的数据。 runtime.chansend 是向 Channel 中发送数据时最终会调用的函数，这个函数负责了发送数据的全部逻辑，如果我们在调用时将 block 参数设置成 true，那么就表示当前发送操作是一个阻塞操作： func chansend(c *hchan, ep unsafe.Pointer, block bool, callerpc uintptr) bool { ... if !block \u0026\u0026 c.closed == 0 \u0026\u0026 full(c) { return false } var t0 int64 if blockprofilerate \u003e 0 { t0 = cputicks() } lock(\u0026c.lock) if c.closed != 0 { unlock(\u0026c.lock) panic(plainError(\"send on closed channel\")) } if sg := c.recvq.dequeue(); sg != nil { // Found a waiting receiver. We pass the value we want to send // directly to the receiver, bypassing the channel buffer (if any). send(c, sg, ep, func() { unlock(\u0026c.lock) }, 3) return true } ... } 在发送数据的逻辑执行之前会先为当前 Channel 加锁，防止发生竞争条件。如果 Channel 已经关闭，那么向该 Channel 发送数据时就会报\"send on closed channel\" 错误并中止程序。 因为 runtime.chansend 函数的实现比较复杂，所以我们这里将该函数的执行过程分成以下的三个部分： 当存在等待的接收者时，通过 runtime.send 直接将数据发送给阻塞的接收者. 当缓冲区存在空余空间时，将发送的数据写入 Channel 的缓冲区. 当不存在缓冲区或者缓冲区已满时，等待其他 Goroutine 从 Channel 接收数据. 因此: 当我们使用 ch \u003c- i 表达式向 Channel 发送数据时遇到的几种情况： 如果当前 Channel 的 recvq 上存在已经被阻塞的 Goroutine，那么会直接将数据发送给当前的 Goroutine 并将其设置成下一个运行的 Goroutine； 如果 Channel 存在缓冲区并且其中还有空闲的容量，我们就会直接将数据直接存储到当前缓冲区 sendx 所在的位置上； 如果不满足上面的两种情况，就会创建一个 runtime.sudog 结构并将其加入 Channel 的 sendq 队列中，当前 Goroutine 也会陷入阻塞等待其他的协程从 Channel 接收数据； 发送数据的过程中可能包含几个会触发 Goroutine 调度的时机： 发送数据时发现 Channel 上存在等待接收数据的 Goroutine，立刻设置处理器的 runnext 属性，但是并不会立刻触发调度. 发送数据时并没有找到接收方并且缓冲区已经满了，这时就会将自己加入 Channel 的 sendq 队列并调用 runtime.goparkunlock 触发 Goroutine 的调度让出处理器的使用权. 接收数据: 接着我们看看接受数据,Go中可以使用两种不同的方式去接收 Channel 中的数据： * i \u003c- ch * i, ok \u003c- ch 虽然不同的接收方式会被转换成 runtime.chanrecv1 和 runtime.chanrecv2 两种不同函数的调用，但是这两个函数最终还是会调用 runtime.chanrecv。 当我们从一个空 Channel 接收数据时会直接调用 runtime.gopark 直接让出处理器的使用权。 func chanrecv(c *hchan, ep unsafe.Pointer, block bool) (selected, received bool) { ... if c == nil { if !block { return } gopark(nil, nil, waitReasonChanReceiveNilChan, traceEvGoStop, 2) throw(\"unreachable\") } lock(\u0026c.lock) if c.closed != 0 \u0026\u0026 c.qcount == 0 { if raceenabled { raceacquire(c.raceaddr()) } unlock(\u0026c.lock) if ep != nil { typedmemclr(c.elemtype, ep) } return true, false } ... } 如果当前 Channel 已经被关闭并且缓冲区中不存在任何的数据，那么就会清除 ep 指针中的数据","date":"2022-10-09","objectID":"/go-interview-summary/:2:44","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中的map的实现 Go中Map是一个KV对集合。底层使用hash table，用链表来解决冲突 ，出现冲突时，不是每一个Key都申请一个结构通过链表串起来，而是以bmap为最小粒度挂载，一个bmap可以放8个kv。 在哈希函数的选择上，会在程序启动时，检测 cpu 是否支持 aes，如果支持，则使用aes hash，否则使用memhash。 hash函数,有加密型和非加密型。加密型的一般用于加密数据、数字摘要等，典型代表就是md5、sha1、sha256、aes256 这种,非加密型的一般就是查找。 在map的应用场景中，用的是查找。 选择hash函数主要考察的是两点：性能、碰撞概率。 每个map的底层结构是hmap，是有若干个结构为bmap的bucket组成的数组。每个bucket底层都采用链表结构。 type hmap struct { count int // 元素个数 flags uint8 // 用来标记状态 B uint8 // 扩容常量相关字段B是buckets数组的长度的对数 2^B noverflow uint16 // noverflow是溢出桶的数量，当B\u003c16时，为精确值,当B\u003e=16时，为估计值 hash0 uint32 // 是哈希的种子，它能为哈希函数的结果引入随机性，这个值在创建哈希表时确定，并在调用哈希函数时作为参数传入 buckets unsafe.Pointer // 桶的地址 oldbuckets unsafe.Pointer // 旧桶的地址，用于扩容 nevacuate uintptr // 搬迁进度，扩容需要将旧数据搬迁至新数据，这里是利用指针来比较判断有没有迁移 extra *mapextra // 用于扩容的指针 } type mapextra struct { overflow *[]*bmap oldoverflow *[]*bmap // nextOverflow holds a pointer to a free overflow bucket. nextOverflow *bmap } // A bucket for a Go map. type bmap struct { tophash [bucketCnt]uint8 // tophash用于记录8个key哈希值的高8位，这样在寻找对应key的时候可以更快，不必每次都对key做全等判断 } //实际上编辑期间会动态生成一个新的结构体 type bmap struct { topbits [8]uint8 keys [8]keytype values [8]valuetype pad uintptr overflow uintptr } bmap 就是我们常说的“桶”，桶里面会最多装 8 个 key，这些 key之所以会落入同一个桶，是因为它们经过哈希计算后，哈希结果是“一类”的，关于key的定位我们在map的查询和赋值中详细说明。 在桶内，又会根据key计算出来的hash值的高8位来决定 key到底落入桶内的哪个位置（一个桶内最多有8个位置)。 当map的key和value都不是指针，并且 size都小于128字节的情况下，会把bmap标记为不含指针，这样可以避免gc时扫描整个hmap。 但是，我们看bmap其实有一个overflow的字段，是指针类型的，破坏了 bmap 不含指针的设想，这时会把overflow移动到 hmap的extra 字段来。 这样随着哈希表存储的数据逐渐增多，我们会扩容哈希表或者使用额外的桶存储溢出的数据，不会让单个桶中的数据超过 8 个，不过溢出桶只是临时的解决方案，创建过多的溢出桶最终也会导致哈希的扩容。 哈希表作为一种数据结构，我们肯定要分析它的常见操作，首先就是读写操作的原理。哈希表的访问一般都是通过下标或者遍历进行的： _ = hash[key] for k, v := range hash { // k, v } 这两种方式虽然都能读取哈希表的数据，但是使用的函数和底层原理完全不同。 第一个需要知道哈希的键并且一次只能获取单个键对应的值，而第二个可以遍历哈希中的全部键值对，访问数据时也不需要预先知道哈希的键。 在编译的类型检查期间，hash[key] 以及类似的操作都会被转换成哈希的 OINDEXMAP 操作，中间代码生成阶段会在 cmd/compile/internal/gc.walkexpr 函数中将这些 OINDEXMAP 操作转换成如下的代码： v := hash[key] // =\u003e v := *mapaccess1(maptype, hash, \u0026key) v, ok := hash[key] // =\u003e v, ok := mapaccess2(maptype, hash, \u0026key) 这里根据赋值语句左侧接受参数的个数会决定使用的运行时方法： 当接受一个参数时，会使用 runtime.mapaccess1，该函数仅会返回一个指向目标值的指针； 当接受两个参数时，会使用 runtime.mapaccess2，除了返回目标值之外，它还会返回一个用于表示当前键对应的值是否存在的 bool 值： mapaccess1 会先通过哈希表设置的哈希函数、种子获取当前键对应的哈希，再通过 runtime.bucketMask 和 runtime.add 拿到该键值对所在的桶序号和哈希高位的 8 位数字。 如果在bucket中没有找到，此时如果overflow不为空，那么就沿着overflow继续查找，如果还是没有找到，那就从别的key槽位查找，直到遍历所有bucket。 func mapaccess1(t *maptype, h *hmap, key unsafe.Pointer) unsafe.Pointer { if raceenabled \u0026\u0026 h != nil { callerpc := getcallerpc() pc := funcPC(mapaccess1) racereadpc(unsafe.Pointer(h), callerpc, pc) raceReadObjectPC(t.key, key, callerpc, pc) } if msanenabled \u0026\u0026 h != nil { msanread(key, t.key.size) } //如果h说明都没有，返回零值 if h == nil || h.count == 0 { if t.hashMightPanic() { //如果哈希函数出错 t.key.alg.hash(key, 0) // see issue 23734 } return unsafe.Pointer(\u0026zeroVal[0]) } //写和读冲突 if h.flags\u0026hashWriting != 0 { throw(\"concurrent map read and map write\") } //不同类型的key需要不同的hash算法需要在编译期间确定 alg := t.key.alg //利用hash0引入随机性，计算哈希值 hash := alg.hash(key, uintptr(h.hash0)) //比如B=5那m就是31二进制是全1， //求bucket num时，将hash与m相与， //达到bucket num由hash的低8位决定的效果， //bucketMask函数掩蔽了移位量，省略了溢出检查。 m := bucketMask(h.B) //b即bucket的地址 b := (*bmap)(add(h.buckets, (hash\u0026m)*uintptr(t.bucketsize))) // oldbuckets 不为 nil，说明发生了扩容 if c := h.oldbuckets; c != nil { if !h.sameSizeGrow() { //新的bucket是旧的bucket两倍 m \u003e\u003e= 1 } //求出key在旧的bucket中的位置 oldb := (*bmap)(add(c, (hash\u0026m)*uintptr(t.bucketsize))) //如果旧的bucket还没有搬迁到新的bucket中，那就在老的bucket中寻找 if !evacuated(oldb) { b = oldb } } //计算tophash高8位 top := tophash(hash) bucketloop: //遍历所有overflow里面的bucket for ; b != nil; b = b.overflow(t) { //遍历8个bucket for i := uintptr(0); i \u003c bucketCnt; i++ { //tophash不匹配，继续 if b.tophash[i] != top { if b.tophash[i] == emptyRest { break bucketloop } continue } //tophash匹配，定位到key的位置 k := add(unsafe.Pointer(b), dataOffset+i*uintptr(t.keysize)) //若key为指针 if t.indirectkey() { //解引用 k = *((*unsafe.Pointer)(k)) } //key相等 if alg.equal(key, k) { //定位value的位置 e := add(unsafe.Pointer(b), dataOffset+bucketCnt*uintptr","date":"2022-10-09","objectID":"/go-interview-summary/:2:45","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中的http包的实现原理 Golang中http包中处理 HTTP 请求主要跟两个东西相关：ServeMux 和 Handler。 ServeMux 本质上是一个 HTTP 请求路由器（或者叫多路复用器，Multiplexor）。它把收到的请求与一组预先定义的 URL 路径列表做对比，然后在匹配到路径的时候调用关联的处理器（Handler）。 处理器（Handler）负责输出HTTP响应的头和正文。任何满足了http.Handler接口的对象都可作为一个处理器。通俗的说，对象只要有个如下签名的ServeHTTP方法即可： ServeHTTP(http.ResponseWriter, *http.Request) Go 语言的 HTTP 包自带了几个函数用作常用处理器，比如FileServer，NotFoundHandler 和 RedirectHandler。 应用示例: package main import ( \"log\" \"net/http\" ) func main() { mux := http.NewServeMux() rh := http.RedirectHandler(\"http://www.baidu.com\", 307) mux.Handle(\"/foo\", rh) log.Println(\"Listening...\") http.ListenAndServe(\":3000\", mux) } 在这个应用示例中,首先在 main 函数中我们只用了 http.NewServeMux 函数来创建一个空的 ServeMux。 然后我们使用 http.RedirectHandler 函数创建了一个新的处理器，这个处理器会对收到的所有请求，都执行307重定向操作到 http://www.baidu.com。 接下来我们使用 ServeMux.Handle 函数将处理器注册到新创建的 ServeMux，所以它在 URL 路径/foo 上收到所有的请求都交给这个处理器。 最后我们创建了一个新的服务器，并通过 http.ListenAndServe 函数监听所有进入的请求，通过传递刚才创建的 ServeMux来为请求去匹配对应处理器。 在浏览器中访问 http://localhost:3000/foo，你应该能发现请求已经成功的重定向了。 此刻你应该能注意到一些有意思的事情：ListenAndServer 的函数签名是 ListenAndServe(addr string, handler Handler) ，但是第二个参数我们传递的是个ServeMux。 通过这个例子我们就可以知道,net/http包在编写golang web应用中有很重要的作用，它主要提供了基于HTTP协议进行工作的client实现和server实现，可用于编写HTTP服务端和客户端。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:46","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Goroutine发生了泄漏如何检测 通常内存泄漏，指的是能够预期的能很快被释放的内存由于附着在了长期存活的内存上、或生命期意外地被延长，导致预计能够立即回收的内存而长时间得不到回收。 在 Go 中，由于Goroutine的存在，因此,内存泄漏除了附着在长期对象上之外，还存在多种不同的形式。 预期能被快速释放的内存因被根对象引用而没有得到迅速释放. 当有一个全局对象时，可能不经意间将某个变量附着在其上，且忽略的将其进行释放，则该内存永远不会得到释放。 Goroutine 泄漏 Goroutine 作为一种逻辑上理解的轻量级线程，需要维护执行用户代码的上下文信息。在运行过程中也需要消耗一定的内存来保存这类信息，而这些内存在目前版本的 Go 中是不会被释放的。 因此，如果一个程序持续不断地产生新的 goroutine、且不结束已经创建的 goroutine 并复用这部分内存，就会造成内存泄漏的现象. 可以通过Go自带的工具pprof或者使用Gops去检测诊断当前在系统上运行的Go进程的占用的资源. 例如: func main() { for i := 0; i \u003c 10000; i++ { go func() { select {} }() } } ","date":"2022-10-09","objectID":"/go-interview-summary/:2:47","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go函数返回局部变量的指针是否安全 在 Go 中是安全的，Go 编译器将会对每个局部变量进行逃逸分析。如果发现局部变量的作用域超出该函数，则不会将内存分配在栈上，而是分配在堆上 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:48","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Go中两个Nil可能不相等吗 Go中两个Nil可能不相等。 接口(interface) 是对非接口值(例如指针，struct等)的封装，内部实现包含 2 个字段，类型 T 和 值 V。一个接口等于 nil，当且仅当 T 和 V 处于 unset 状态（T=nil，V is unset）。 两个接口值比较时，会先比较 T，再比较 V。 接口值与非接口值比较时，会先将非接口值尝试转换为接口值，再比较。 func main() { var p *int = nil var i interface{} = p fmt.Println(i == p) // true fmt.Println(p == nil) // true fmt.Println(i == nil) // false } 这个例子中，将一个nil非接口值p赋值给接口i，此时,i的内部字段为(T=*int, V=nil)，i与p作比较时，将 p 转换为接口后再比较，因此 i == p，p 与 nil 比较，直接比较值，所以 p == nil。 但是当 i 与nil比较时，会将nil转换为接口(T=nil, V=nil),与i(T=*int, V=nil)不相等，因此 i != nil。因此 V 为 nil ，但 T 不为 nil 的接口不等于 nil。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:49","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Goroutine和KernelThread之间是什么关系 首先我们先看下进程和线程还有协程之间的区别: 进程 计算机的操作系统模式是一种多任务系统，操作系统接管了所有的硬件资源，并且本身运行在一个受硬件保护的级别。所有的应用程序都以进程(process)的方式运行在比操作系统权限更低的级别，每个进程都有自己独立的地址空间，使得进程之间的地址空间相互隔离。CPU由操作系统一进行分配，每个进程根据进程的优先级的高低都有机会得到CPU,但是如果允许时间超出了一定的时间，操作系统会暂停该进程，将CPU资源分配给其他等待的进程。这种CPU的分配方式即所谓的抢占式，操作系统可以强制剥夺CPU资源并且分配给它认为目前最需要的进程。如果操作系统分配给每个进程的时间都很短，即CPU在多个进程间快速地切换，从而造成了很多进程都在同时运行的假象。 线程 线程有时被称为轻量级进程（Lightweight Process）,是程序执行流的最小单元，一个标准的线程由线程ID,当前指令指针（PC）、寄存器集合和堆栈组成，通常意义上，一个进程🈶一个到多个线程组成，各个线程之间共享程序的内存空间（包括代码段、数据段、堆等）及一些进程级的资源（如打开文件和信号）。 协程 协程（coroutine）是Go语言中的轻量级线程实现，由Go运行时（runtime）管理。 进程、线程、协程的关系和区别： 进程拥有自己独立的堆和栈，既不共享堆，亦不共享栈，进程由操作系统调度。 线程拥有自己独立的栈和共享的堆，共享堆，不共享栈，线程亦由操作系统调度(标准线程是的)。 协程和线程一样共享堆，不共享栈，协程由程序开发者在协程的代码里显示调度。 为什么协程比线程轻量？ a. go协程调用跟切换比线程效率高. 线程并发执行流程: 线程是内核对外提供的服务，应用程序可以通过系统调用让内核启动线程，由内核来负责线程调度和切换。线程在等待IO操作时线程变为unrunnable状态会触发上下文切换。现代操作系统一般都采用抢占式调度，上下文切换一般发生在时钟中断和系统调用返回前，调度器计算当前线程的时间片，如果需要切换就从运行队列中选出一个目标线程，保存当前线程的环境，并且恢复目标线程的运行环境，最典型的就是切换ESP指向目标线程内核堆栈，将EIP指向目标线程上次被调度出时的指令地址。 go协程并发执行流程：不依赖操作系统和其提供的线程，golang自己实现的CSP并发模型实现：M, P, G .go协程也叫用户态线程，协程之间的切换发生在用户态。在用户态没有时钟中断，系统调用等机制,因此效率高 b. go协程占用内存少. 执行go协程只需要极少的栈内存（大概是4～5KB），默认情况下，线程栈的大小为1MB。goroutine就是一段代码，一个函数入口，以及在堆上为其分配的一个堆栈。所以它非常廉价，我们可以很轻松的创建上万个goroutine，但它们并不是被操作系统所调度执行。 因此协程和线程一样共享堆，不共享栈，协程由用户态下面的轻量级线程。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:50","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"为何GPM调度要有P 我们先看下go1.0源码当时是c实现的go的调度： static void schedule(G *gp) { ... schedlock(); if(gp != nil) { ... switch(gp-\u003estatus){ case Grunnable: case Gdead: // Shouldn't have been running! runtime·throw(\"bad gp-\u003estatus in sched\"); case Grunning: gp-\u003estatus = Grunnable; gput(gp); break; } gp = nextgandunlock(); gp-\u003ereadyonstop = 0; gp-\u003estatus = Grunning; m-\u003ecurg = gp; gp-\u003em = m; ... runtime·gogo(\u0026gp-\u003esched, 0); } 这里调度的作用: 调用 schedlock 方法来获取全局锁。 获取全局锁成功后，将当前 Goroutine 状态从 Running（正在被调度） 状态修改为 Runnable（可以被调度）状态。 调用 gput 方法来保存当前 Goroutine 的运行状态等信息，以便于后续的使用。 调用 nextgandunlock 方法来寻找下一个可运行 Goroutine，并且释放全局锁给其他调度使用。 获取到下一个待运行的 Goroutine 后，将其运行状态修改为 Running。 调用 runtime·gogo 方法，将刚刚所获取到的下一个待执行的 Goroutine 运行起来，进入下一轮调度。 GM 模型的缺点： Go1.0 的 GM 模型的 Goroutine 调度器限制了用 Go 编写的并发程序的可扩展性，尤其是高吞吐量服务器和并行计算程序。 GM调度存在的问题： 存在单一的全局 mutex（Sched.Lock）和集中状态管理： mutex 需要保护所有与 goroutine 相关的操作（创建、完成、重排等），导致锁竞争严重。 Goroutine 传递的问题： goroutine（G）交接（G.nextg）：工作者线程（M’s）之间会经常交接可运行的 goroutine。 而且可能会导致延迟增加和额外的开销。每个 M 必须能够执行任何可运行的 G，特别是刚刚创建 G 的 M。 每个 M 都需要做内存缓存（M.mcache）： 这样会导致资源消耗过大（每个 mcache 可以吸纳到 2M 的内存缓存和其他缓存），数据局部性差。 频繁的线程阻塞/解阻塞： 在存在 syscalls 的情况下，线程经常被阻塞和解阻塞。这增加了很多额外的性能开销。 为了解决 GM 模型的以上诸多问题，在 Go1.1 时，Dmitry Vyukov 在 GM 模型的基础上，新增了一个 P（Processor）组件。并且实现了 Work Stealing 算法来解决一些新产生的问题。 加了 P 之后会带来什么改变呢？ 每个 P 有自己的本地队列，大幅度的减轻了对全局队列的直接依赖，所带来的效果就是锁竞争的减少。而 GM 模型的性能开销大头就是锁竞争。 每个 P 相对的平衡上，在 GMP 模型中也实现了 Work Stealing 算法，如果 P 的本地队列为空，则会从全局队列或其他 P 的本地队列中窃取可运行的 G 来运行，减少空转，提高了资源利用率。 为什么要有P呢？ 一般来讲，M 的数量都会多于 P。像在 Go 中，M 的数量默认是10000，P 的默认数量的 CPU 核数。另外由于 M 的属性，也就是如果存在系统阻塞调用，阻塞了 M，又不够用的情况下，M 会不断增加。 M 不断增加的话，如果本地队列挂载在 M 上，那就意味着本地队列也会随之增加。这显然是不合理的，因为本地队列的管理会变得复杂，且 Work Stealing 性能会大幅度下降。 M 被系统调用阻塞后，我们是期望把他既有未执行的任务分配给其他继续运行的，而不是一阻塞就导致全部停止。 因此使用 M 是不合理的，那么引入新的组件 P，把本地队列关联到 P 上，就能很好的解决这个问题。 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:51","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"如何在goroutine执行一半就退出协程 在go中，调度时候也不是每个G都能一直处于运行状态，等G不能运行时，就把它存起来，再调度下一个能运行的G过来运行。暂时不能运行的G，P上会有个本地队列去存放这些这些G，P的本地队列存不下的话，还有个全局队列，干的事情也类似。 在这个这个背景后， 通过goexit0 观察，做的事情就是将当前的协程G置为_Gdead状态，然后把它从M上摘下来，尝试放回到P的本地队列中。然后重新调度一波，获取另一个能跑的G，拿出来跑。 因此只要执行 goexit 这个函数，当前协程就会退出，同时还能调度下一个可执行的协程出来跑。 通过 runtime.Goexit()可以做到提前结束协程，且结束前还能执行到defer的内容• runtime.Goexit()其实是对goexit0的封装，只要执行 goexit0 这个函数，当前协程就会退出，同时还能调度下一个可执行的协程出来跑 ","date":"2022-10-09","objectID":"/go-interview-summary/:2:52","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql基础知识 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:0","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql索引用的是什么算法 Mysql 索引选用的是B+树,平衡二叉树的高度太高,查找可能需要较多的磁盘IO。B树索引占用内存较高(非叶子节点存储数据)。 B+树, 主要是查询效率高，O(logN)，可以充分利用磁盘预读的特性，多叉树，深度小，叶子结点有序且存储数据. ","date":"2022-10-09","objectID":"/go-interview-summary/:3:1","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql事务的基本要素 原子性：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。 一致性：事务开始前和结束后，数据库的完整性约束没有被破坏。 隔离性：同一时间，只允许一个事务请求同一数据，不同的事务之间彼此没有任何干扰。 持久性：事务完成后，事务对数据库的所有更新将被保存到数据库，不能回滚。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:2","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql的存储引擎 InnoDB存储引擎 InnoDB存储引擎支持事务，其设计目标主要面向在线事务处理（OLTP）的应用。 其特点是行锁设计，支持外键，并支持非锁定锁，即默认读取操作不会产生锁。从Mysql5.5.8版本开始，InnoDB存储引擎是默认的存储引擎。 MyISAM存储引擎 MyISAM存储引擎不支持事务、表锁设计，支持全文索引，主要面向一些OLAP数据库应用。 InnoDB的数据文件本身就是主索引文件，而MyISAM的主索引和数据是分开的。 NDB存储引擎 NDB存储引擎是一个集群存储引擎，其结构是share nothing的集群架构，能提供更高的可用性。 NDB的特点是数据全部放在内存中（从MySQL 5.1版本开始，可以将非索引数据放在磁盘上），因此主键查找的速度极快，并且通过添加NDB数据存储节点可以线性地提高数据库性能，是高可用、高性能的集群系统。 NDB存储引擎的连接操作是在MySQL数据库层完成的，而不是在存储引擎层完成的。这意味着，复杂的连接操作需要巨大的网络开销，因此查询速度很慢。如果解决了这个问题，NDB存储引擎的市场应该是非常巨大的。 Memory存储引擎 Memory存储引擎（之前称HEAP存储引擎）将表中的数据存放在内存中，如果数据库重启或发生崩溃，表中的数据都将消失。 它非常适合用于存储临时数据的临时表，以及数据仓库中的纬度表。Memory存储引擎默认使用哈希索引，而不是我们熟悉的B+树索引。 虽然Memory存储引擎速度非常快，但在使用上还是有一定的限制。比如，只支持表锁，并发性能较差，并且不支持TEXT和BLOB列类型。最重要的是，存储变长字段时是按照定常字段的方式进行的，因此会浪费内存。 Archive存储引擎 Archive存储引擎只支持INSERT和SELECT操作，从MySQL 5.1开始支持索引。 Archive存储引擎使用zlib算法将数据行（row）进行压缩后存储，压缩比一般可达1∶10。正如其名字所示，Archive存储引擎非常适合存储归档数据，如日志信息。 Archive存储引擎使用行锁来实现高并发的插入操作，但是其本身并不是事务安全的存储引擎，其设计目标主要是提供高速的插入和压缩功能。 Maria存储引擎 Maria存储引擎是新开发的引擎，设计目标主要是用来取代原有的MyISAM存储引擎，从而成为MySQL的默认存储引擎。它可以看做是MyISAM的后续版本。 Maria存储引擎的特点是：支持缓存数据和索引文件，应用了行锁设计，提供了MVCC功能，支持事务和非事务安全的选项，以及更好的BLOB字符类型的处理性能。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:3","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql事务隔离级别 Mysql有四种事务隔离级别,默认的是可重复读. 事务隔离级别 脏读 不可重复读 幻读 读未提交 e 是 是 是 读已提交 否 是 是 可重复读 否 否 是 串行 否 否 否 读未提交(Read uncommitted) 一个事务可以读取另一个未提交事务的数据，最低级别，任何情况都无法保证。 (1)所有事务都可以看到其他未提交事务的执行结果 (2)本隔离级别很少用于实际应用，因为它的性能也不比其他级别好多少 (3)该级别引发的问题是——脏读(Dirty Read)：读取到了未提交的数据 读已提交(Read committed) 一个事务要等另一个事务提交后才能读取数据，可避免脏读的发生。 (1)这是大多数数据库系统的默认隔离级别（但不是MySQL默认的） (2)它满足了隔离的简单定义：一个事务只能看见已经提交事务所做的改变 (3)这种隔离级别出现的问题是——不可重复读(Nonrepeatable Read),不可重复读意味着我们在同一个事务中执行完全相同的select语句时可能看到不一样的结果。 导致这种情况的原因可能有： (1)有一个交叉的事务有新的commit，导致了数据的改变; (2)一个数据库被多个实例操作时,同一事务的其他实例在该实例处理其间可能会有新的commit. 可重复读(Repeatable read) 就是在开始读取数据（事务开启）时，不再允许修改操作，可避免脏读、不可重复读的发生。 (1)这是MySQL的默认事务隔离级别. (2)它确保同一事务的多个实例在并发读取数据时，会看到同样的数据行. (3)此级别可能出现的问题——幻读(Phantom Read)：当用户读取某一范围的数据行时，另一个事务又在该范围内插入了新行，当用户再读取该范围的数据行时，会发现有新的“幻影” 行. (4)InnoDB和Falcon存储引擎通过多版本并发控制(MVCC，Multiversion Concurrency Control)机制解决了该问题.InnoDB采用MVCC来支持高并发，实现了四个标准隔离级别。默认基本是可重复读，并且提供间隙锁（next-key locks）策略防止幻读出现。 串行(Serializable) 串行(Serializable)，是最高的事务隔离级别，在该级别下，事务串行化顺序执行，可以避免脏读、不可重复读与幻读。 但是这种事务隔离级别效率低下，比较耗数据库性能，一般不使用。Mysql的默认隔离级别是Repeatable read。 (1)这是最高的隔离级别. (2)它通过强制事务排序，使之不可能相互冲突，从而解决幻读问题。简言之,它是在每个读的数据行上加上共享锁。 (3)在这个级别，可能导致大量的超时现象和锁竞争. ","date":"2022-10-09","objectID":"/go-interview-summary/:3:4","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql高可用方案有哪些 Mysql高可用方案包括: 主从复制方案 这是MySQL自身提供的一种高可用解决方案，数据同步方法采用的是MySQL replication技术。MySQL replication就是从服务器到主服务器拉取二进制日志文件，然后再将日志文件解析成相应的SQL在从服务器上重新执行一遍主服务器的操作，通过这种方式保证数据的一致性。 为了达到更高的可用性，在实际的应用环境中，一般都是采用MySQL replication技术配合高可用集群软件keepalived来实现自动failover，这种方式可以实现95.000%的SLA。 MMM/MHA高可用方案 MMM提供了MySQL主主复制配置的监控、故障转移和管理的一套可伸缩的脚本套件。在MMM高可用方案中，典型的应用是双主多从架构，通过MySQL replication技术可以实现两个服务器互为主从，且在任何时候只有一个节点可以被写入，避免了多点写入的数据冲突。 同时，当可写的主节点故障时，MMM套件可以立刻监控到，然后将服务自动切换到另一个主节点，继续提供服务，从而实现MySQL的高可用。 Heartbeat/SAN高可用方案 在这个方案中，处理failover的方式是高可用集群软件Heartbeat，它监控和管理各个节点间连接的网络，并监控集群服务，当节点出现故障或者服务不可用时，自动在其他节点启动集群服务。 在数据共享方面，通过SAN（Storage Area Network）存储来共享数据，这种方案可以实现99.990%的SLA。 Heartbeat/DRBD高可用方案 这个方案处理failover的方式上依旧采用Heartbeat，不同的是，在数据共享方面，采用了基于块级别的数据同步软件DRBD来实现。 DRBD是一个用软件实现的、无共享的、服务器之间镜像块设备内容的存储复制解决方案。和SAN网络不同，它并不共享存储，而是通过服务器之间的网络复制数据。 NDB CLUSTER高可用方案 国内用NDB集群的公司非常少，貌似有些银行有用。NDB集群不需要依赖第三方组件，全部都使用官方组件，能保证数据的一致性，某个数据节点挂掉，其他数据节点依然可以提供服务，管理节点需要做冗余以防挂掉。 缺点是：管理和配置都很复杂，而且某些SQL语句例如join语句需要避免。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:5","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql中utf8和utf8mb4区别 MySQL在5.5.3之后增加了这个utf8mb4的编码，mb4就是most bytes 4的意思，专门用来兼容四字节的unicode。好在utf8mb4是utf8的超集，除了将编码改为utf8mb4外不需要做其他转换。当然，为了节省空间，一般情况下使用utf8也就可以了。 Mysql支持的 utf8 编码最大字符长度为 3 字节，如果遇到 4 字节的宽字符就会插入异常了。三个字节的 UTF-8 最大能编码的 Unicode 字符是 0xffff，也就是 Unicode 中的基本多文种平面(BMP)。任何不在基本多文本平面的 Unicode字符，都无法使用 Mysql 的 utf8 字符集存储。 包括 Emoji 表情(Emoji 是一种特殊的 Unicode 编码，常见于 ios 和 android 手机上)，和很多不常用的汉字，以及任何新增的 Unicode 字符等等。 Mysql 中保存 4 字节长度的 UTF-8 字符，需要使用utf8mb4 字符集，但只有5.5.3版本以后的才支持(查看版本： select version();)。因此呢，为了获取更好的兼容性，应该总是使用 utf8mb4 而非 utf8. 对于 CHAR 类型数据，utf8mb4 会多消耗一些空间，根据 Mysql 官方建议，使用 VARCHAR 替代 CHAR。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:6","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql中乐观锁和悲观锁区别 悲观锁(Pessimistic Lock) 悲观锁顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。 传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 乐观锁(Optimistic Lock), 乐观锁顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。 乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库如果提供类似于write_condition机制的其实都是提供的乐观锁。 乐观锁的特点先进行业务操作，不到万不得已不去拿锁。即“乐观”的认为拿锁多半是会成功的，因此在进行完业务操作需要实际更新数据的最后一步再去拿一下锁就好。 乐观锁在数据库上的实现完全是逻辑的，不需要数据库提供特殊的支持。一般的做法是在需要锁的数据上增加一个版本号，或者时间戳，然后按照如下方式实现： 1. SELECT data AS old_data, version AS old_version FROM …; 2. 根据获取的数据进行业务操作，得到new_data和new_version 3. UPDATE SET data = new_data, version = new_version WHERE version = old_version if (updated row \u003e 0) { // 乐观锁获取成功，操作完成 } else { // 乐观锁获取失败，回滚并重试 } 乐观锁是否在事务中其实都是无所谓的，其底层机制是这样：在数据库内部update同一行的时候是不允许并发的，即数据库每次执行一条update语句时会获取被update行的写锁，直到这一行被成功更新后才释放。 因此在业务操作进行前获取需要锁的数据的当前版本号，然后实际更新数据时再次对比版本号确认与之前获取的相同，并更新版本号，即可确认这之间没有发生并发的修改。如果更新失败即可认为老版本的数据已经被并发修改掉而不存在了，此时认为获取锁失败，需要回滚整个业务操作并可根据需要重试整个过程。 两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 但如果经常产生冲突，上层应用会不断的进行retry，这样反倒是降低了性能，所以这种情况下用悲观锁就比较合适。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:7","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql索引主要是哪些 索引的目的在于提高查询效率. 索引的类型： UNIQUE(唯一索引)：不可以出现相同的值，可以有NULL值 INDEX(普通索引)：允许出现相同的索引内容 PRIMARY KEY(主键索引)：不允许出现相同的值 fulltext index(全文索引)：可以针对值中的某个单词，但效率确实不敢恭维 组合索引：实质上是将多个字段建到一个索引里，列值的组合必须唯一 索引虽然好处很多，但过多的使用索引可能带来相反的问题，索引也是有缺点的： 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT,UPDATE和DELETE。因为更新表时，mysql不仅要保存数据，还要保存一下索引文件. 建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在要给大表上建了多种组合索引，索引文件会膨胀很宽, 索引只是提高效率的一个方式，如果mysql有大数据量的表，就要花时间研究建立最优的索引，或优化查询语句。 使用索引时，有一些技巧： 索引不会包含有NULL的列 只要列中包含有NULL值，都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此符合索引就是无效的。 使用短索引 对串列进行索引，如果可以就应该指定一个前缀长度。例如，如果有一个char（255）的列，如果在前10个或20个字符内，多数值是唯一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 索引列排序 mysql查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作，尽量不要包含多个列的排序，如果需要最好给这些列建复合索引。 like语句操作 一般情况下不鼓励使用like操作，如果非使用不可，注意正确的使用方式。like ‘%aaa%’不会使用索引，而like ‘aaa%’可以使用索引。 不要在列上进行运算 不使用NOT IN 、\u003c\u003e、！=操作，但\u003c,\u003c=，=，\u003e,\u003e=,BETWEEN,IN是可以用到索引的 索引要建立在经常进行select操作的字段上。 这是因为，如果这些列很少用到，那么有无索引并不能明显改变查询速度。相反，由于增加了索引，反而降低了系统的维护速度和增大了空间需求。 索引要建立在值比较唯一的字段上。 对于那些定义为text、image和bit数据类型的列不应该增加索引。因为这些列的数据量要么相当大，要么取值很少。 在where和join中出现的列需要建立索引。 where的查询条件里有不等号(where column != …),mysql将无法使用索引。 如果where字句的查询条件里使用了函数(如：where DAY(column)=…),mysql将无法使用索引。 在join操作中(需要从多个数据表提取数据时)，mysql只有在主键和外键的数据类型相同时才能使用索引，否则及时建立了索引也不会使用。 组合索引的作用: 减少开销。 建一个组合索引(col1,col2,col3)，实际相当于建了(col1),(col1,col2),(col1,col2,col3)三个索引。每多一个索引，都会增加写操作的开销和磁盘空间的开销。 对于大量数据的表，使用组合索引会大大的减少开销。 覆盖索引。 通常指一个查询语句的执行只用从索引中就能够取得，不必从数据表中读取。也可以称之为实现了索引覆盖。 对组合索引(col1,col2,col3)，如果有如下的sql: select col1,col2,col3 from test where col1=1 and col2=2。 那么MySQL可以直接通过遍历索引取得数据，而无需回表，这减少了很多的随机io操作。减少io操作，特别的随机io其实是dba主要的优化策略。 所以，在真正的实际应用中，覆盖索引是主要的提升性能的优化手段之一。 效率高。 索引列越多，通过索引筛选出的数据越快。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:8","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql联合索引最左匹配原则 最左前缀匹配原则： 在MySQL建立联合索引时会遵守最左前缀匹配原则，即最左优先. 在检索数据时从联合索引的最左边开始匹配，Mysql会一直向右匹配直到遇到范围查询（\u003e、\u003c、between、like）就停止匹配了. 就比如 a=3 and b=4 and c\u003e5 and d=6 如果建立(abcd)顺序的索引,d就用不到索引了，如果建立(abdc)的索引则都可以用到索引，其中abd的顺序可以任意调整，因为查询优化器会重新编排（即使是c\u003e5 and b=4 and d=6 and a=3也会全部用到 abdc索引 ）. =和in可以乱序，比如a=1 and b=2 and c=3 建立(abc)索引可以任意顺序，mysql查询优化器会优化顺序. 这里需要注意下, 比如abc索引 那么只要查询条件有a即可用到abc索引（如abc ab ac a）,没有a就用不到。 最左前缀匹配成因：Mysql是创建复合索引的规则是根据索引最左边的字段进行排序，在第一个字段排序的基础上再进行第二个字段排序，类似于order by col1，col2… 所以第一个字段是绝对有序的 第二个字段就是无序的了，所以Mysql 强调最左前缀匹配. ","date":"2022-10-09","objectID":"/go-interview-summary/:3:9","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"聚簇索引和非聚簇索引区别 聚簇索引与非聚簇索引的区别是：叶子节点是否存放一整行记录. InnoDB 主键使用的是聚簇索引，MyISAM 不管是主键索引，还是二级索引使用的都是非聚簇索引。 对于聚簇索引表来说（左图），表数据是和主键一起存储的，主键索引的叶结点存储行数据(包含了主键值)，二级索引的叶结点存储行的主键值。 使用的是B+树作为索引的存储结构，非叶子节点都是索引关键字，但非叶子节点中的关键字中不存储对应记录的具体内容或内容地址。叶子节点上的数据是主键与具体记录(数据内容)。 对于非聚簇索引表来说（右图），表数据和索引是分成两部分存储的，主键索引和二级索引存储上没有任何区别。使用的是B+树作为索引的存储结构，所有的节点都是索引，叶子节点存储的是索引+索引对应的记录的数据。 因此, 聚簇索引的叶子节点就是数据节点，而非聚簇索引的叶子节点仍然是索引节点，只不过有指向对应数据块的指针. ","date":"2022-10-09","objectID":"/go-interview-summary/:3:10","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"如何查询一个字段是否命中了索引 通过explain sql可看下SQL是否走了索引，很快对比出来 . 当一个sql中索引字段为int类型时，例如搜索条件where num=\"111\"与where num=111都可以使用该字段的索引. 当一个中索引字段为varchar类型时，例如搜索条件where num=\"111\"可以使用索引，where num=111不可以使用索引. ","date":"2022-10-09","objectID":"/go-interview-summary/:3:11","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql中查询数据什么情况下不会命中索引 通常不命中索引有接种情况: 索引规范不合理,sql解析器不命中索引. 表中索引是以表中数据量字段最多的建立的索引,sql解析器不命中索引.(实际就是索引没用,最后全局查找了) bool的字段做索引,sql选择器不命中索引. 模糊查询 %like 索引列参与计算,使用了函数 非最左前缀顺序 where对null判断 where不等于 or操作有至少一个字段没有索引 需要回表的查询结果集过大（超过配置的范围） ","date":"2022-10-09","objectID":"/go-interview-summary/:3:12","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql中的MVCC是什么 数据库并发控制——锁, Multi version (version) concurrency control (MCC or MVCC) 多版本并发控制 ，它是数据库管理系统一种常见的并发控制。 并发控制常用的是锁，当线程要对一个共享资源进行操作的时候，加锁是一种非常简单粗暴的方法(事务开始时给 DQL 加读锁，给 DML 加写锁)，这种锁是一种 悲观 的实现方式，也就是说这会给其他事务造成堵塞，从而影响数据库性能。 其中在数据库中最常见的就是悲观锁和乐观锁: 悲观锁 当一个线程需要对共享资源进行操作的时候，首先对共享资源进行加锁，当该线程持有该资源的锁的时候，其他线程对该资源进行操作的时候会被阻塞. 乐观锁 当一个线程需要对一个共享资源进行操作的时候，不对它进行加锁，而是在操作完成之后进行判断。 比如乐观锁会通过一个版本号控制，如果操作完成后通过版本号进行判断在该线程操作过程中是否有其他线程已经对该共享资源进行操作了，如果有则通知操作失败，如果没有则操作成功，当然除了版本号还有CAS，如果不了解的可以去学习一下，这里不做过多涉及。 MVCC的两种读形式: 快照读 读取的只是当前事务的可见版本，不用加锁。而你只要记住 简单的 select操作就是快照读(select * from table where id = xxx)。 当前读 读取的是当前版本，比如 特殊的读操作，更新/插入/删除操作. 比如： select * from table where xxx lock in share mode， select * from table where xxx for update， update table set.... insert into table (xxx,xxx) values (xxx,xxx) delete from table where id = xxx MVCC的实现原理: MVCC 使用了“三个隐藏字段”来实现版本并发控制，MySQL在创建建表的时候 innoDB 创建的真正的三个隐藏列吧。 RowID DB_TRX_ID DB_ROLL_PTR id name password 自动创建的id 事务id 回滚指针 id name password RowID：隐藏的自增ID，当建表没有指定主键，InnoDB会使用该RowID创建一个聚簇索引。 DB_TRX_ID：最近修改（更新/删除/插入）该记录的事务ID。 DB_ROLL_PTR：回滚指针，指向这条记录的上一个版本。 其实还有一个删除的flag字段，用来判断该行记录是否已经被删除。 而 MVCC 使用的是其中的 事务字段，回滚指针字段，是否删除字段。 我们来看一下现在的表格(isDelete是我自己取的，按照官方说法是在一行开头的content里面，这里其实位置无所谓，你只要知道有就行了)。 isDelete DB_TRX_ID DB_ROLL_PTR id name password true/false 事务id 回滚指针 id name password ","date":"2022-10-09","objectID":"/go-interview-summary/:3:13","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mvcc和Redolog和Undolog以及Binlog有什么不同 Mvcc MVCC多版本并发控制是MySQL中基于乐观锁理论实现隔离级别的方式，用于读已提交和可重复读取隔离级别的实现。在MySQL中，会在表中每一条数据后面添加两个字段,最近修改该行数据的事务ID，指向该行（undolog表中）回滚段的指针。 Read View判断行的可见性，创建一个新事务时，copy一份当前系统中的活跃事务列表。意思是，当前不应该被本事务看到的其他事务id列表。 UndoLog UndoLog也就是我们常说的回滚日志文件 主要用于事务中执行失败，进行回滚，以及MVCC中对于数据历史版本的查看。由引擎层的InnoDB引擎实现,是逻辑日志,记录数据修改被修改前的值,比如\"把id=‘B’ 修改为id = ‘B2’ ，那么undo日志就会用来存放id =‘B’的记录”。 当一条数据需要更新前,会先把修改前的记录存储在undolog中,如果这个修改出现异常,,则会使用undo日志来实现回滚操作,保证事务的一致性。当事务提交之后，undo log并不能立马被删除,而是会被放到待清理链表中,待判断没有事物用到该版本的信息时才可以清理相应undolog。它保存了事务发生之前的数据的一个版本，用于回滚，同时可以提供多版本并发控制下的读(MVCC)也即非锁定读。 Redolog Redolog是重做日志文件是记录数据修改之后的值，用于持久化到磁盘中。 Redolog包括两部分： 一. 是内存中的日志缓冲(redo log buffer)，该部分日志是易失性的； 二. 是磁盘上的重做日志文件(redo log file)，该部分日志是持久的。 由引擎层的InnoDB引擎实现,是物理日志,记录的是物理数据页修改的信息,比如“某个数据页上内容发生了哪些改动”。当一条数据需要更新时,InnoDB会先将数据更新，然后记录redoLog 在内存中，然后找个时间将redoLog的操作执行到磁盘上的文件上。 不管是否提交成功我都记录，你要是回滚了，那我连回滚的修改也记录。它确保了事务的持久性。 Binlog Binlog由Mysql的Server层实现,是逻辑日志,记录的是sql语句的原始逻辑，比如\"把id=‘B’ 修改为id = ‘B2’。 Binlog会写入指定大小的物理文件中,是追加写入的,当前文件写满则会创建新的文件写入。 产生:事务提交的时候,一次性将事务中的sql语句,按照一定的格式记录到binlog中。 用于复制和恢复在主从复制中，从库利用主库上的binlog进行重播(执行日志中记录的修改逻辑),实现主从同步。业务数据不一致或者错了，用binlog恢复。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:14","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql读写分离以及主从同步 原理：主库将变更写binlog日志，然后从库连接到主库后，从库有一个IO线程，将主库的binlog日志拷贝到自己本地，写入一个中继日志中，接着从库中有一个sql线程会从中继日志读取binlog，然后执行binlog日志中的内容，也就是在自己本地再执行一遍sql，这样就可以保证自己跟主库的数据一致。 问题：这里有很重要一点，就是从库同步主库数据的过程是串行化的，也就是说主库上并行操作，在从库上会串行化执行，由于从库从主库拷贝日志以及串行化执行sql特点，在高并发情况下，从库数据一定比主库慢一点，是有延时的，所以经常出现，刚写入主库的数据可能读不到了，要过几十毫秒，甚至几百毫秒才能读取到。还有一个问题，如果突然主库宕机了，然后恰巧数据还没有同步到从库，那么有些数据可能在从库上是没有的，有些数据可能就丢失了。所以mysql实际上有两个机制，一个是半同步复制，用来解决主库数据丢失问题，一个是并行复制，用来解决主从同步延时问题。 半同步复制：semi-sync复制，指的就是主库写入binlog日志后，就会将强制此时立即将数据同步到从库，从库将日志写入自己本地的relay log之后，接着会返回一个ack给主库，主库接收到至少一个从库ack之后才会认为写完成。 并发复制：指的是从库开启多个线程，并行读取relay log中不同库的日志，然后并行重放不同库的日志，这样库级别的并行。（将主库分库也可缓解延迟问题） ","date":"2022-10-09","objectID":"/go-interview-summary/:3:15","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"InnoDB的关键特性 插入缓冲：对于非聚集索引的插入或更新操作，不是每一次直接插入到索引页中，而是先判断插入的非聚集索引页是否在缓冲池中，若在，则直接插入；若不在，则先放入到一个Insert Buffer对象中。然后再以一定的频率和情况进行Insert Buffer和辅助索引页子节点的merge（合并）操作，这时通常能将多个插入合并到一个操作中（因为在一个索引页中），这就大大提高了对于非聚集索引插入的性能。 两次写：两次写带给InnoDB存储引擎的是数据页的可靠性，有经验的DBA也许会想，如果发生写失效，可以通过重做日志进行恢复。这是一个办法。但是必须清楚地认识到，如果这个页本身已经发生了损坏（物理到page页的物理日志成功页内逻辑日志失败），再对其进行重做是没有意义的。这就是说，在应用（apply）重做日志前，用户需要一个页的副本，当写入失效发生时，先通过页的副本来还原该页，再进行重做。在对缓冲池的脏页进行刷新时，并不直接写磁盘，而是会通过memcpy函数将脏页先复制到内存中的doublewrite buffer，之后通过doublewrite buffer再分两次，每次1MB顺序地写入共享表空间的物理磁盘上，这就是doublewrite。 自适应哈希索引：InnoDB存储引擎会监控对表上各索引页的查询。如果观察到建立哈希索引可以带来速度提升，则建立哈希索引，称之为自适应哈希索引。 异步IO：为了提高磁盘操作性能，当前的数据库系统都采用异步IO（AIO）的方式来处理磁盘操作。AIO的另一个优势是可以进行IO Merge操作，也就是将多个IO合并为1个IO，这样可以提高IOPS的性能。 刷新邻接页：当刷新一个脏页时，InnoDB存储引擎会检测该页所在区（extent）的所有页，如果是脏页，那么一起进行刷新。这样做的好处显而易见，通过AIO可以将多个IO写入操作合并为一个IO操作，故该工作机制在传统机械磁盘下有着显著的优势。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:16","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql如何保证一致性和持久性 Mysql为了保证ACID中的一致性和持久性，使用了WAL(Write-Ahead Logging,先写日志再写磁盘)。Redo log就是一种WAL的应用。 当数据库忽然掉电，再重新启动时，Mysql可以通过Redo log还原数据。也就是说，每次事务提交时，不用同步刷新磁盘数据文件，只需要同步刷新Redo log就足够了。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:17","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"为什么选择B+树作为索引结构 Hash索引：Hash索引底层是哈希表，哈希表是一种以key-value存储数据的结构，所以多个数据在存储关系上是完全没有任何顺序关系的，所以，对于区间查询是无法直接通过索引查询的，就需要全表扫描。所以，哈希索引只适用于等值查询的场景。而B+ 树是一种多路平衡查询树，所以他的节点是天然有序的（左子节点小于父节点、父节点小于右子节点），所以对于范围查询的时候不需要做全表扫描 二叉查找树：解决了排序的基本问题，但是由于无法保证平衡，可能退化为链表。 平衡二叉树：通过旋转解决了平衡的问题，但是旋转操作效率太低。 红黑树：通过舍弃严格的平衡和引入红黑节点，解决了AVL旋转效率过低的问题，但是在磁盘等场景下，树仍然太高，IO次数太多。 B+树：在B树的基础上，将非叶节点改造为不存储数据纯索引节点，进一步降低了树的高度；此外将叶节点使用指针连接成链表，范围查询更加高效。 此外, B+树, 主要是查询效率高，O(logN)，可以充分利用磁盘预读的特性，多叉树，深度小，叶子结点有序且存储数据. ","date":"2022-10-09","objectID":"/go-interview-summary/:3:18","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"InnoDB的行锁模式 共享锁(S)：用法lock in share mode，又称读锁，允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 若事务T对数据对象A加上S锁，则事务T可以读A但不能修改A，其他事务只能再对A加S锁，而不能加X锁，直到T释放A上的S锁。这保证了其他事务可以读A，但在T释放A上的S锁之前不能对A做任何修改。 排他锁(X)：用法for update，又称写锁，允许获取排他锁的事务更新数据，阻止其他事务取得相同的数据集共享读锁和排他写锁。 若事务T对数据对象A加上X锁，事务T可以读A也可以修改A，其他事务不能再对A加任何锁，直到T释放A上的锁。在没有索引的情况下，InnoDB只能使用表锁。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:19","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"哈希(hash)比树(tree)更快，索引结构为什么要设计成树型 加速查找速度的数据结构，常见的有两类： 哈希，例如HashMap，查询/插入/修改/删除的平均时间复杂度都是O(1)； 树，例如平衡二叉搜索树，查询/插入/修改/删除的平均时间复杂度都是O(lg(n))； 哈希只能满足等值查询, 不满足范围和大小查询, 其次哈希不可以排序. Mysql是用等值查询,用树的话,等值查询只需要顺序遍历即可. 但是对于排序查询的sql需求：分组：group by ,排序：order by ,比较：\u003c、\u003e等,哈希型的索引，时间复杂度会退化为O(n)，而树型的“有序”特性，依然能够保持O(log(n)) 的高效率。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:20","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"为什么索引的key长度不能太长 key 太长会导致一个页当中能够存放的 key 的数目变少，间接导致索引树的页数目变多，索引层次增加，从而影响整体查询变更的效率。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:21","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql的数据如何恢复到任意时间点 恢复到任意时间点以定时的做全量备份，以及备份增量的 binlog 日志为前提。恢复到任意时间点首先将全量备份恢复之后，再此基础上回放增加的 binlog 直至指定的时间点。 ","date":"2022-10-09","objectID":"/go-interview-summary/:3:22","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Mysql为什么加了索引可以加快查询 在数据十分庞大的时候，索引可以大大加快查询的速度，这是因为使用索引后可以不用扫描全表来定位某行的数据，而是先通过索引表找到该行数据对应的物理地址然后访问相应的数据。 索引的优缺点: 优势：可以快速检索，减少I/O次数，加快检索速度；根据索引分组和排序，可以加快分组和排序； 劣势：索引本身也是表，因此会占用存储空间，一般来说，索引表占用的空间的数据表的1.5倍；索引表的维护和创建需要时间成本，这个成本随着数据量增大而增大；构建索引会降低数据表的修改操作（删除，添加，修改）的效率，因为在修改数据表的同时还需要修改索引表. ","date":"2022-10-09","objectID":"/go-interview-summary/:3:23","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Explain命令有什么用 在开发的过程中,我们有时会用慢查询去记录一些执行时间比较久的Sql语句，找出这些Sql语句并不意味着完事了，这个时候我们就需要用到explain这个命令来查看一个这些Sql语句的执行计划，查看该Sql语句有没有使用上了索引，有没有做全表扫描，这些都可以通过explain命令来查看。 所以我们深入了解Mysql的基于开销的优化器，还可以获得很多可能被优化器考虑到的访问策略的细节，以及当运行SQL语句时哪种策略预计会被优化器采用。 \u003e explain select * from server; +----+-------------+---------+------+---------------+------+---------+------+------+-------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+---------+------+---------------+------+---------+------+------+-------+ | 1 | SIMPLE | server | ALL | NULL | NULL | NULL | NULL | 1 | NULL | +----+-------------+---------+------+---------------+------+---------+------+------+-------+ 1 row in set (0.03 sec) expain出来的信息有10列，分别是id、select_type、table、type、possible_keys、key、key_len、ref、rows、Extra. id: select选择标识符. select_type: 表示查询的类型. table: 输出结果集的表. partitions: 匹配的分区. type: 表示表的连接类型. possible_keys: 表示查询时，可能使用的索引. key: 表示实际使用的索引. key_len: 索引字段的长度. ref: 列与索引的比较. rows: 扫描出的行数(估算的行数). filtered: 按表条件过滤的行百分比. Extra: 执行情况的描述和说明. id id是Sql执行的顺序的标识,Sql从大到小的执行: id相同时，执行顺序由上至下. 如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行. id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行. select_type 查询的类型 示查询中每个select子句的类型: SIMPLE(简单SELECT,不使用UNION或子查询等) PRIMARY(查询中若包含任何复杂的子部分,最外层的select被标记为PRIMARY) UNION(UNION中的第二个或后面的SELECT语句) DEPENDENT UNION(UNION中的第二个或后面的SELECT语句，取决于外面的查询) UNION RESULT(UNION的结果) SUBQUERY(子查询中的第一个SELECT) DEPENDENT SUBQUERY(子查询中的第一个SELECT，取决于外面的查询) DERIVED(派生表的SELECT, FROM子句的子查询) UNCACHEABLE SUBQUERY(一个子查询的结果不能被缓存，必须重新评估外链接的第一行) table table显示这一行的数据是关于哪张表的，有时不是真实的表名字,看到的是derivedx. \u003e explain select * from (select * from ( select * from t1 where id=2602) a) b; +----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+ | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra | +----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+ | 1 | PRIMARY | \u003cderived2\u003e | system | NULL | NULL | NULL | NULL | 1 | | | 2 | DERIVED | \u003cderived3\u003e | system | NULL | NULL | NULL | NULL | 1 | | | 3 | DERIVED | t1 | const | PRIMARY,idx_t1_id | PRIMARY | 4 | | 1 | | +----+-------------+------------+--------+-------------------+---------+---------+------+------+-------+ type 表的连接类型 type表示Mysql在表中找到所需行的方式，又称“访问类型”。 常用的类型有： ALL, index, range, ref, eq_ref, const, system, NULL（从左到右，性能从差到好）. ALL：Full Table Scan， Mysql将遍历全表以找到匹配的行. index: Full Index Scan，index与ALL区别为index类型只遍历索引树. range:只检索给定范围的行，使用一个索引来选择行. ref: 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值. eq_ref: 类似ref，区别就在使用的索引是唯一索引，对于每个索引键值，表中只有一条记录匹配，简单来说，就是多表连接中使用primary key或者 unique key作为关联条件. const、system: 当Mysql对查询某部分进行优化，并转换为一个常量时，使用这些类型访问。如将主键置于where列表中，Mysql就能将该查询转换为一个常量,system是const类型的特例，当查询的表只有一行的情况下，使用system. NULL: Mysql在优化过程中分解语句，执行时甚至不用访问表或索引，例如从一个索引列里选取最小值可以通过单独索引查找完成。 possible_keys possible_keys指出Mysql能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用. 该列完全独立于EXPLAIN输出所示的表的次序。这意味着在possible_keys中的某些键实际上不能按生成的表次序使用。 如果该列是NULL，则没有相关的索引。在这种情况下，可以通过检查WHERE子句看是否它引用某些列或适合索引的列来提高你的查询性能。如果是这样，创造一个适当的索引并且再次用EXPLAIN检查查询. Key key列显示MySql实际决定使用的键（索引）. 如果没有选择索引，键是NULL。要想强制Mysql使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。 key_len key_len表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的）不损失精确性的情况下，长度越短越好 . ref ref表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值. rows rows表示Mysql根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数. Extra执行情况的描述和说明 该列包含Mysql解决查询的详细信息,有以下几种情况： Using where:列数据是从仅仅使用了索引中的信息而没有读取实际的行动的表返回的，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示mysql服务器将在存储引擎检索行后再进行过滤. Using temporary：表示Mysql需要使用临时表来存储结果集，常见于排序和分组查询. Using filesort：Mysql中无法利用索引完成的排序操作称为“文件排序” Using join buffer：改值强调了在获取连接条件时没有使用索引，并且需要连接缓冲区来存储中间结果。如果出现了这个值，那应该注意，根据查询的具体情况可能需要添加索引来改进能。 Impossible where：这个值强调了where语句会导致没有符合条件的行。 Select tables optimized away：这个值意味着仅通过使用索引，优化器可能仅从聚合函数结果中返回一行. ","date":"2022-10-09","objectID":"/go-interview-summary/:3:24","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Redis基础知识 ","date":"2022-10-09","objectID":"/go-interview-summary/:4:0","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Redis的数据结构及使用场景 String字符串 字符串类型是 Redis 最基础的数据结构，首先键都是字符串类型，而且 其他几种数据结构都是在字符串类型基础上构建的，我们常使用的 set key value 命令就是字符串。常用在缓存、计数、共享Session、限速等。 Hash哈希 在Redis中，哈希类型是指键值本身又是一个键值对结构，哈希可以用来存放用户信息，比如实现购物车。 List列表（双向链表） 列表（list）类型是用来存储多个有序的字符串。可以做简单的消息队列的功能。 Set集合 集合（set）类型也是用来保存多个的字符串元素，但和列表类型不一 样的是，集合中不允许有重复元素，并且集合中的元素是无序的，不能通过索引下标获取元素。 利用 Set 的交集、并集、差集等操作，可以计算共同喜好，全部的喜好，自己独有的喜好等功能。 Sorted Set有序集合（跳表实现） Sorted Set 多了一个权重参数 Score，集合中的元素能够按 Score 进行排列。可以做排行榜应用，取 TOP N 操作。 ","date":"2022-10-09","objectID":"/go-interview-summary/:4:1","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Redis持久化的几种方式 Redis为了保证效率，数据缓存在了内存中，但是会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件中，以保证数据的持久化。 Redis的持久化策略有两种： RDB：快照形式是直接把内存中的数据保存到一个dump的文件中，定时保存，保存策略。 当Redis需要做持久化时，Redis会fork一个子进程，子进程将数据写到磁盘上一个临时RDB文件中。当子进程完成写临时文件后，将原来的RDB替换掉。 AOF：把所有的对Redis的服务器进行修改的命令都存到一个文件里，命令的集合。 使用AOF做持久化，每一个写命令都通过write函数追加到appendonly.aof中。 aof的默认策略是每秒钟fsync一次，在这种配置下，就算发生故障停机，也最多丢失一秒钟的数据。 缺点是对于相同的数据集来说，AOF的文件体积通常要大于RDB文件的体积。根据所使用的fsync策略，AOF的速度可能会慢于RDB。 Redis默认是快照RDB的持久化方式。对于主从同步来说，主从刚刚连接的时候，进行全量同步（RDB）,全同步结束后，进行增量同步(AOF)。 ","date":"2022-10-09","objectID":"/go-interview-summary/:4:2","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Redis的LRU具体实现 传统的LRU是使用栈的形式，每次都将最新使用的移入栈顶，但是用栈的形式会导致执行select *的时候大量非热点数据占领头部数据，所以需要改进。 Redis每次按key获取一个值的时候，都会更新value中的lru字段为当前秒级别的时间戳。Redis初始的实现算法很简单，随机从dict中取出五个key,淘汰一个lru字段值最小的。 在3.0的时候，又改进了一版算法，首先第一次随机选取的key都会放入一个pool中(pool的大小为16),pool中的key是按lru大小顺序排列的。 接下来每次随机选取的keylru值必须小于pool中最小的lru才会继续放入，直到将pool放满。放满之后，每次如果有新的key需要放入，需要将pool中lru最大的一个key取出。淘汰的时候，直接从pool中选取一个lru最小的值然后将其淘汰。 ","date":"2022-10-09","objectID":"/go-interview-summary/:4:3","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"单线程的Redis为什么快 纯内存操作 单线程操作，避免了频繁的上下文切换 合理高效的数据结构 采用了非阻塞I/O多路复用机制 ","date":"2022-10-09","objectID":"/go-interview-summary/:4:4","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Redis的数据过期策略 Redis 中数据过期策略采用定期删除和惰性删除策略: 定期删除策略 Redis 启用一个定时器定时监视所有的 key，判断key是否过期，过期的话就删除。 这种策略可以保证过期的 key 最终都会被删除，但是也存在严重的缺点：每次都遍历内存中所有的数据，非常消耗 CPU资源，并且当 key 已过期，但是定时器还处于未唤起状态，这段时间内 key 仍然可以用。 惰性删除策略 在获取 key 时，先判断 key 是否过期，如果过期则删除。 这种方式存在一个缺点：如果这个 key一直未被使用，那么它一直在内存中，其实它已经过期了，会浪费大量的空间。 这两种策略天然的互补，结合起来之后，定时删除策略就发生了一些改变，不在是每次扫描全部的 key 了，而是随机抽取一部分 key 进行检查，这样就降低了对 CPU 资源的损耗，惰性删除策略互补了为检查到的key，基本上满足了所有要求。 但是有时候就是那么的巧，既没有被定时器抽取到，又没有被使用，这些数据又如何从内存中消失？ 这个时候就需要用到了,内存淘汰机制. 内存淘汰机制分为： 当内存不足以容纳新写入数据时，新写入操作会报错。（Redis 默认策略） 当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 Key。（LRU推荐使用） 当内存不足以容纳新写入数据时，在键空间中，随机移除某个 Key。 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 Key。这种情况一般是把 Redis 既当缓存，又做持久化存储的时候才用。 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 Key。 当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 Key 优先移除。 ","date":"2022-10-09","objectID":"/go-interview-summary/:4:5","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"如何解决Redis缓存雪崩问题 使用 Redis 高可用架构：使用 Redis 集群来保证 Redis 服务不会挂掉 缓存时间不一致，给缓存的失效时间，加上一个随机值，避免集体失效 限流降级策略：有一定的备案，比如个性推荐服务不可用了，换成热点数据推荐服务 ","date":"2022-10-09","objectID":"/go-interview-summary/:4:6","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"如何解决Redis缓存穿透问题 在接口层做校验 存null值（缓存击穿加锁） 布隆过滤器拦截：将所有可能的查询key 先映射到布隆过滤器中，查询时先判断key是否存在布隆过滤器中，存在才继续向下执行，如果不存在，则直接返回。 布隆过滤器将值进行多次哈希bit存储，布隆过滤器说某个元素在，可能会被误判。布隆过滤器说某个元素不在，那么一定不在。 ","date":"2022-10-09","objectID":"/go-interview-summary/:4:7","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Redis并发竞争key如何解决 可以利用分布式锁和时间戳来解决. 利用消息队列解决. ","date":"2022-10-09","objectID":"/go-interview-summary/:4:8","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Redis的主从模式和哨兵模式和集群模式区别 Redis集群方式共有三种：主从模式，哨兵模式，集群(cluster)模式 主从模式 主从模式是三种集群方式里最简单的。它主要是基于Redis的主从复制特性架构的。通常我们会设置一个主节点，N个从节点;默认情况下，主节点负责处理使用者的IO操作，而从节点则会对主节点的数据进行备份，并且也会对外提供读操作的处理。 主要的特点如下： 主从模式下，当某一节点损坏时，因为其会将数据备份到其它Redis实例上，这样做在很大程度上可以恢复丢失的数据。 主从模式下，可以保证负载均衡. 主从模式下，主节点和从节点是读写分离的。使用者不仅可以从主节点上读取数据，还可以很方便的从从节点上读取到数据，这在一定程度上缓解了主机的压力。 从节点也是能够支持写入数据的，只不过从从节点写入的数据不会同步到主节点以及其它的从节点下。 从以上，我们不难看出Redis在主从模式下，必须保证主节点不会宕机——一旦主节点宕机，其它节点不会竞争称为主节点，此时，Redis将丧失写的能力。这点在生产环境中，是致命的。 哨兵模式 哨兵模式是基于主从模式做的一定变化，它能够为Redis提供了高可用性。 在实际生产中，服务器难免不会遇到一些突发状况：服务器宕机，停电，硬件损坏等。这些情况一旦发生，其后果往往是不可估量的。 而哨兵模式在一定程度上能够帮我们规避掉这些意外导致的灾难性后果。其实，哨兵模式的核心还是主从复制。 只不过相对于主从模式在主节点宕机导致不可写的情况下，多了一个竞选机制——从所有的从节点竞选出新的主节点。竞选机制的实现，是依赖于在系统中启动一个sentinel进程。 sentinel特点： 监控：它会监听主服务器和从服务器之间是否在正常工作。 通知：它能够通过API告诉系统管理员或者程序，集群中某个实例出了问题。 故障转移：它在主节点出了问题的情况下，会在所有的从节点中竞选出一个节点，并将其作为新的主节点。 提供主服务器地址：它还能够向使用者提供当前主节点的地址。这在故障转移后，使用者不用做任何修改就可以知道当前主节点地址。 sentinel，也可以集群，部署多个哨兵，sentinel可以通过发布与订阅来自动发现Redis集群上的其它sentinel。sentinel在发现其它sentinel进程后，会将其放入一个列表中，这个列表存储了所有已被发现的sentinel。 集群中的所有sentinel不会并发着去对同一个主节点进行故障转移。故障转移只会从第一个sentinel开始，当第一个故障转移失败后，才会尝试下一个。 当选择一个从节点作为新的主节点后，故障转移即成功了(而不会等到所有的从节点配置了新的主节点后)。这过程中，如果重启了旧的主节点，那么就会出现无主节点的情况，这种情况下，只能重启集群。 当竞选出新的主节点后，被选为新的主节点的从节点的配置信息会被sentinel改写为旧的主节点的配置信息。完成改写后，再将新主节点的配置广播给所有的从节点。 集群模式 Redis 集群是一个提供在多个Redis间节点间共享数据的程序集, 其中Redis集群分为主节点和从节点。主节点用于处理槽,而从节点用于复制某个主节点，并在被复制的主节点下线时，代替下线的主节点继续处理命令请求。 Redis集群并不支持处理多个keys的命令,因为这需要在不同的节点间移动数据,从而达不到像Redis那样的性能,在高负载的情况下可能会导致不可预料的错误. Redis 集群通过分区来提供一定程度的可用性,在实际环境中当某个节点宕机或者不可达的情况下继续处理命令. Redis 集群的优势: 自动分割数据到不同的节点上。 整个集群的部分节点失败或者不可达的情况下能够继续处理命令。 Redis集群的数据分片 Redis 集群没有使用一致性hash, 而是引入了哈希槽的概念. Redis 集群有16384个哈希槽,每个key通过CRC16校验后对16384取模来决定放置哪个槽.集群的每个节点负责一部分hash槽. 例如,当前集群有3个节点,那么: 节点 A 包含 0 到 5500号哈希槽. 节点 B 包含5501 到 11000 号哈希槽. 节点 C 包含11001 到 16384号哈希槽. 这种结构很容易添加或者删除节点. 比如如果我想新添加个节点D, 我需要从节点 A, B, C中得部分槽到D上. 如果我想移除节点A,需要将A中的槽移到B和C节点上,然后将没有任何槽的A节点从集群中移除即可. 由于从一个节点将哈希槽移动到另一个节点并不会停止服务,所以无论添加删除或者改变某个节点的哈希槽的数量都不会造成集群不可用的状态. Redis 集群的主从复制模型 为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型,每个节点都会有N-1个复制品. ","date":"2022-10-09","objectID":"/go-interview-summary/:4:9","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Redis有序集合zset底层怎么实现的 Redis中的set数据结构底层用的是跳表和哈希表实现的(新的版本优化). 跳表是一个随机化的数据结构，实质就是一种可以进行二分查找的有序链表。跳表在原有的有序链表上面增加了多级索引，通过索引来实现快速查找。跳表不仅能提高搜索性能，同时也可以提高插入和删除操作的性能。 跳表是可以实现二分查找的有序链表； 每个元素插入时随机生成它的level； 最低层包含所有的元素； 如果一个元素出现在level(x)，那么它肯定出现在x以下的level中； 每个索引节点包含两个指针，一个向下，一个向右； 跳表查询、插入、删除的时间复杂度为O(log n)，与平衡二叉树接近； 为什么Redis选择使用跳表而不是红黑树来实现有序集合？(O(logN)) 首先，我们来分析下Redis的有序集合支持的操作： 插入元素 删除元素 查找元素 有序输出所有元素 查找区间内所有元素 其中，前4项红黑树都可以完成，且时间复杂度与跳表一致。但是，最后一项，红黑树的效率就没有跳表高了。 在跳表中，要查找区间的元素，我们只要定位到两个区间端点在最低层级的位置，然后按顺序遍历元素就可以了，非常高效。 而红黑树只能定位到端点后，再从首位置开始每次都要查找后继节点，相对来说是比较耗时的。 此外，跳表实现起来很容易且易读，红黑树实现起来相对困难，所以Redis选择使用跳表来实现有序集合。 ","date":"2022-10-09","objectID":"/go-interview-summary/:4:10","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"跳表的查询过程是怎么样的，查询和插入的时间复杂度 先从第一层查找，不满足就下沉到第二层找，因为每一层都是有序的，写入和插入的时间复杂度都是O(logN) ","date":"2022-10-09","objectID":"/go-interview-summary/:4:11","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"redis如何分片 Redis Cluster 一般由多个节点组成，节点数量至少为 6 个才能保证组成完整高可用的集群，其中三个为主节点，三个为从节点。三个主节点会分配槽，处理客户端的命令请求，而从节点可用在主节点故障后，顶替主节点。一般来说，主 Redis 节点会处理 Clients 的读写操作，而从节点只处理读操作。 分布式数据存储方案中最为重要的一点就是数据分片，也就是所谓的 Sharding。 为了使得集群能够水平扩展，首要解决的问题就是如何将整个数据集按照一定的规则分配到多个节点上，常用的数据分片的方法有：范围分片，哈希分片，一致性哈希算法，哈希槽等。 范围分片假设数据集是有序，将顺序相临近的数据放在一起，可以很好的支持遍历操作。范围分片的缺点是面对顺序写时，会存在热点。比如日志类型的写入，一般日志的顺序都是和时间相关的，时间是单调递增的，因此写入的热点永远在最后一个分片。 Redis Cluster 采用虚拟哈希槽分区，所有的键根据哈希函数映射到 0 ~ 16383 整数槽内，计算公式：slot = CRC16(key) \u0026 16383。每一个节点负责维护一部分槽以及槽所映射的键值数据。 Redis 虚拟槽分区的特点： 解耦数据和节点之间的关系，简化了节点扩容和收缩难度。 节点自身维护槽的映射关系，不需要客户端或者代理服务维护槽分区元数据 支持节点、槽和键之间的映射查询，用于数据路由，在线集群伸缩等场景。 Redis 集群提供了灵活的节点扩容和收缩方案。在不影响集群对外服务的情况下，可以为集群添加节点进行扩容也可以下线部分节点进行缩容。可以说，槽是 Redis 集群管理数据的基本单位，集群伸缩就是槽和数据在节点之间的移动。 ","date":"2022-10-09","objectID":"/go-interview-summary/:4:12","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"网络协议基础 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:0","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"TCP和UDP有什么区别 TCP与UDP区别总结： 1、TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接. 2、TCP提供可靠的服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP尽最大努力交付，即不保证可靠交付. 3、TCP面向字节流，实际上是TCP把数据看成一连串无结构的字节流;UDP是面向报文的. UDP没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（对实时应用很有用，如IP电话，实时视频会议等）. 4、每一条TCP连接只能是点到点的;UDP支持一对一，一对多，多对一和多对多的交互通信. 5、TCP首部开销20字节;UDP的首部开销小，只有8个字节. 6、TCP的逻辑通信信道是全双工的可靠信道，UDP则是不可靠信道. 因此UDP不提供复杂的控制机制，利用IP提供面向无连接的通信服务，随时都可以发送数据，处理简单且高效. 经常用于以下场景： 包总量较小的通信（DNS、SNMP） 视频、音频等多媒体通信（即时通信） 广播通信 TCP 使用场景: 相对于 UDP，TCP 实现了数据传输过程中的各种控制，可以进行丢包时的重发控制，还可以对次序乱掉的分包进行顺序控制。 在对可靠性要求较高的情况下，可以使用 TCP，即不考虑 UDP 的时候，都可以选择 TCP。 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:1","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"TCP中三次握手和四次挥手 三次握手 假设 A 为客户端，B 为服务器端。 首先 B 处于 LISTEN（监听）状态，等待客户的连接请求。 A 向 B 发送连接请求报文段，SYN=1，ACK=0，选择一个初始的序号 seq = x。 B 收到连接请求报文段，如果同意建立连接，则向 A 发送连接确认报文段，SYN=1，ACK=1，确认号为 x+1，同时也选择一个初始的序号 seq = y。 A 收到 B 的连接确认报文段后，还要向 B 发出确认，确认号为 ack = y+1，序号为 seq = x+1。 A 的 TCP 通知上层应用进程，连接已经建立。 B 收到 A 的确认后，连接建立。 B 的 TCP 收到主机 A 的确认后，也通知其上层应用进程：TCP 连接已经建立。 为什么TCP连接需要三次握手，两次不可以吗，为什么? TCP是一个双向通信协议，通信双方都有能力发送信息，并接收响应。如果只是两次握手， 至多只有连接发起方的起始序列号能被确认， 另一方选择的序列号则得不到确认 四次挥手 数据传输结束后，通信的双方都可释放连接。现在 A 的应用进程先向其 TCP 发出连接释放报文段，并停止再发送数据，主动关闭 TCP连接。 A 把连接释放报文段首部的 FIN = 1，其序号 seq = u，等待 B 的确认。 B 发出确认，确认号 ack = u+1，而这个报文段自己的序号 seq = v。（TCP 服务器进程通知高层应用进程）。 从 A 到 B 这个方向的连接就释放了，TCP 连接处于半关闭状态。A 不能向 B 发送数据；B 若发送数据，A 仍要接收。 当 B 不再需要连接时，发送连接释放请求报文段，FIN=1。 A 收到后发出确认，进入TIME-WAIT状态，等待 2 MSL（2*2 = 4 mins）时间后释放连接。 B 收到 A 的确认后释放连接。 四次挥手的原因: 客户端发送了 FIN 连接释放报文之后，服务器收到了这个报文， ","date":"2022-10-09","objectID":"/go-interview-summary/:5:2","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"TCP的LISTEN状态是什么 TCP的LISTEN是服务器处于监听状态: CLOSED：初始状态。 LISTEN：服务器处于监听状态。 TIME_WAIT：客户端收到服务端的FIN包，并立即发出ACK包做最后的确认，在此之后的2MSL时间称为TIME_WAIT状态。 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:3","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"常见的HTTP状态码有哪些 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:4","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"301和302有什么区别 301: Moved Permanently 被请求的资源已永久移动到新位置，并且将来任何对此资源的引用都应该使用本响应返回的若干个URI之一。如果可能，拥有链接编辑功能的客户端应当自动把请求的地址修改为从服务器反馈回来的地址。除非额外指定，否则这个响应也是可缓存的。 302 Found 请求的资源现在临时从不同的URI响应请求。由于这样的重定向是临时的，客户端应当继续向原有地址发送以后的请求。只有在Cache-Control或Expires中进行了指定的情况下，这个响应才是可缓存的。 301是永久重定向，而302是临时重定向。 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:5","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"504和500有什么区别 500的错误通常是由于服务器上代码出错或者是抛出了异常. 502即 Bad Gateway网关(这里的网关是指CGI,即通用网关接口)错误,通常是程序空指针错误。 504即Gateway timeout,即超时错误. ","date":"2022-10-09","objectID":"/go-interview-summary/:5:6","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"HTTPS和HTTP有什么区别 http协议和https协议的区别：传输信息安全性不同、连接方式不同、端口不同、证书专申请方式不同. 一、传输信息安全性不同 http协议：是超文本传输协议，信息是明文传输。如果攻击者截取了Web浏览器和网站服务器之间的传输报文，就可以直接读懂其中的信息。 https协议：是具有安全性的ssl加密传输协议，为浏览器和服务器之间的通信加密，确保数据传输的安 二,连接方式不同 http协议：http的连接很简单，是无状态的。 https协议：是由SSL＋HTTP协议构建的可进行加密传输、身份认证的网络协议。 三、端口不同 http协议：使用的端口是80。 https协议：使用的端口是443． 四、证书申请方式不同 http协议：免费申请。 https协议：需要到ca申请证书，一般免费证书很少，需要交费。 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:7","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Quic有什么优点相比Http2 HTTP1 有连接无法复用、队头阻塞、协议开销大和安全因素等多个缺陷. HTTP2 通过多路复用、二进制流、Header 压缩等等技术，极大地提高了性能，但是还是存在着问题的 Quic 基于 UDP 实现，是 HTTP3 中的底层支撑协议，该协议基于 UDP，又取了 TCP 中的精华，实现了即快又可靠的协议.quic中加密认证的报文,(TCP 协议头部没有经过任何加密和认证，所以在传输过程中很容易被中间网络设备篡改，注入和窃听。比如修改序列号、滑动窗口。这些行为有可能是出于性能优化，也有可能是主动攻击。)这样只要对 QUIC 报文任何修改，接收端都能够及时发现，有效地降低了安全风险。 此外quic还有向前纠错的能力,QUIC 协议有一个非常独特的特性，称为向前纠错 (Forward Error Correction，FEC)，每个数据包除了它本身的内容之外，还包括了部分其他数据包的数据，因此少量的丢包可以通过其他包的冗余数据直接组装而无需重传。 向前纠错牺牲了每个数据包可以发送数据的上限，但是减少了因为丢包导致的数据重传，因为数据重传将会消耗更多的时间(包括确认数据包丢失、请求重传、等待新数据包等步骤的时间消耗), 假如说这次我要发送三个包，那么协议会算出这三个包的异或值并单独发出一个校验包，也就是总共发出了四个包。当出现其中的非校验包丢包的情况时，可以通过另外三个包计算出丢失的数据包的内容。 当然这种技术只能使用在丢失一个包的情况下，如果出现丢失多个包就不能使用纠错机制了，只能使用重传的方式了。 Quic 相比现在广泛应用的 http2+tcp+tls 协议有如下优势: 减少了 TCP 三次握手及 TLS 握手时间。改进的拥塞控制。避免队头阻塞的多路复用。连接迁移。前向冗余纠错. ","date":"2022-10-09","objectID":"/go-interview-summary/:5:8","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Grpc的优缺点 gRPC是Google公司基于Protobuf开发的跨语言的开源RPC框架。gRPC基于HTTP/2协议设计，可以基于一个HTTP/2链接提供多个服务，对于移动设备更加友好。 最底层为TCP或Unix Socket协议，在此之上是HTTP/2协议的实现，然后在HTTP/2协议之上又构建了针对Go语言的gRPC核心库。应用程序通过gRPC插件生产的Stub代码和gRPC核心库通信，也可以直接和gRPC核心库通信。 Grpc优缺点： 优点： protobuf二进制消息，性能好/效率高（空间和时间效率都很不错） proto文件生成目标代码，简单易用 序列化反序列化直接对应程序中的数据类，不需要解析后在进行映射(XML,JSON都是这种方式) 支持向前兼容（新加字段采用默认值）和向后兼容（忽略新加字段），简化升级 支持多种语言（可以把proto文件看做IDL文件） 缺点： GRPC尚未提供连接池，需要自行实现 尚未提供“服务发现”、“负载均衡”机制 因为基于HTTP2，绝大部多数HTTP Server、Nginx都尚不支持，即Nginx不能将GRPC请求作为HTTP请求来负载均衡，而是作为普通的TCP请求。（nginx1.9版本已支持） Protobuf二进制可读性差（貌似提供了Text_Format功能）默认不具备动态特性（可以通过动态定义生成消息类型或者动态编译支持） ","date":"2022-10-09","objectID":"/go-interview-summary/:5:9","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Get和Post区别 Get和Post的区别和不同: Get是不安全的，因为在传输过程，数据被放在请求的URL中；Post的所有操作对用户来说都是不可见的。 Get传送的数据量较小，这主要是因为受URL长度限制；Post传送的数据量较大，一般被默认为不受限制。 Get限制Form表单的数据集的值必须为ASCII字符；而Post支持整个ISO10646字符集。 Get执行效率却比Post方法好。Get是form提交的默认方法。 GET产生一个TCP数据包；POST产生两个TCP数据包。（非必然，客户端可灵活决定） ","date":"2022-10-09","objectID":"/go-interview-summary/:5:10","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Unicode和ASCII以及Utf8的区别 计算机内部，所有信息最终都是一个二进制值。每一个二进制位（bit）有0和1两种状态，因此八个二进制位就可以组合出256种状态，这被称为一个字节（byte）。 也就是说，一个字节一共可以用来表示256种不同的状态，每一个状态对应一个符号，就是256个符号，从00000000到11111111。 上个世纪60年代，美国制定了一套字符编码，对英语字符与二进制位之间的关系，做了统一规定。这被称为 ASCII 码，一直沿用至今。 ASCII 码一共规定了128个字符的编码，比如空格SPACE是32（二进制00100000），大写的字母A是65（二进制01000001）。 这128个符号（包括32个不能打印出来的控制符号），只占用了一个字节的后面7位，最前面的一位统一规定为0。 Unicode 是字符集 如果有一种编码，将世界上所有的符号都纳入其中。每一个符号都给予一个独一无二的编码，那么乱码问题就会消失。这就是 Unicode，就像它的名字都表示的，这是一种所有符号的编码。 Unicode 当然是一个很大的集合，现在的规模可以容纳100多万个符号。 UTF-8 是编码规则 Unicode 只是一个符号集，它只规定了符号的二进制代码，却没有规定这个二进制代码应该如何存储。 互联网的普及，强烈要求出现一种统一的编码方式。UTF-8 就是在互联网上使用最广的一种 Unicode 的实现方式。其他实现方式还包括 UTF-16（字符用两个字节或四个字节表示）和 UTF-32（字符用四个字节表示），不过在互联网上基本不用。 这里需要注意下，这里的关系是，UTF-8 是 Unicode 的实现方式之一。 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:11","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Cookie与Session异同 Cookie 和 Session 都为了用来保存状态信息，都是保存客户端状态的机制，它们都是为了解决HTTP无状态的问题而所做的努力。 Cookie机制 简单地说，Cookie 就是浏览器储存在用户电脑上的一小段文本文件。Cookie 是纯文本格式，不包含任何可执行的代码。 一个 Web 页面或服务器告知浏览器按照一定规范来储存这些信息，并在随后的请求中将这些信息发送至服务器，Web 服务器就可以使用这些信息来识别不同的用户。 大多数需要登录的网站在用户验证成功之后都会设置一个 Cookie，只要这个 Cookie 存在并可以，用户就可以自由浏览这个网站的任意页面。 Cookie 会被浏览器自动删除，通常存在以下几种原因： 会话 Cooke (Session Cookie) 在会话结束时（浏览器关闭）会被删除 持久化 Cookie（Persistent Cookie）在到达失效日期时会被删除 如果浏览器中的 Cookie 数量达到限制，那么 Cookie 会被删除以为新建的 Cookie 创建空间。 大多数浏览器支持最大为 4096 字节的 Cookie。由于这限制了 Cookie 的大小，最好用 Cookie 来存储少量数据，或者存储用户 ID 之类的标识符。 用户 ID 随后便可用于标识用户，以及从数据库或其他数据源中读取用户信息。 浏览器还限制站点可以在用户计算机上存储的 Cookie 的数量。 大多数浏览器只允许每个站点存储 20 个 Cookie；如果试图存储更多 Cookie，则最旧的 Cookie 便会被丢弃。 有些浏览器还会对它们将接受的来自所有站点的 Cookie 总数作出绝对限制，通常为 300 个。 使用 Cookie 的缺点： 不良站点用 Cookie 收集用户隐私信息； Cookie窃取：黑客以可以通过窃取用户的cookie来模拟用户的请求行为。（跨站脚本攻击XSS） Session 机制 Session机制是一种服务器端的机制，服务器使用一种类似于散列表的结构（也可能就是使用散列表）来保存信息。当程序需要为某个客户端的请求创建一个session的时候，服务器首先检查这个客户端的请求里是否已包含了一个Session标识（Session id）. 如果已包含一个SessionID 则说明以前已经为此客户端创建过Session，服务器就按照SessionID把这个 Session检索出来使用（如果检索不到，可能会新建一个）。 如果客户端请求不包含SessionID，则为此客户端创建一个Session并且生成一个与此Session相关联的SessionID，SessionID的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，这个SessionID将被在本次响应中返回给客户端保存。 具体实现方式： Cookie方式 服务器给每个Session分配一个唯一的JSESSIONID，并通过Cookie发送给客户端。 当客户端发起新的请求的时候，将在Cookie头中携带这个JSESSIONID，这样服务器能够找到这个客户端对应的Session。 URL回写 服务器在发送给浏览器页面的所有链接中都携带JSESSIONID的参数，这样客户端点击任何一个链接都会把JSESSIONID带回服务器。如果直接在浏览器输入服务端资源的url来请求该资源，那么Session是匹配不到的。 Web 缓存: WEB缓存(cache)位于Web服务器和客户端之间，缓存机制会根据请求保存输出内容的副本，例如html页面，图片，文件，当下一个请求来到的时候：如果是相同的URL，缓存直接使用副本响应访问请求，而不是向源服务器再次发送请求。 主要分三种情况: 未找到缓存(黑色线)：当没有找到缓存时，说明本地并没有这些数据，这种情况一般发生在我们首次访问网站，或者以前访问过，但是清除过缓存后。 浏览器就会先访问服务器，然后把服务器上的内容取回来，内容取回来以后，就要根据情况来决定是否要保留到缓存中了。 缓存未过期(蓝色线)：缓存未过期，指的是本地缓存没有过期，不需要访问服务器了，直接就可以拿本地的缓存作为响应在本地使用了。这样节省了不少网络成本，提高了用户体验过。 缓存已过期(红色线)：当满足过期的条件时，会向服务器发送请求，发送的请求一般都会进行一个验证，目的是虽然缓存文档过期了，但是文档内容不一定会有什么改变，所以服务器返回的也许是一个新的文档，这时候的HTTP状态码是200，或者返回的只是一个最新的时间戳和304状态码。 缓存过期后，有两种方法来判定服务端的文件有没有更新。 第一种在上一次服务端告诉客户端约定的有效期的同时，告诉客户端该文件最后修改的时间，当再次试图从服务端下载该文件的时候，check下该文件有没有更新（对比最后修改时间），如果没有，则读取缓存. 第二种方式是在上一次服务端告诉客户端约定有效期的同时，同时告诉客户端该文件的版本号，当服务端文件更新的时候，改变版本号，再次发送请求的时候check一下版本号是否一致就行了，如一致，则可直接读取缓存。 浏览器是依靠请求和响应中的的头信息来控制缓存的，如下： Expires与Cache-Control：服务端用来约定和客户端的有效时间的。 Expires规定了缓存失效时间（Date为当前时间），而Cache-Control的max-age规定了缓存有效时间（2552s）。 Expires是HTTP1.0的东西，而Cache-Control是HTTP1.1的，规定如果max-age和Expires同时存在，前者优先级高于后者。 Last-Modified/If-Modified-Since缓存过期后，check服务端文件是否更新的第一种方式。 ETag/If-None-Match：缓存过期时check服务端文件是否更新的第二种方式。 实际上ETag并不是文件的版本号，而是一串可以代表该文件唯一的字符串，当客户端发现和服务器约定的直接读取缓存的时间过了，就在请求中发送If-None-Match选项，值即为上次请求后响应头的ETag值. 该值在服务端和服务端代表该文件唯一的字符串对比（如果服务端该文件改变了，该值就会变），如果相同，则相应HTTP304，客户端直接读取缓存，如果不相同，HTTP200，下载正确的数据，更新ETag值。 当然并不是所有请求都能被缓存。无法被浏览器缓存的请求： HTTP信息头中包含Cache-Control:no-cache，pragma:no-cache（HTTP1.0），或Cache-Control:max-age=0等告诉浏览器不用缓存的请求 需要根据Cookie，认证信息等决定输入内容的动态请求是不能被缓存的 POST请求无法被缓存 浏览器缓存过程还和用户行为有关。譬如先打开一个主页有个jquery的请求（假设访问后会缓存下来）。 接着如果直接在地址栏输入 jquery 地址，然后回车，响应HTTP200（from cache），因为有效期还没过直接读取的缓存；如果ctrl+r进行刷新，则会相应HTTP304（Not Modified），虽然还是读取的本地缓存，但是多了一次服务端的请求；而如果是ctrl+shift+r强刷，则会直接从服务器下载新的文件，响应HTTP200。 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:12","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Client如何实现长连接 TCP协议的KeepAlive机制与HeartBeat心跳包 HeartBeat心跳包 很多应用层协议都有HeartBeat机制，通常是客户端每隔一小段时间向服务器发送一个数据包，通知服务器自己仍然在线，并传输一些可能必要的数据。使用心跳包的典型协议是IM，比如QQ/MSN/飞信等协议。 心跳包之所以叫心跳包是因为：它像心跳一样每隔固定时间发一次，以此来告诉服务器，这个客户端还活着。事实上这是为了保持长连接，至于这个包的内容，是没有什么特别规定的，不过一般都是很小的包，或者只包含包头的一个空包。 在TCP的机制里面，本身是存在有心跳包的机制的，也就是TCP的选项：SO_KEEPALIVE。系统默认是设置的2小时的心跳频率。但是它检查不到机器断电、网线拔出、防火墙这些断线。而且逻辑层处理断线可能也不是那么好处理。一般，如果只是用于保活还是可以的。 心跳包一般来说都是在逻辑层发送空的echo包来实现的。下一个定时器，在一定时间间隔下发送一个空包给客户端，然后客户端反馈一个同样的空包回来，服务器如果在一定时间内收不到客户端发送过来的反馈包，那就只有认定说掉线了。 其实，要判定掉线，只需要send或者recv一下，如果结果为零，则为掉线。但是，在长连接下，有可能很长一段时间都没有数据往来。 理论上说，这个连接是一直保持连接的，但是实际情况中，如果中间节点出现什么故障是难以知道的。更要命的是，有的节点（防火墙）会自动把一定时间之内没有数据交互的连接给断掉。在这个时候，就需要我们的心跳包了，用于维持长连接，保活。 在获知了断线之后，服务器逻辑可能需要做一些事情，比如断线后的数据清理呀，重新连接呀……当然，这个自然是要由逻辑层根据需求去做了。 总的来说，心跳包主要也就是用于长连接的保活和断线处理。一般的应用下，判定时间在30-40秒比较不错。如果实在要求高，那就在6-9秒。 TCP协议的KeepAlive机制 TCP的IP传输层的两个主要协议是UDP和TCP，其中UDP是无连接的、面向packet的，而TCP协议是有连接、面向流的协议。 TCP的KeepAlive机制，首先它貌似默认是不打开的，要用setsockopt将SOL_SOCKET.SO_KEEPALIVE设置为1才是打开，并且可以设置三个参数tcp_keepalive_time/tcp_keepalive_probes/tcp_keepalive_intvl，分别表示连接闲置多久开始发keepalive的ack包、发几个ack包不回复才当对方死了、两个ack包之间间隔多. 在测试的时候用Ubuntu Server 10.04下面默认值是7200秒（2个小时，要不要这么蛋疼啊！）、9次、75秒。 于是连接就了有一个超时时间窗口，如果连接之间没有通信，这个时间窗口会逐渐减小，当它减小到零的时候，TCP协议会向对方发一个带有ACK标志的空数据包（KeepAlive探针），对方在收到ACK包以后，如果连接一切正常，应该回复一个ACK；如果连接出现错误了（例如对方重启了，连接状态丢失），则应当回复一个RST；如果对方没有回复，服务器每隔intvl的时间再发ACK，如果连续probes个包都被无视了，说明连接被断开了。 在http早期，每个http请求都要求打开一个tpc socket连接，并且使用一次之后就断开这个tcp连接。 使用keep-alive可以改善这种状态，即在一次TCP连接中可以持续发送多份数据而不会断开连接。通过使用keep-alive机制，可以减少tcp连接建立次数，也意味着可以减少TIME_WAIT状态连接，以此提高性能和提高httpd服务器的吞吐率(更少的tcp连接意味着更少的系统内核调用,socket的accept()和close()调用)。 但是，keep-alive并不是免费的午餐,长时间的tcp连接容易导致系统资源无效占用。配置不当的keep-alive，有时比重复利用连接带来的损失还更大。所以，正确地设置keep-alive timeout时间非常重要。 使用http keep-alvie，可以减少服务端TIME_WAIT数量(因为由服务端httpd守护进程主动关闭连接)。道理很简单，相较而言，启用keep-alive，建立的tcp连接更少了，自然要被关闭的tcp连接也相应更少了。 使用启用keepalive的不同。另外，http keepalive是客户端浏览器与服务端httpd守护进程协作的结果，所以，我们另外安排篇幅介绍不同浏览器的各种情况对keep-alive的利用。 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:13","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Http1和Http2和Grpc之间的区别是什么 在互联网流量传输只使用了几个网络协议。使用 IPv4 进行路由，使用 TCP 进行连接层面的流量控制，使用 SSL/TLS 协议实现传输安全，使用 DNS 进行域名解析，使用 HTTP 进行应用数据的传输。 但是使用Http进行应用数据的传输,却是在不断的改变,那么Http1和Http2和Grpc之间的区别是什么,我们下面分析下. 通常影响一个 HTTP 网络请求的因素主要有两个：带宽和延迟。 带宽 如果说我们还停留在拨号上网的阶段，带宽可能会成为一个比较严重影响请求的问题，但是现在网络基础建设已经使得带宽得到极大的提升，我们不再会担心由带宽而影响网速，那么就只剩下延迟了。 延迟 浏览器阻塞（HOL blocking）：浏览器会因为一些原因阻塞请求。浏览器对于同一个域名，同时只能有 4 个连接（这个根据浏览器内核不同可能会有所差异），超过浏览器最大连接数限制，后续请求就会被阻塞。 DNS 查询（DNS Lookup）：浏览器需要知道目标服务器的 IP 才能建立连接。将域名解析为 IP 的这个系统就是 DNS。这个通常可以利用DNS缓存结果来达到减少这个时间的目的。 建立连接（Initial connection）：HTTP 是基于 TCP 协议的，浏览器最快也要在第三次握手时才能捎带 HTTP 请求报文，达到真正的建立连接，但是这些连接无法复用会导致每次请求都经历三次握手和慢启动。三次握手在高延迟的场景下影响较明显，慢启动则对文件类大请求影响较大。 然而,HTTP2并不是对HTTP1协议的重写，相对于HTTP1，HTTP2 的侧重点主要在性能。其中请求方法，状态码和语义和HTTP1都是相同的，可以使用与 HTTP1相同的 API（可能有一些小的添加）来表示协议。 HTTP2主要有两个规范组成: Hypertext Transfer Protocol version 2 (超文本传输协议版本 2) HPACK - HTTP2 的头压缩 （HPACK 是一种头部压缩算法） HTTP2和HTTP1相比的新特性包括: 新的二进制格式（Binary Format） HTTP1.x的解析是基于文本。基于文本协议的格式解析存在天然缺陷，文本的表现形式有多样性，要做到健壮性考虑的场景必然很多，二进制则不同，只认0和1的组合。基于这种考虑HTTP2.0的协议解析决定采用二进制格式，实现方便且健壮。 多路复用（Multiplexing） 连接共享，即每一个request都是是用作连接共享机制的。一个request对应一个id，这样一个连接上可以有多个request，每个连接的request可以随机的混杂在一起，接收方可以根据request的 id将request再归属到各自不同的服务端请求里面。 Header压缩 Header压缩，如上文中所言，对前面提到过HTTP1.x的header带有大量信息，而且每次都要重复发送，HTTP2.0使用encoder来减少需要传输的header大小，通讯双方各自cache一份header fields表，既避免了重复header的传输，又减小了需要传输的大小。 服务端推送（server push） 服务端推送（server push），同SPDY一样，HTTP2.0也具有server push功能。 Grpc的设计目标是在任何环境下运行，支持可插拔的负载均衡，跟踪，运行状况检查和身份验证。它不仅支持数据中心内部和跨数据中心的服务调用，它也适用于分布式计算的最后一公里，将设备，移动应用程序和浏览器连接到后端服务，同时，它也是高性能的，而 HTTP2 恰好支持这些。 而Grpc是基于http2的. HTTP2天然的通用性满足各种设备，场景. HTTP2的性能相对来说也是很好的，除非你需要极致的性能. HTTP2的安全性非常好，天然支持 SSL. HTTP2的鉴权也非常成熟. Grpc基于 HTTP2 多语言实现也更容易. ","date":"2022-10-09","objectID":"/go-interview-summary/:5:14","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Tcp中的拆包和粘包是怎么回事 拆包和粘包是在socket编程中经常出现的情况，在socket通讯过程中，如果通讯的一端一次性连续发送多条数据包，tcp协议会将多个数据包打包成一个tcp报文发送出去，这就是所谓的粘包。 而如果通讯的一端发送的数据包超过一次tcp报文所能传输的最大值时，就会将一个数据包拆成多个最大tcp长度的tcp报文分开传输，这就叫做拆包。 MTU: 泛指通讯协议中的最大传输单元。一般用来说明TCP/IP四层协议中数据链路层的最大传输单元，不同类型的网络MTU也会不同，我们普遍使用的以太网的MTU是1500，即最大只能传输1500字节的数据帧。可以通过ifconfig命令查看电脑各个网卡的MTU。 MSS: 指TCP建立连接后双方约定的可传输的最大TCP报文长度，是TCP用来限制应用层可发送的最大字节数。如果底层的MTU是1500byte，则 MSS = 1500- 20(IP Header) -20 (TCP Header) = 1460 byte。 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:15","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"TFO的原理是什么 TCP快速打开（TCP Fast Open，TFO）是对TCP的一种简化握手手续的拓展，用于提高两端点间连接的打开速度。 简而言之，就是在TCP的三次握手过程中传输实际有用的数据。这个扩展最初在Linux系统实现，Linux服务器，Linux系统上的Chrome浏览器，或运行在Linux上的其他支持的软件。 它通过握手开始时的SYN包中的TFO cookie来验证一个之前连接过的客户端。如果验证成功，它可以在三次握手最终的ACK包收到之前就开始发送数据，这样便跳过了一个绕路的行为，更在传输开始时就降低了延迟。 这个加密的Cookie被存储在客户端，在一开始的连接时被设定好。然后每当客户端连接时，这个Cookie被重复返回。 请求Tcp Fast Open Cookie 客户端发送SYN数据包，该数据包包含Fast Open选项，且该选项的Cookie为空，这表明客户端请求Fast Open Cookie； 支持TCP Fast Open的服务器生成Cookie，并将其置于SYN-ACK数据包中的Fast Open选项以发回客户端； 客户端收到SYN-ACK后，缓存Fast Open选项中的Cookie。 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:16","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"TIME_WAIT的作用 主动关闭的Socket端会进入TIME_WAIT状态，并且持续2MSL时间长度，MSL就是maximum segment lifetime(最大分节生命期），这是一个IP数据包能在互联网上生存的最长时间，超过这个时间将在网络中消失。MSL在RFC 1122上建议是2分钟，而源自berkeley的TCP实现传统上使用30秒，因而，TIME_WAIT状态一般维持在1-4分钟。 可靠地实现TCP全双工连接的终止 在进行关闭连接四路握手协议时，最后的ACK是由主动关闭端发出的，如果这个最终的ACK丢失，服务器将重发最终的FIN，因此客户端必须维护状态信息允 许它重发最终的ACK。 如果不维持这个状态信息，那么客户端将响应RST分节，服务器将此分节解释成一个错误（在java中会抛出connection reset的SocketException)。 因而，要实现TCP全双工连接的正常终止，必须处理终止序列四个分节中任何一个分节的丢失情况，主动关闭 的客户端必须维持状态信息进入TIME_WAIT状态。 允许老的重复分节在网络中消逝 TCP分节可能由于路由器异常而“迷途”，在迷途期间，TCP发送端可能因确认超时而重发这个分节，迷途的分节在路由器修复后也会被送到最终目的地，这个 原来的迷途分节就称为lost duplicate。 在关闭一个TCP连接后，马上又重新建立起一个相同的IP地址和端口之间的TCP连接，后一个连接被称为前一个连接的化身 （incarnation)，那么有可能出现这种情况，前一个连接的迷途重复分组在前一个连接终止后出现，从而被误解成从属于新的化身。 为了避免这个情况，TCP不允许处于TIME_WAIT状态的连接启动一个新的化身，因为TIME_WAIT状态持续2MSL，就可以保证当成功建立一个TCP连接的时 候，来自连接先前化身的重复分组已经在网络中消逝。 ","date":"2022-10-09","objectID":"/go-interview-summary/:5:17","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"网络的性能指标有哪些 通常是以4个指标来衡量网络的性能，分别是带宽、延时、吞吐率、PPS（Packet Per Second），它们表示的意义如下： 带宽，表示链路的最大传输速率，单位是 b/s （比特 / 秒），带宽越大，其传输能力就越强。 延时，表示请求数据包发送后，收到对端响应，所需要的时间延迟。不同的场景有着不同的含义，比如可以表示建立 TCP 连接所需的时间延迟，或一个数据包往返所需的时间延迟。 吞吐率，表示单位时间内成功传输的数据量，单位是 b/s（比特 / 秒）或者 B/s（字节 / 秒），吞吐受带宽限制，带宽越大，吞吐率的上限才可能越高。 PPS，全称是 Packet Per Second（包 / 秒），表示以网络包为单位的传输速率，一般用来评估系统对于网络的转发能力。 当然，除了以上这四种基本的指标，还有一些其他常用的性能指标，比如： 网络的可用性，表示网络能否正常通信； 并发连接数，表示 TCP 连接数量； 丢包率，表示所丢失数据包数量占所发送数据组的比率； 重传率，表示重传网络包的比例； ","date":"2022-10-09","objectID":"/go-interview-summary/:5:18","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Linux基础 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:0","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"异步和非阻塞的区别 异步和非阻塞的区别: 异步：调用在发出之后，这个调用就直接返回，不管有无结果；异步是过程。 非阻塞：关注的是程序在等待调用结果（消息，返回值）时的状态，指在不能立刻得到结果之前，该调用不会阻塞当前线程。 同步和异步的区别： 同步：一个服务的完成需要依赖其他服务时，只有等待被依赖的服务完成后，才算完成，这是一种可靠的服务序列。要么成功都成功，失败都失败，服务的状态可以保持一致。 异步：一个服务的完成需要依赖其他服务时，只通知其他依赖服务开始执行，而不需要等待被依赖的服务完成，此时该服务就算完成了。被依赖的服务是否最终完成无法确定，一次它是一个不可靠的服务序列。 消息通知中的同步和异步： 同步：当一个同步调用发出后，调用者要一直等待返回消息（或者调用结果）通知后，才能进行后续的执行。 异步：当一个异步过程调用发出后，调用者不能立刻得到返回消息（结果）。在调用结束之后，通过消息回调来通知调用者是否调用成功。 阻塞与非阻塞的区别： 阻塞：阻塞调用是指调用结果返回之前，当前线程会被挂起，一直处于等待消息通知，不能够执行其他业务,函数只有在得到结果之后才会返回。 非阻塞：非阻塞和阻塞的概念相对应，指在不能立刻得到结果之前，该函数不会阻塞当前线程，而会立刻返回。 同步与异步是对应的，它们是线程之间的关系，两个线程之间要么是同步的，要么是异步的。 阻塞与非阻塞是对同一个线程来说的，在某个时刻，线程要么处于阻塞，要么处于非阻塞。 阻塞是使用同步机制的结果，非阻塞则是使用异步机制的结果。 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:1","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"虚拟内存作用是什么 我们都知道一个进程是与其他进程共享CPU和内存资源的。正因如此，操作系统需要有一套完善的内存管理机制才能防止进程之间内存泄漏的问题. 为了更加有效地管理内存并减少出错，现代操作系统提供了一种对主存的抽象概念，即是虚拟内存（Virtual Memory）。虚拟内存为每个进程提供了一个一致的、私有的地址空间，它让每个进程产生了一种自己在独享主存的错觉（每个进程拥有一片连续完整的内存空间）。 虚拟内存的重要意义是它定义了一个连续的虚拟地址空间，使得程序的编写难度降低。并且，把内存扩展到硬盘空间只是使用虚拟内存的必然结果，虚拟内存空间会存在硬盘中，并且会被内存缓存（按需），有的操作系统还会在内存不够的情况下，将某一进程的内存全部放入硬盘空间中，并在切换到该进程时再从硬盘读取. 虚拟内存主要提供了如下三个重要的能力： 它把主存看作为一个存储在硬盘上的虚拟地址空间的高速缓存，并且只在主存中缓存活动区域（按需缓存）。 它为每个进程提供了一个一致的地址空间，从而降低了程序员对内存管理的复杂性。 它还保护了每个进程的地址空间不会被其他进程破坏。 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:2","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Linux查看端口占用和cpu负载 linux ps命令，查看某进程cpu和内存占用率情况: \u003e ps aux USER PID %CPU %MEM VSZ RSS TT STAT STARTED TIME COMMAND admin 72824 17.3 1.4 5518204 118212 ?? R 27 519 54:49.93 /Applications/iTerm.app/Contents/MacOS/iTerm2 _windowserver 179 16.1 0.6 7525352 46552 ?? Rs 21 519 457:09.25 /System/Library/PrivateFrameworks/SkyLight.fra admin 734 12.2 3.3 6095348 273108 ?? R 21 519 635:17.25 /Users/admin/Desktop/Google Chrome.app/Content admin 10718 9.0 2.7 5604388 223604 ?? S 22 519 557:56.89 /Users/admin/Desktop/Google Chrome.app/Content admin 750 6.4 0.6 4633300 52372 ?? S 21 519 147:59.59 /Users/admin/Desktop/Google Chrome.app/Content admin 749 5.6 1.2 5570904 96832 ?? S 21 519 359:56.37 /Users/admin/Desktop/Google Chrome.app/Content admin 818 4.5 0.1 6557980 5508 ?? S 21 519 557:27.52 com.docker.hyperkit -A -u -F vms/0/hyperkit.pi admin 32898 3.5 1.4 4977204 117684 ?? S 10:54上午 0:02.27 /Users/admin/Desktop/Google Chrome.app/Content admin 30591 2.2 3.7 9505844 310584 ?? S 9:47上午 10:49.28 /Applications/GoLand.app/Contents/MacOS/goland root 1300 1.9 0.1 4334916 6212 ?? Ss 21 519 123:53.86 /usr/libexec/taskgated admin 31232 1.2 1.1 10553808 88860 ?? S 10:24上午 3:28.67 /Applications/WebStorm.app/Contents/MacOS/webs admin 18704 0.7 0.2 19282032 12948 ?? S 3:56下午 4:18.12 /private/var/folders/kp/3yqnp9cj4f3_9539b06q4 linux 下的ps命令 USER 进程运行用户 PID 进程编号 %CPU 进程的cpu占用率 %MEM 进程的内存占用率 VSZ 进程所使用的虚存的大小 RSS 进程 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:3","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Linux如何发送信号给一个进程 通常在linux中可以通过pkill 命令、kill 命令和 killall 命令,或者组合键向进程发送各种信号. Ctrl + C: 中断信号，发送 SIGINT 信号到运行在前台的进程. Ctrl + Y: 延时挂起信号，使运行的进程在尝试从终端读取输入时停止。控制权返回给 Shell，使用户可以将进程放在前台或后台，或杀掉该进程. Ctrl + Z: 挂起信号，发送 SIGTSTP 信号到运行的进程，由此将其停止，并将控制权返回给 Shell. 也可以使用 kill命令结束进程: 发送SIGKILL信号到PID是 123 的进程： \u003e kill -9 123 killall 命令会发送信号到运行任何指定命令的所有进程。所以，当一个进程启动了多个实例时，使用killall命令来杀掉这些进程会更方便一些。 使用 killall 命令杀掉所有 firefox 进程: \u003e killall firefox 使用 pkill 命令，可以通过指定进程名、用户名、组名、终端、UID、EUID和GID等属性来杀掉相应的进程。pkill 命令默认也是发送 SIGTERM 信号到进程。 使用 pkill 命令杀掉所有用户的 firefox 进程. \u003e pkill firefox ","date":"2022-10-09","objectID":"/go-interview-summary/:6:4","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"如何避免死锁 死锁是指多个进程因竞争资源而造成的一种僵局（互相等待），若无外力作用，这些进程都将无法向前推进。 例如，在某一个计算机系统中只有一台打印机和一台输入 设备，进程P1正占用输入设备，同时又提出使用打印机的请求，但此时打印机正被进程P2 所占用，而P2在未释放打印机之前，又提出请求使用正被P1占用着的输入设备。 这样两个进程相互无休止地等待下去，均无法继续执行，此时两个进程陷入死锁状态。 死锁产生的原因: 系统资源的竞争 系统资源的竞争导致系统资源不足，以及资源分配不当，导致死锁。 进程运行推进顺序不合适 进程在运行过程中，请求和释放资源的顺序不当，会导致死锁。 死锁的四个必要条件: 互斥条件：一个资源每次只能被一个进程使用，即在一段时间内某 资源仅为一个进程所占有。此时若有其他进程请求该资源，则请求进程只能等待。 请求与保持条件：进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源 已被其他进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。 不可剥夺条件:进程所获得的资源在未使用完毕之前，不能被其他进程强行夺走，即只能 由获得该资源的进程自己来释放（只能是主动释放)。 循环等待条件: 若干进程间形成首尾相接循环等待资源的关系 这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之一不满足，就不会发生死锁。 死锁的避免与预防: 死锁避免的基本思想 系统对进程发出的每一个系统能够满足的资源申请进行动态检查，并根据检查结果决定是否分配资源，如果分配后系统可能发生死锁，则不予分配，否则予以分配，这是一种保证系统不进入死锁状态的动态策略。 如果操作系统能保证所有进程在有限时间内得到需要的全部资源，则系统处于安全状态否则系统是不安全的。 安全状态 如果系统存在 由所有的安全序列{P1，P2，…Pn},则系统处于安全状态。一个进程序列是安全的，如果对其中每一个进程Pi(i \u003e=1 \u0026\u0026 i \u003c= n)他以后尚需要的资源不超过系统当前剩余资源量与所有进程Pj(j \u003c i)当前占有资源量之和，系统处于安全状态则不会发生死锁。 不安全状态：如果不存在任何一个安全序列，则系统处于不安全状态。 我们可以通过破坏死锁产生的4个必要条件来预防死锁，由于资源互斥是资源使用的固有特性是无法改变的。 破坏“不可剥夺”条件：一个进程不能获得所需要的全部资源时便处于等待状态，等待期间他占有的资源将被隐式的释放重新加入到系统的资源列表中，可以被其他的进程使用，而等待的进程只有重新获得自己原有的资源以及新申请的资源才可以重新启动，执行。 破坏”请求与保持条件“：第一种方法静态分配即每个进程在开始执行时就申请他所需要的全部资源。第二种是动态分配即每个进程在申请所需要的资源时他本身不占用系统资源。 破坏“循环等待”条件：采用资源有序分配其基本思想是将系统中的所有资源顺序编号，将紧缺的，稀少的采用较大的编号，在申请资源时必须按照编号的顺序进行，一个进程只有获得较小编号的进程才能申请较大编号的进程。 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:5","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"孤儿进程和僵尸进程区别 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。 孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:6","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"滑动窗口的概念以及应用 滑窗(sliding window)被同时应用于接收方和发送方。发送方和接收方各有一个滑窗。当片段位于滑窗中时，表示TCP正在处理该片段。滑窗中可以有多个片段，也就是可以同时处理多个片段。 滑窗越大，越大的滑窗同时处理的片段数目越多(当然，计算机也必须分配出更多的缓存供滑窗使用)。 滑动窗口概念不仅存在于数据链路层，也存在于传输层，两者有不同的协议，但基本原理是相近的。其中一个重要区别是，一个是针对于帧的传送，另一个是字节数据的传送。 滑动窗口（Sliding window）是一种流量控制技术。早期的网络通信中，通信双方不会考虑网络的拥挤情况直接发送数据。 由于大家不知道网络拥塞状况，同时发送数据，导致中间节点阻塞掉包，谁也发不了数据，所以就有了滑动窗口机制来解决此问题。参见滑动窗口如何根据网络拥塞发送数据仿真视频。 滑动窗口协议是用来改善吞吐量的一种技术，即容许发送方在接收任何应答之前传送附加的包。接收方告诉发送方在某一时刻能送多少包（称窗口尺寸）。 CP中采用滑动窗口来进行传输控制，滑动窗口的大小意味着接收方还有多大的缓冲区可以用于接收数据。 发送方可以通过滑动窗口的大小来确定应该发送多少字节的数据。当滑动窗口为0时，发送方一般不能再发送数据报，但有两种情况除外，一种情况是可以发送紧急数据，例如，允许用户终止在远端机上的运行进程。 另一种情况是发送方可以发送一个1字节的数据报来通知接收方重新声明它希望接收的下一字节及发送方的滑动窗口大小。 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:7","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Epoll和Select的区别 epoll 和 select 都是 I/O 多路复用的技术，都可以实现同时监听多个 I/O 事件的状态。 epoll 相比 select 效率更高，主要是基于其操作系统支持的I/O事件通知机制，而 select 是基于轮询机制。 epoll 支持水平触发和边沿触发两种模式。 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:8","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"进程之间为什么要进行通信呢 数据传输：一个进程需要将它的数据发送给另一个进程，发送的数据量在一个字节到几兆字节之间。 共享数据：多个进程想要操作共享数据，一个进程对共享数据的修改，别的进程应该立刻看到。 通知事件：一个进程需要向另一个或一组进程发送消息，通知它（它们）发生了某种事件（如进程终止时要通知父进程）。 资源共享：多个进程之间共享同样的资源。为了作到这一点，需要内核提供锁和同步机制。 进程控制：有些进程希望完全控制另一个进程的执行（如Debug进程），此时控制进程希望能够拦截另一个进程的所有陷入和异常，并能够及时知道它的状态改变。 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:9","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"输入PingIP后敲回车,发包前会发生什么 首先根据目的IP和路由表决定走哪个网卡，再根据网卡的子网掩码地址判断目的IP是否在子网内。如果不在则会通过arp缓存查询IP的网卡地址，不存在的话会通过广播询问目的IP的mac地址，得到后就开始发包了，同时mac地址也会被arp缓存起来。 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:10","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"进程和进程间的通信方式区别和不同 进程: 计算机中是资源分配的基本单位。 进程控制块 (Process Control Block, PCB) 描述进程的基本信息和运行状态，所谓的创建进程和撤销进程，都是指对 PCB 的操作。 线程: 是独立调度的基本单位。 一个进程中可以有多个线程，它们共享进程资源。 例如QQ 和浏览器是两个进程，浏览器进程里面有很多线程，例如 HTTP 请求线程、事件响应线程、渲染线程等等，线程的并发执行使得在浏览器中点击一个新链接从而发起 HTTP 请求时，浏览器还可以响应用户的其它事件。 在 linux 下进程间通信主要是： 管道（Pipe）及有名管道（named pipe）：管道可用于具有亲缘关系进程间的通信，有名管道克服了管道没有名字的限制，因此，除具有管道所具有的功能外，它还允许无亲缘关系进程间的通信； 信号（Signal）：信号是比较复杂的通信方式，用于通知接受进程有某种事件发生，除了用于进程间通信外，进程还可以发送信号给进程本身；linux除了支持Unix早期信号语义函数sigal外，还支持语义符合Posix.1标准的信号函数sigaction（实际上，该函数是基于BSD的，BSD为了实现可靠信号机制，又能够统一对外接口，用sigaction函数重新实现了signal函数）； 消息队列（Message）：消息队列是消息的链接表，包括Posix消息队列system V消息队列。有足够权限的进程可以向队列中添加消息，被赋予读权限的进程则可以读走队列中的消息。消息队列克服了信号承载信息量少，管道只能承载无格式字节流以及缓冲区大小受限等缺点。 共享内存：使得多个进程可以访问同一块内存空间，是最快的可用IPC形式。是针对其他通信机制运行效率较低而设计的。往往与其它通信机制，如信号量结合使用，来达到进程间的同步及互斥。 信号量（semaphore）：主要作为进程间以及同一进程不同线程之间的同步手段。 套接口（Socket）：更为一般的进程间通信机制，可用于不同机器之间的进程间通信。起初是由Unix系统的BSD分支开发出来的，但现在一般可以移植到其它类Unix系统上：Linux和System V的变种都支持套接字。 ","date":"2022-10-09","objectID":"/go-interview-summary/:6:11","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"如何查看二进制可执行文件引用的动态链接库 linux中可以用ldd查看，macOs中可以用otool可以查看. ","date":"2022-10-09","objectID":"/go-interview-summary/:6:12","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Algorithm和Structures ","date":"2022-10-09","objectID":"/go-interview-summary/:7:0","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"哪些排序算法是稳定的 选择排序、快速排序、希尔排序、堆排序不是稳定的排序算法，冒泡排序、插入排序、归并排序和基数排序是稳定的排序算法. 排序算法的稳定性这个应该是清晰明了的，通俗地讲就是能保证排序前2个相等的数其在序列的前后位置顺序和排序后它们两个的前后位置顺序相同。在简单形式化一下，如果Ai = Aj，Ai原来在位置前，排序后Ai还是要在Aj位置前。 其次，说一下稳定性的好处。排序算法如果是稳定的，那么从一个键上排序，然后再从另一个键上排序，第一个键排序的结果可以为第二个键排序所用。 基数排序就是这样，先按低位排序，逐次按高位排序，低位相同的元素其顺序再高位也相同时是不会改变的。另外，如果排序算法稳定，对基于比较的排序算法而言，元素交换的次数可能会少一些。 现在我们来分析一下常见的排序算法的稳定性。 (1)冒泡排序 冒泡排序就是把小的元素往前调或者把大的元素往后调。比较是相邻的两个元素比较，交换也发生在这两个元素之间。 所以，如果两个元素相等，我想你是不会再无聊地把他们俩交换一下的；如果两个相等的元素没有相邻，那么即使通过前面的两两交换把两个相邻起来，这时候也不会交换，所以相同元素的前后顺序并没有改变，所以冒泡排序是一种稳定排序算法。 (2)选择排序 选择排序是给每个位置选择当前元素最小的，比如给第一个位置选择最小的，在剩余元素里面给第二个元素选择第二小的，依次类推，直到第n - 1个元素，第n个元素不用选择了，因为只剩下它一个最大的元素了。那么，在一趟选择，如果当前元素比一个元素小，而该小的元素又出现在一个和当前元素相等的元素后面，那么交换后稳定性就被破坏了。 比较拗口，举个例子，序列5 8 5 2 9，我们知道第一遍选择第1个元素5会和2交换，那么原序列中2个5的相对前后顺序就被破坏了，所以选择排序不是一个稳定的排序算法。 (3)插入排序 插入排序是在一个已经有序的小序列的基础上，一次插入一个元素。当然，刚开始这个有序的小序列只有1个元素，就是第一个元素。比较是从有序序列的末尾开始，也就是想要插入的元素和已经有序的最大者开始比起，如果比它大则直接插入在其后面，否则一直往前找直到找到它该插入的位置。 如果碰见一个和插入元素相等的，那么插入元素把想插入的元素放在相等元素的后面。所以，相等元素的前后顺序没有改变，从原无序序列出去的顺序就是排好序后的顺序，所以插入排序是稳定的。 (4)快速排序 快速排序有两个方向，左边的i下标一直往右走，当a[i] \u003c= a[center index]，其中center_index是中枢元素的数组下标，一般取为数组第0个元素。而右边的j下标一直往左走，当a[j] \u003e a[center_index]。如果i和j都走不动了，i \u003c= j，交换a[i]和a[j],重复上面的过程，直到i \u003e j。 交换a[j]和a[center_index]，完成一趟快速排序。 在中枢元素和a[j]交换的时候，很有可能把前面的元素的稳定性打乱，比如序列为5 3 3 4 3 8 9 10 11，现在中枢元素5和3（第5个元素，下标从1开始计）交换就会把元素3的稳定性打乱，所以快速排序是一个不稳定的排序算法，不稳定发生在中枢元素和a[j]交换的时刻。 (5)归并排序 归并排序是把序列递归地分成短序列，递归出口是短序列只有1个元素（认为直接有序）或者2个序列（1次比较和交换），然后把各个有序的段序列合并成一个有序的长序列，不断合并直到原序列全部排好序。可以发现，在1个或2个元素时，1个元素不会交换，2个元素如果大小相等也没有人故意交换，这不会破坏稳定性。那么，在短的有序序列合并的过程中，稳定是是否受到破坏？没有，合并过程中我们可以保证如果两个当前元素相等时，我们把处在前面的序列的元素保存在结果序列的前面，这样就保证了稳定性。所以，归并排序也是稳定的排序算法。 (6)基数排序 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序，最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。基数排序基于分别排序，分别收集，所以其是稳定的排序算法。 (7)希尔排序(shell) 希尔排序是按照不同步长对元素进行插入排序，当刚开始元素很无序的时候，步长最大，所以插入排序的元素个数很少，速度很快；当元素基本有序了，步长很小， 插入排序对于有序的序列效率很高。所以，希尔排序的时间复杂度会比O(n^2)好一些。由于多次插入排序，我们知道一次插入排序是稳定的，不会改变相同元素的相对顺序，但在不同的插入排序过程中，相同的元素可能在各自的插入排序中移动，最后其稳定性就会被打乱，所以shell排序是不稳定的。 (8)堆排序 我们知道堆的结构是节点i的孩子为2 * i和2 * i + 1节点，大顶堆要求父节点大于等于其2个子节点，小顶堆要求父节点小于等于其2个子节点。在一个长为n 的序列，堆排序的过程是从第n / 2开始和其子节点共3个值选择最大（大顶堆）或者最小（小顶堆），这3个元素之间的选择当然不会破坏稳定性。但当为n / 2 - 1， n / 2 - 2， … 1这些个父节点选择元素时，就会破坏稳定性。有可能第n / 2个父节点交换把后面一个元素交换过去了，而第n / 2 - 1个父节点把后面一个相同的元素没 有交换，那么这2个相同的元素之间的稳定性就被破坏了。 所以，堆排序不是稳定的排序算法。 因此会有: 选择排序、快速排序、希尔排序、堆排序不是稳定的排序算法，而冒泡排序、插入排序、归并排序和基数排序是稳定的排序算法 ","date":"2022-10-09","objectID":"/go-interview-summary/:7:1","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"给定一个二叉树,判断其是否是一个有效的二叉搜索树 什么是二叉树（Binary Tree)? 每个结点至多拥有两棵子树的树结构(即二叉树中不存在度大于2的结点)。并且，二叉树的子树有左右之分，其次序不能任意颠倒。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）。二叉树常被用于实现二叉查找树和二叉堆。 上面概念中提到了“度”的概念，“度”其实就是某个节点子节点的数量。如果某个节点的子节点数量为1，则该节点的度为1，如果有8个子节点，则度为8，以此类推。 假设一个二叉搜索树具有如下特征： 节点的左子树只包含小于当前节点的数。 节点的右子树只包含大于当前节点的数。 所有左子树和右子树自身必须也是二叉搜索树。 示例 1: 输入: 2 / \\ 1 3 输出: true 示例 2: 输入: 5 / \\ 1 4 / \\ 3 6 输出: false 解释: 输入为: [5,1,4,null,null,3,6]。 根节点的值为 5 ，但是其右子节点值为 4 。 可以直接按照定义比较大小，比 root 节点小的都在左边，比 root 节点大的都在右边: type TreeNode struct { Val int Left *TreeNode Right *TreeNode } func isValidBST(root *TreeNode) bool { return isValid(root, math.MinInt64, math.MaxInt64) } func isValid(root *TreeNode, min, max int) bool { if root == nil { return true } if root.Val \u003c= min { return false } if root.Val \u003e= max { return false } return isValid(root.Left, min, root.Val) \u0026\u0026 isValid(root.Right, root.Val, max) } ","date":"2022-10-09","objectID":"/go-interview-summary/:7:2","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"排序算法 排序算法又分为稳定性算法和不稳定性算法： 稳定的排序算法：冒泡排序、插入排序、归并排序和基数排序。 不是稳定的排序算法：选择排序、快速排序、希尔排序、堆排序。 冒泡排序 func main() { var arr = []int{9,10,11,5,3,4,27,2,1,3,20} //升序 bubbleAscendingSort(arr) //降序 bubbleDescendingSort(arr) } //升序 func bubbleAscendingSort(arr []int) { for i :=0; i \u003c len(arr)-1; i++ { for j := i+1; j\u003c len(arr); j++ { if (arr[i] \u003e arr[j]) { arr[i],arr[j] = arr[j],arr[i] } } } fmt.Println(\"bubbleAscendingSort:\",arr) } //降序 func bubbleDescendingSort(arr []int) { for i :=0; i \u003c len(arr)-1; i++ { for j := i+1; j\u003c len(arr); j++ { if (arr[i] \u003c arr[j]) { arr[i],arr[j] = arr[j],arr[i] } } } fmt.Println(\"bubbleDescendingSort:\",arr) } 运行结果: bubbleAscendingSort: [1 2 3 3 4 5 9 10 11 20 27] bubbleDescendingSort: [27 20 11 10 9 5 4 3 3 2 1] 选择排序 func main() { var arr = []int{19,28,17,5,13,4,6,7,9,3,10} //升序 selectAscendingSort(arr) //降序 selectDescendingSort(arr) } //升序 func selectAscendingSort(arr []int) { l := len(arr) m := len(arr) - 1 for i := 0; i \u003c m; i++ { k := i for j := i+1; j \u003c l; j++ { if arr[k] \u003e arr[j] { k = j } } if k != i { arr[k],arr[i] = arr[i],arr[k] } } fmt.Println(\"selectAscendingSort:\",arr) } //降序 func selectDescendingSort(arr []int) { l := len(arr) m := len(arr) - 1 for i := 0; i \u003c m; i++ { k := i for j := i+1; j \u003c l; j++ { if arr[k] \u003c arr[j] { k = j } } if k != i { arr[k],arr[i] = arr[i],arr[k] } } fmt.Println(\"selectDescendingSort:\",arr) } 运行结果: selectDescendingSort: [3 4 5 6 7 9 10 13 17 19 28] selectAscendingSort: [28 19 17 13 10 9 7 6 5 4 3] 插入排序 func main() { var arr = []int{19,13,27,15,3,4,26,12,1,0} insertSort(arr) fmt.Println(\"insertSort:\",arr) } func insertSort(arr []int) { n := len(arr) if n \u003c 2 { return } for i := 1; i \u003c n; i++ { for j := i; j \u003e0 \u0026\u0026 arr[j] \u003c arr[j-1]; j-- { arr[j], arr[j-1] = arr[j-1], arr[j] } } } 运行结果: insertSort: [0 1 3 4 12 13 15 19 26 27] 希尔排序 func main() { var arr = []int{19,8,27,15,3,17,6,2,1,0} shellSort(arr) fmt.Println(\"shellSort:\",arr) } func shellSort(arr []int) { n := len(arr) h := 1 //寻找合适的间隔h for h \u003c n/3 { h = 3*h +1 } for h \u003e= 1 { for i := h; i \u003c n; i++ { for j := i; j \u003e= h \u0026\u0026 arr[j] \u003c arr[j-1]; j -= h { arr[j], arr[j-1] = arr[j-1], arr[j] } } h /= 3 } } 运行结果: shellSort: [0 1 2 3 6 8 15 17 19 27] 归并排序 func main() { array := []int{55, 94, 87, 12, 4, 32, 11,8, 39, 42, 64, 53, 70, 12, 9} fmt.Println(\"before MergeSort\",array) array = MergeSort(array) fmt.Println(\"after MergeSort:\",array) } func MergeSort(array []int) []int{ n := len(array) if n \u003c 2 { return array } key := n / 2 left := MergeSort(array[0:key]) right := MergeSort(array[key:]) return merge(left, right) } func merge(left []int, right []int) []int{ tmp := make([]int, 0) i, j := 0,0 for i \u003c len(left) \u0026\u0026 j \u003c len(right) { if left[i] \u003c right[j]{ tmp = append(tmp, left[i]) i ++ }else{ tmp = append(tmp, right[j]) j ++ } } tmp = append(tmp, left[i:]...) tmp = append(tmp, right[j:]...) return tmp } 运行结果: before MergeSort [55 94 87 12 4 32 11 8 39 42 64 53 70 12 9] after MergeSort: [4 8 9 11 12 12 32 39 42 53 55 64 70 87 94] 快速排序 func main() { var arr = []int{19,8,16,15,23,34,6,3,1,0,2,9,7} quickAscendingSort(arr, 0, len(arr)-1) fmt.Println(\"quickAscendingSort:\",arr) quickDescendingSort(arr, 0, len(arr)-1) fmt.Println(\"quickDescendingSort:\",arr) } //升序 func quickAscendingSort(arr []int, start, end int) { if (start \u003c end) { i, j := start, end key := arr[(start + end)/2] for i \u003c= j { for arr[i] \u003c key { i++ } for arr[j] \u003e key { j-- } if i \u003c= j { arr[i], arr[j] = arr[j], arr[i] i++ j-- } } if start \u003c j { quickAscendingSort(arr, start, j) } if end \u003e i { quickAscendingSort(arr, i, end) } } } //降序 func quickDescendingSort(arr []int, start, end int) { if (start \u003c end) { i, j := start, end key := arr[(start + end)/2] for i \u003c= j { for arr[i] \u003e key { i++ } for arr[j] \u003c key { j-- } if i \u003c= j { arr[i], arr[j] = arr[j], arr[i] i++ j-- } } if start \u003c j { quickDescendingSort(arr, start, j) } if end \u003e i { quickDescendingSort(arr, i, end) } } } 运行结果: quickAscendingSort: [0 1 2 3 6 7 8 9 15 16 19 23 34] quickDescendingSort: [34 23 19 16 15 9 8 7","date":"2022-10-09","objectID":"/go-interview-summary/:7:3","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"如何通过递归反转单链表 链表是一种物理存储单元上非连续、非顺序的存储结构，数据元素的逻辑顺序是通过链表中的指针链接次序实现的。链表由一系列结点（链表中每一个元素称为结点）组成，结点可以在运行时动态生成。 每个结点包括两个部分：一个是存储数据元素的数据域，另一个是存储下一个结点地址的指针域。 相比于线性表顺序结构，操作复杂。 由于不必须按顺序存储，链表在插入的时候可以达到O(1)的复杂度，比另一种线性表顺序表快得多，但是查找一个节点或者访问特定编号的节点则需要O(n)的时间，而线性表和顺序表相应的时间复杂度分别是O(logn)和O(1)。 使用链表结构可以克服数组链表需要预先知道数据大小的缺点，链表结构可以充分利用计算机内存空间，实现灵活的内存动态管理。但是链表失去了数组随机读取的优点，同时链表由于增加了结点的指针域，空间开销比较大。 链表最明显的好处就是，常规数组排列关联项目的方式可能不同于这些数据项目在记忆体或磁盘上顺序，数据的存取往往要在不同的排列顺序中转换。链表允许插入和移除表上任意位置上的节点，但是不允许随机存取。 链表有很多种不同的类型：单向链表，双向链表以及循环链表。链表可以在多种编程语言中实现。像Lisp和Scheme这样的语言的内建数据类型中就包含了链表的存取和操作。 package main import \"fmt\" // 通过递归反转单链表 type Node struct { Value int NextNode *Node } func Param(node *Node){ for node !=nil{ fmt.Print(node.Value,\"---\u003e\") node = node.NextNode } fmt.Println() } func reverse(headNode *Node) *Node{ if headNode ==nil { return headNode } if headNode.NextNode == nil{ return headNode } var newNode = reverse(headNode.NextNode) headNode.NextNode.NextNode = headNode headNode.NextNode = nil return newNode } func main() { var node1 = \u0026Node{} node1.Value = 1 node2 := new(Node) node2.Value = 2 node3 := new(Node) node3.Value = 3 node4 := new(Node) node4.Value = 4 node1.NextNode = node2 node2.NextNode = node3 node3.NextNode = node4 Param(node1) reverseNode := reverse(node1) Param(reverseNode) } 运行结果: 1---\u003e2---\u003e3---\u003e4---\u003e 4---\u003e3---\u003e2---\u003e1---\u003e ","date":"2022-10-09","objectID":"/go-interview-summary/:7:4","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"链表和数组相比有什么优缺点 数组是一块连续的空间，声明时长度就要确定,链表是一块不连续的动态空间，长度可变,数组的优点是速度快，数据操作直接使用偏移地址， 链表需要按顺序检索节点，效率低,链表的优点是可以快速插入和删除节点，大小动态分配长度不需要固定. 链表不存在越界问题，数组有越界问题. ","date":"2022-10-09","objectID":"/go-interview-summary/:7:5","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"通常一般会用到哪些数据结构 数据结构是计算机存储、组织数据的方式。对于特定的数据结构(比如数组)，有些操作效率很高(读某个数组元素)，有些操作的效率很低(删除某个数组元素)。开发者的目标是为当前的问题选择最优的数据结构。 数组:是最常用的数据结构了。其他数据结构，比如栈和队列都是由数组衍生出来的。 栈:是限定仅表尾进行插入和删除操作的线性表 。 队列 链表 图 树 前缀树 哈希表 算法可大致分为基本算法、数据结构的算法、数论与代数算法、计算几何的算法、图论的算法、动态规划以及数值分析、加密算法、排序算法、检索算法、随机化算法、并行算法，厄米变形模型，随机森林算法。 数据对象的运算和操作：计算机可以执行的基本操作是以指令的形式描述的。一个计算机系统能执行的所有指令的集合，成为该计算机系统的指令系统。 一个计算机的基本运算和操作有如下四类： 算术运算：加减乘除等运算. 逻辑运算：或、且、非等运算. 关系运算：大于、小于、等于、不等于等运算. 数据传输：输入、输出、赋值等运算. ","date":"2022-10-09","objectID":"/go-interview-summary/:7:6","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"其他基础 ","date":"2022-10-09","objectID":"/go-interview-summary/:8:0","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"中间件原理 中间件（middleware）是基础软件的一大类，属于可复用软件的范畴。 中间件处于操作系统软件与用户的应用软件的中间。中间件在操作系统、网络和数据库之上，应用软件的下层，总的作用是为处于自己上层的应用软件提供运行与开发的环境，帮助用户灵活、高效地开发和集成复杂的应用软件. IDC的定义是：中间件是一种独立的系统软件或服务程序，分布式应用软件借助这种软件在不同的技术之间共享资源，中间件位于客户机服务器的操作系统之上，管理计算资源和网络通信。 中间件解决的问题是： 在中间件产生以前，应用软件直接使用操作系统、网络协议和数据库等开发，这些都是计算机最底层的东西，越底层越复杂，开发者不得不面临许多很棘手的问题，如操作系统的多样性，繁杂的网络程序设计、管理，复杂多变的网络环境，数据分散处理带来的不一致性问题、性能和效率、安全，等等。 这些与用户的业务没有直接关系，但又必须解决，耗费了大量有限的时间和精力。于是，有人提出能不能将应用软件所要面临的共性问题进行提炼、抽象，在操作系统之上再形成一个可复用的部分，供成千上万的应用软件重复使用。这一技术思想最终构成了中间件这类的软件。 中间件屏蔽了底层操作系统的复杂性，使程序开发人员面对一个简单而统一的开发环境，减少程序设计的复杂性，将注意力集中在自己的业务上，不必再为程序在不同系统软件上的移植而重复工作，从而大大减少了技术上的负担。 ","date":"2022-10-09","objectID":"/go-interview-summary/:8:1","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Hash冲突有什么解决办法 解决hash冲突的办法目前主要有四种: 开放定址法（线性探测再散列，二次探测再散列，伪随机探测再散列）. 再哈希法. 链地址法. 建立一个公共溢出区. ","date":"2022-10-09","objectID":"/go-interview-summary/:8:2","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"微服务架构是什么样子的 通常传统的项目体积庞大，需求、设计、开发、测试、部署流程固定。新功能需要在原项目上做修改。 但是微服务可以看做是对大项目的拆分，是在快速迭代更新上线的需求下产生的。新的功能模块会发布成新的服务组件，与其他已发布的服务组件一同协作。 服务内部有多个生产者和消费者，通常以http rest的方式调用，服务总体以一个（或几个）服务的形式呈现给客户使用。 微服务架构是一种思想对微服务架构我们没有一个明确的定义，但简单来说微服务架构是： 采用一组服务的方式来构建一个应用，服务独立部署在不同的进程中，不同服务通过一些轻量级交互机制来通信，例如 RPC、HTTP 等，服务可独立扩展伸缩，每个服务定义了明确的边界，不同的服务甚至可以采用不同的编程语言来实现，由独立的团队来维护。 Golang的微服务框架kit中有详细的微服务的例子,可以参考学习. 微服务架构设计包括: 服务熔断降级限流机制 熔断降级的概念(Rate Limiter 限流器,Circuit breaker 断路器). 框架调用方式解耦方式 Kit 或 Istio 或 Micro 服务发现(consul zookeeper kubeneters etcd ) RPC调用框架. 链路监控,zipkin和prometheus. 多级缓存. 网关 (kong gateway). Docker部署管理 Kubenetters. 自动集成部署 CI/CD 实践. 自动扩容机制规则. 压测 优化. Trasport 数据传输(序列化和反序列化). Logging 日志. Metrics 指针对每个请求信息的仪表盘化. 微服务架构介绍详细的可以参考: Microservice Architectures ","date":"2022-10-09","objectID":"/go-interview-summary/:8:3","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"分布式锁实现 在分析分布式锁的三种实现方式之前，先了解一下分布式锁应该具备哪些条件： 在分布式系统环境下，一个方法在同一时间只能被一个机器的一个线程执行； 高可用的获取锁与释放锁； 高性能的获取锁与释放锁； 具备可重入特性； 具备锁失效机制，防止死锁； 具备非阻塞锁特性，即没有获取到锁将直接返回获取锁失败。 分布式的CAP理论告诉我们“任何一个分布式系统都无法同时满足一致性（Consistency）、可用性（Availability）和分区容错性（Partition tolerance），最多只能同时满足两项”。 所以，很多系统在设计之初就要对这三者做出取舍。在互联网领域的绝大多数的场景中，都需要牺牲强一致性来换取系统的高可用性，系统往往只需要保证“最终一致性”，只要这个最终时间是在用户可以接受的范围内即可。 通常分布式锁以单独的服务方式实现，目前比较常用的分布式锁实现有三种： 基于数据库实现分布式锁。 基于缓存Redis实现分布式锁。 基于Etcd实现分布式锁。 尽管有这三种方案，但是不同的业务也要根据自己的情况进行选型，他们之间没有最好只有更适合！ 基于数据库的实现方式 基于数据库的实现方式的核心思想是,在数据库中创建一个表，表中包含方法名等字段，并在方法名字段上创建唯一索引，想要执行某个方法，就使用这个方法名向表中插入数据，成功插入则获取锁，执行完成后删除对应的行数据释放锁。 使用基于数据库的这种实现方式很简单，但是对于分布式锁应该具备的条件来说，它有一些问题需要解决及优化： 1、因为是基于数据库实现的，数据库的可用性和性能将直接影响分布式锁的可用性及性能，所以，数据库需要双机部署、数据同步、主备切换； 2、不具备可重入的特性，因为同一个线程在释放锁之前，行数据一直存在，无法再次成功插入数据，所以，需要在表中新增一列，用于记录当前获取到锁的机器和线程信息，在再次获取锁的时候，先查询表中机器和线程信息是否和当前机器和线程相同，若相同则直接获取锁； 3、没有锁失效机制，因为有可能出现成功插入数据后，服务器宕机了，对应的数据没有被删除，当服务恢复后一直获取不到锁，所以，需要在表中新增一列，用于记录失效时间，并且需要有定时任务清除这些失效的数据； 4、不具备阻塞锁特性，获取不到锁直接返回失败，所以需要优化获取逻辑，循环多次去获取。 5、在实施的过程中会遇到各种不同的问题，为了解决这些问题，实现方式将会越来越复杂；依赖数据库需要一定的资源开销，性能问题需要考虑。 基于Redis的实现方式 利用Redis的 SetNX来实现分布式锁,性能高, edis可持久化，也能保证数据不易丢失,redis集群方式提高稳定性。 基于Etcd实现分布式锁 Etcd提供了实现分布式锁的特性: Raft一致性，是工程上使用较为广泛，强一致性、去中心化、高可用的分布式协议。 Raft提供了分布式系统的可靠性功能。详细的查看Raft. Lease功能 Lease功能，就是租约机制(TimeToLive,即TTL)。 Etcd可以对存储Key Value的数据设置租约，也就是给Key Value设置一个过期时间，当租约到期，Key Value将会失效而被Etcd删除。 Etcd同时也支持续约租期，可以通过客户端在租约到期之间续约，以避免Key Value失效； Etcd还支持解约，一旦解约，与该租约绑定的Key Value将会失效而删除。 Lease 功能可以保证分布式锁的安全性，为锁对应的 key配置租约，即使锁的持有者因故障而不能主动释放锁，锁也会因租约到期而自动释放。 Watch功能 监听功能。Watch机制支持监听某个固定的key，它也支持Watch一个范围（前缀机制），当被watch的key或范围发生变化时，客户端将收到通知。 在实现分布式锁时，如果抢锁失败，可通过 Prefix 机制返回的Key Value列表获得 Revision 比自己小且相差最小的 key（称为 pre-key），对 pre-key 进行监听，因为只有它释放锁，自己才能获得锁，如果 Watch 到 pre-key 的 DELETE 事件，则说明pre-ke已经释放，自己已经持有锁。 Prefix功能 前缀机制: 目录机制，如两个 key 命名如下：key1=“/mykey/key1″ , key2=”/mykey/key2″，那么，可以通过前缀“/mykey”查询，返回包含两个 Key Value对的列表。可以和前面的watch功能配合使用。 通常呢, 例如我们创建一个名为 /mylock 的锁，两个争抢它的客户端进行写操作，实际写入的 key 分别为：key1=”/mylock/UUID1″，key2=”/mylock/UUID2″，其中，UUID 表示全局唯一的 ID，确保两个 key 的唯一性。 很显然，写操作都会成功，但返回的Revision 不一样，那么，如何判断谁获得了锁呢？通过前缀 /mylock 查询，返回包含两个Key Value对的的 Key Value列表，同时也包含它们的 Revision，通过 Revision 大小，客户端可以判断自己是否获得锁，如果抢锁失败，则等待锁释放（对应的 key 被删除或者租约过期），然后再判断自己是否可以获得锁。 Lease 功能和 Prefix功能，能解决上面的死锁问题。 Revision功能 每个 key 带有一个 Revision 号，每进行一次事务加一，因此它是全局唯一的，如初始值为 0，进行一次 put(key, value)，key 的 Revision 变为 1； 同样的操作，再进行一次，Revision 变为 2；换成 key1 进行 put(key1, value) 操作，Revision 将变为 3。 这种机制有一个作用： 通过 Revision 的大小就可以知道进行写操作的顺序。在实现分布式锁时，多个客户端同时抢锁，根据 Revision 号大小依次获得锁，可以避免\"惊群效应\"，实现公平锁。 ","date":"2022-10-09","objectID":"/go-interview-summary/:8:4","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"负载均衡原理是什么 负载均衡Load Balance）是高可用网络基础架构的关键组件，通常用于将工作负载分布到多个服务器来提高网站、应用、数据库或其他服务的性能和可靠性。负载均衡，其核心就是网络流量分发，分很多维度。 负载均衡（Load Balance）通常是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其它关键任务服务器等，从而共同完成工作任务。 负载均衡是建立在现有网络结构之上，它提供了一种廉价有效透明的方法扩展网络设备和服务器的带宽、增加吞吐量、加强网络数据处理能力、提高网络的灵活性和可用性。 下面通过一个例子详细介绍: 没有负载均衡 web 架构 在这里用户是直连到 web 服务器，如果这个服务器宕机了，那么用户自然也就没办法访问了。另外，如果同时有很多用户试图访问服务器，超过了其能处理的极限，就会出现加载速度缓慢或根本无法连接的情况。 而通过在后端引入一个负载均衡器和至少一个额外的 web 服务器，可以缓解这个故障。通常情况下，所有的后端服务器会保证提供相同的内容，以便用户无论哪个服务器响应，都能收到一致的内容。 有负载均衡 web 架构 用户访问负载均衡器，再由负载均衡器将请求转发给后端服务器。在这种情况下，单点故障现在转移到负载均衡器上了。这里又可以通过引入第二个负载均衡器来缓解。 那么负载均衡器的工作方式是什么样的呢,负载均衡器又可以处理什么样的请求？ 负载均衡器的管理员能主要为下面四种主要类型的请求设置转发规则: HTTP (七层) HTTPS (七层) TCP (四层) UDP (四层) 负载均衡器如何选择要转发的后端服务器？ 负载均衡器一般根据两个因素来决定要将请求转发到哪个服务器。首先，确保所选择的服务器能够对请求做出响应，然后根据预先配置的规则从健康服务器池（healthy pool）中进行选择。 因为，负载均衡器应当只选择能正常做出响应的后端服务器，因此就需要有一种判断后端服务器是否健康的方法。为了监视后台服务器的运行状况，运行状态检查服务会定期尝试使用转发规则定义的协议和端口去连接后端服务器。 如果服务器无法通过健康检查，就会从池中剔除，保证流量不会被转发到该服务器，直到其再次通过健康检查为止。 负载均衡算法 负载均衡算法决定了后端的哪些健康服务器会被选中。 其中常用的算法包括： Round Robin（轮询）：为第一个请求选择列表中的第一个服务器，然后按顺序向下移动列表直到结尾，然后循环。 Least Connections（最小连接）：优先选择连接数最少的服务器，在普遍会话较长的情况下推荐使用。 Source：根据请求源的 IP 的散列（hash）来选择要转发的服务器。这种方式可以一定程度上保证特定用户能连接到相同的服务器。 如果你的应用需要处理状态而要求用户能连接到和之前相同的服务器。可以通过 Source 算法基于客户端的 IP 信息创建关联，或者使用粘性会话（sticky sessions）。 除此之外，想要解决负载均衡器的单点故障问题，可以将第二个负载均衡器连接到第一个上，从而形成一个集群。 ","date":"2022-10-09","objectID":"/go-interview-summary/:8:5","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"互斥锁和读写锁和死锁问题是怎么解决 互斥锁 互斥锁就是互斥变量mutex，用来锁住临界区的. 条件锁就是条件变量，当进程的某些资源要求不满足时就进入休眠，也就是锁住了。当资源被分配到了，条件锁打开，进程继续运行；读写锁，也类似，用于缓冲区等临界资源能互斥访问的。 读写锁 通常有些公共数据修改的机会很少，但其读的机会很多。并且在读的过程中会伴随着查找，给这种代码加锁会降低我们的程序效率。读写锁可以解决这个问题。 注意：写独占，读共享，写锁优先级高 死锁 一般情况下，如果同一个线程先后两次调用lock，在第二次调用时，由于锁已经被占用，该线程会挂起等待别的线程释放锁，然而锁正是被自己占用着的，该线程又被挂起而没有机会释放锁，因此就永远处于挂起等待状态了，这叫做死锁（Deadlock）。 另外一种情况是：若线程A获得了锁1，线程B获得了锁2，这时线程A调用lock试图获得锁2，结果是需要挂起等待线程B释放锁2，而这时线程B也调用lock试图获得锁1，结果是需要挂起等待线程A释放锁1，于是线程A和B都永远处于挂起状态了。 死锁产生的四个必要条件: 互斥条件：一个资源每次只能被一个进程使用. 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件:进程已获得的资源，在未使用完之前，不能强行剥夺。 循环等待条件:若干进程之间形成一种头尾相接的循环等待资源关系。 这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之一不满足，就不会发生死锁。 a. 预防死锁 可以把资源一次性分配:(破坏请求和保持条件). 然后剥夺资源：即当某进程新的资源未满足时，释放已占有的资源（破坏不可剥夺条件）. 资源有序分配法：系统给每类资源赋予一个编号，每一个进程按编号递增的顺序请求资源，释放则相反（破坏环路等待条件）. b. 避免死锁 预防死锁的几种策略，会严重地损害系统性能。因此在避免死锁时，要施加较弱的限制，从而获得 较满意的系统性能。由于在避免死锁的策略中，允许进程动态地申请资源。因而，系统在进行资源分配之前预先计算资源分配的安全性。 若此次分配不会导致系统进入不安全状态，则将资源分配给进程；否则，进程等待。其中最具有代表性的避免死锁算法是银行家算法。 c. 检测死锁 首先为每个进程和每个资源指定一个唯一的号码,然后建立资源分配表和进程等待表. d. 解除死锁 当发现有进程死锁后，便应立即把它从死锁状态中解脱出来，常采用的方法有. e. 剥夺资源 从其它进程剥夺足够数量的资源给死锁进程，以解除死锁状态. f. 撤消进程 可以直接撤消死锁进程或撤消代价最小的进程，直至有足够的资源可用，死锁状态.消除为止.所谓代价是指优先级、运行代价、进程的重要性和价值等。 ","date":"2022-10-09","objectID":"/go-interview-summary/:8:6","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Etcd中的Raft一致性算法原理 Etcd中的Raft一致性算法原理： Raft将系统中的角色分为领导者（Leader）、跟从者（Follower）和候选人（Candidate）. Leader：接受客户端请求，并向Follower同步请求日志，当日志同步到大多数节点上后告诉Follower提交日志。 Follower：接受并持久化Leader同步的日志，在Leader告之日志可以提交之后，提交日志。 Candidate：Leader选举过程中的临时角色。 Raft要求系统在任意时刻最多只有一个Leader，正常工作期间只有Leader和Followers。 Raft算法角色状态转换如下： Follower只响应其他服务器的请求。如果Follower超时没有收到Leader的消息，它会成为一个Candidate并且开始一次Leader选举。收到大多数服务器投票的Candidate会成为新的Leader。Leader在宕机之前会一直保持Leader的状态。 Raft算法将时间分为一个个的任期（term），每一个term的开始都是Leader选举。在成功选举Leader之后，Leader会在整个term内管理整个集群。如果Leader选举失败，该term就会因为没有Leader而结束。 Leader选举: Raft 使用心跳（heartbeat）触发Leader选举。当服务器启动时，初始化为Follower。Leader向所有Followers周期性发送heartbeat。如果Follower在选举超时时间内没有收到Leader的heartbeat，就会等待一段随机的时间后发起一次Leader选举。 Follower将其当前term加一然后转换为Candidate。它首先给自己投票并且给集群中的其他服务器发送 RequestVote RPC （RPC细节参见八、Raft算法总结）。 结果有以下三种情况: 1.赢得了多数的选票，成功选举为Leader； 2.收到了Leader的消息，表示有其它服务器已经抢先当选了Leader； 3.没有服务器赢得多数的选票，Leader选举失败，等待选举时. Raft日志同步保证如下两点： 如果不同日志中的两个条目有着相同的索引和任期号，则它们所存储的命令是相同的。 如果不同日志中的两个条目有着相同的索引和任期号，则它们之前的所有条目都是完全一样的。 Leader通过强制Followers复制它的日志来处理日志的不一致，Followers上的不一致的日志会被Leader的日志覆盖。Leader为了使Followers的日志同自己的一致，Leader需要找到Followers同它的日志一致的地方，然后覆盖Followers在该位置之后的条目。 Leader会从后往前试，每次AppendEntries失败后尝试前一个日志条目，直到成功找到每个Follower的日志一致位点，然后向后逐条覆盖Followers在该位置之后的条目。 Raft增加了如下两条限制以保证安全性： 拥有最新的已提交的log entry的Follower才有资格成为Leader。 这个保证是在RequestVote RPC中做的，Candidate在发送RequestVote RPC时，要带上自己的最后一条日志的term和log index，其他节点收到消息时，如果发现自己的日志比请求中携带的更新，则拒绝投票。日志比较的原则是，如果本地的最后一条log entry的term更大，则term大的更新，如果term一样大，则log index更大的更新。 Leader只能推进commit index来提交当前term的已经复制到大多数服务器上的日志，旧term日志的提交要等到提交当前term的日志来间接提交（log index 小于 commit index的日志被间接提交）。 日志压缩: 在实际的系统中，不能让日志无限增长，否则系统重启时需要花很长的时间进行回放，从而影响可用性。Raft采用对整个系统进行snapshot来解决，snapshot之前的日志都可以丢弃。 每个副本独立的对自己的系统状态进行snapshot，并且只能对已经提交的日志记录进行snapshot。 做snapshot既不要做的太频繁，否则消耗磁盘带宽， 也不要做的太不频繁，否则一旦节点重启需要回放大量日志，影响可用性。推荐当日志达到某个固定的大小做一次snapshot。 做一次snapshot可能耗时过长，会影响正常日志同步。可以通过使用copy-on-write技术避免snapshot过程影响正常日志同步. ","date":"2022-10-09","objectID":"/go-interview-summary/:8:7","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Git的merge跟rebase的区别 在使用 git 进行版本管理的项目中，当完成一个特性的开发并将其合并到 master 分支时，我们有两种方式：git merge 和 git rebase。 通常，我们对git merge 使用的较多，而对于 git rebase 使用的较少，其实 git rebase 也是极其强大的一种方法。 git merge git merge 的使用方法很简单，假如你想将分支 feature 合并到分支 master，那么只需执行如下两步即可： 将分支切换到 master 上去：git checkout master 将分支 feature 合并到当前分支（即 master 分支）上：git merge feature. git merge 有如下特点： git merge只处理一次冲突,引入了一次合并的历史记录，合并后的所有 commit 会按照提交时间从旧到新排列所有的过程信息更多，可能会提高之后查找问题的难度 因此git merge 提交的信息过多可能会影响查找问题的难度,在一个大型项目中，单纯依靠git merge方法进行合并，会保存所有的提交过程的信息：引出分支，合并分支，在分支上再引出新的分支等等，类似这样的操作一多，提交历史信息就会显得杂乱，这时如果有问题需要查找就会比较困难了。 git rebase git rebase 的目的也是将一个分支的更改并入到另外一个分支中去。但是git rebase会把你的所有分支信息衍合成一条信息,减少中间不必要的历史记录. git rebase特点: git rebase会改变当前分支从 master 上拉出分支的位置. 没有多余的合并历史的记录，且合并后的 commit 顺序不一定按照 commit 的提交时间排列. 可能会多次解决同一个地方的冲突（有 squash 来解决）. 更清爽一些，master 分支上每个 commit 点都是相对独立完整的功能单元. 因此, 当需要保留详细的合并信息的时候建议使用git merge，特别是需要将分支合并进入master分支时；当发现自己修改某个功能时，频繁进行了git commit提交时，发现其实过多的提交信息没有必要时，可以尝试git rebase。 ","date":"2022-10-09","objectID":"/go-interview-summary/:8:8","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"如何对一个20GB的文件进行排序 内存肯定没有20GB大，所以不可能采用传统排序法。但是可以将文件分成许多块，每块xMB,针对每个块各自进行排序，存回文件系统。然后将这些块逐一合并，最终得到全部排好序的文件。 外排序的一个例子是外归并排序（External merge sort），它读入一些能放在内存内的数据量，在内存中排序后输出为一个顺串（即是内部数据有序的临时文件），处理完所有的数据后再进行归并。比如，要对900MB的数据进行排序，但机器上只有100 MB的可用内存时，外归并排序按如下方法操作： 读入100 MB的数据至内存中，用某种常规方式（如快速排序、堆排序、归并排序等方法）在内存中完成排序。 将排序完成的数据写入磁盘。 重复步骤1和2直到所有的数据都存入了不同的100 MB的块（临时文件）中。在这个例子中，有900 MB数据，单个临时文件大小为100 MB，所以会产生9个临时文件。 读入每个临时文件（顺串）的前10 MB（ = 100 MB / (9块 + 1)）的数据放入内存中的输入缓冲区，最后的10 MB作为输出缓冲区。（实践中，将输入缓冲适当调小，而适当增大输出缓冲区能获得更好的效果。） 执行九路归并算法，将结果输出到输出缓冲区。一旦输出缓冲区满，将缓冲区中的数据写出至目标文件，清空缓冲区。一旦9个输入缓冲区中的一个变空，就从这个缓冲区关联的文件，读入下一个10M数据，除非这个文件已读完。这是“外归并排序”能在主存外完成排序的关键步骤,因为“归并算法”(merge algorithm)对每一个大块只是顺序地做一轮访问(进行归并)，每个大块不用完全载入主存。 ","date":"2022-10-09","objectID":"/go-interview-summary/:8:9","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"LVS原理是什么 LVS是 Linux Virtual Server 的简称，也就是Linux虚拟服务器。这是一个由章文嵩博士发起的一个开源项目，它的官方网站是LinuxVirtualServer现在 LVS 已经是 Linux 内核标准的一部分。 使用 LVS 可以达到的技术目标是：通过 LVS 达到的负载均衡技术和 Linux 操作系统实现一个高性能高可用的 Linux 服务器集群，它具有良好的可靠性、可扩展性和可操作性。 从而以低廉的成本实现最优的性能。LVS 是一个实现负载均衡集群的开源软件项目，LVS架构从逻辑上可分为调度层、Server集群层和共享存储。 LVS的基本工作原理: 当用户向负载均衡调度器（Director Server）发起请求，调度器将请求发往至内核空间. PREROUTING链首先会接收到用户请求，判断目标IP确定是本机IP，将数据包发往INPUT链. IPVS是工作在INPUT链上的，当用户请求到达INPUT时，IPVS会将用户请求和自己已定义好的集群服务进行比对，如果用户请求的就是定义的集群服务，那么此时IPVS会强行修改数据包里的目标IP地址及端口，并将新的数据包发往POSTROUTING链. POSTROUTING链接收数据包后发现目标IP地址刚好是自己的后端服务器，那么此时通过选路，将数据包最终发送给后端的服务器. LVS的由2部分程序组成，包括 Ipvs 和 Ipvsadm。 Ipvs(ip virtual server)：一段代码工作在内核空间，叫Ipvs，是真正生效实现调度的代码。 Ipvsadm：另外一段是工作在用户空间，叫Ipvsadm，负责为Ipvs内核框架编写规则，定义谁是集群服务，而谁是后端真实的服务器(Real Server) ","date":"2022-10-09","objectID":"/go-interview-summary/:8:10","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"为什么需要消息队列 解耦，异步处理，削峰/限流. ","date":"2022-10-09","objectID":"/go-interview-summary/:8:11","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"高并发系统的设计与实现 ","date":"2022-10-09","objectID":"/go-interview-summary/:8:12","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Kafka的文件存储机制 Kafka是最初由Linkedin公司开发，是一个分布式、分区的、多副本的、多订阅者，基于zookeeper协调的分布式日志系统(也可以当做MQ系统)，常见可以用于web/nginx日志、访问日志，消息服务等。 一个商业化消息队列的性能好坏，其文件存储机制设计是衡量一个消息队列服务技术水平和最关键指标之一。 Kafka中的名词解释如下： Broker：消息中间件处理结点，一个Kafka节点就是一个broker，多个broker可以组成一个Kafka集群。 Topic：一类消息，例如page view日志、click日志等都可以以topic的形式存在，Kafka集群能够同时负责多个topic的分发。 Partition：topic物理上的分组，一个topic可以分为多个partition，每个partition是一个有序的队列。 Segment：partition物理上由多个segment组成，下面2.2和2.3有详细说明。 offset：每个partition都由一系列有序的、不可变的消息组成，这些消息被连续的追加到partition中。partition中的每个消息都有一个连续的序列号叫做offset,用于partition唯一标识一条消息. Kafka运行时很少有大量读磁盘的操作，主要是定期批量写磁盘操作，因此操作磁盘很高效。这跟Kafka文件存储中读写message的设计是息息相关的。Kafka中读写message有如下特点: 写message： 消息从堆转入page cache(即物理内存)。 由异步线程刷盘,消息从page cache刷入磁盘。 读message： 消息直接从page cache转入socket发送出去。 当从page cache没有找到相应数据时，此时会产生磁盘IO,从磁 盘Load消息到page cache,然后直接从socket发出去。 Kafka高效文件存储设计特点： Kafka把topic中一个parition大文件分成多个小文件段，通过多个小文件段，就容易定期清除或删除已经消费完文件，减少磁盘占用。 通过索引信息可以快速定位message和确定response的最大大小。 通过index元数据全部映射到memory，可以避免segment file的IO磁盘操作。 通过索引文件稀疏存储，可以大幅降低index文件元数据占用空间大小。 ","date":"2022-10-09","objectID":"/go-interview-summary/:8:13","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Kafka如何保证可靠性 在 Kafka 0.8.0 版本之前，Kafka 是没有副本的概念的，那时候人们只会用 Kafka 存储一些不重要的数据，因为没有副本，数据很可能会丢失。但是随着业务的发展，支持副本的功能越来越强烈，所以为了保证数据的可靠性，Kafka 从 0.8.0 版本开始引入了分区副本。也就是说每个分区可以人为的配置几个副本（比如创建主题的时候指定 replication-factor，也可以在 Broker 级别进行配置 default.replication.factor），一般会设置为3。 Kafka 可以保证单个分区里的事件是有序的，分区可以在线（可用），也可以离线（不可用）。在众多的分区副本里面有一个副本是 Leader，其余的副本是 follower，所有的读写操作都是经过 Leader 进行的，同时 follower 会定期地去 leader 上复制数据。当 Leader 挂掉之后，其中一个 follower 会重新成为新的 Leader。通过分区副本，引入了数据冗余，同时也提供了 Kafka 的数据可靠性。 Kafka 的分区多副本架构是 Kafka 可靠性保证的核心，把消息写入多个副本可以使 Kafka 在发生崩溃时仍能保证消息的持久性。 ","date":"2022-10-09","objectID":"/go-interview-summary/:8:14","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"Kafka是如何实现高吞吐率的 kafka主要使用了以下几个方式实现了超高的吞吐率: 顺序读写 kafka的消息是不断追加到文件中的，这个特性使kafka可以充分利用磁盘的顺序读写性能,顺序读写不需要硬盘磁头的寻道时间，只需很少的扇区旋转时间，所以速度远快于随机读写. Kafka官方给出了测试数据(Raid-5，7200rpm)： 顺序I/O: 600MB/s. 随机I/O: 100KB/s. 零拷贝 我们可以先简单了解下文件系统的操作流程，例如一个程序要把文件内容发送到网络,这个程序是工作在用户空间，文件和网络socket属于硬件资源，两者之间有一个内核空间. 在操作系统内部，整个过程为： 在Linux kernel2.2 之后出现了一种叫做\"零拷贝(zero-copy)“系统调用机制，就是跳过“用户缓冲区”的拷贝，建立一个磁盘空间和内存的直接映射，数据不再复制到“用户态缓冲区”，系统上下文切换减少为2次，可以提升一倍的性能。 文件分段 kafka的队列topic被分为了多个区partition，每个partition又分为多个段segment，所以一个队列中的消息实际上是保存在N多个片段文件中。 通过分段的方式，每次文件操作都是对一个小文件的操作，非常轻便，同时也增加了并行处理能力。 批量发送 Kafka允许进行批量发送消息，先将消息缓存在内存中，然后一次请求批量发送出去，比如可以指定缓存的消息达到某个量的时候就发出去，或者缓存了固定的时间后就发送出去，如100条消息就发送，或者每5秒发送一次，这种策略将大大减少服务端的I/O次数。 数据压缩 Kafka还支持对消息集合进行压缩，Producer可以通过GZIP或Snappy格式对消息集合进行压缩，压缩的好处就是减少传输的数据量，减轻对网络传输的压力。Producer压缩之后，在Consumer需进行解压，虽然增加了CPU的工作，但在对大数据处理上，瓶颈在网络上而不是CPU，所以这个成本很值得。 分布式事务有哪几种 分布式事务就是指事务的发起者、资源及资源管理器和事务协调者分别位于分布式系统的不同节点之上。分布式事务是在分布式环境下，遵循CAP理论，在CAP三者中只能够三选二，那么分布式事务是那种的组合呢？ 分布式事务是CP+HA，其中A是没有完全符合，但是能够达到Highly-Available，即高可用。 近些年分布式理论进一步发展，产生了Paxos、Raft等CP的协议，在这基础上，加上硬件稳定性升级，可以在保证CP的情况下，做到高可用。谷歌分布式锁Chubby的公开数据显示，集群能提供99.99958％的平均可用性，一年也就130s的运行中断，已经能够满足非常严苛的应用要求。现在的SQL类数据库软件，都是走CP+HA，只是HA会比谷歌的这个极致数据更低一些，但一般都能够达到4个9 CP+HA意味着不是BASE，意味着你只要写入成功，那么接下来的读，能够读取到最新的结果，开发人员不用担心读取到的不是最新数据，在多副本读写上面，与单机是一致的。 分布式事务会部分遵循 ACID 规范： 原子性：严格遵循 一致性：事务完成后的一致性严格遵循；事务中的一致性可适当放宽 隔离性：并行事务间不可影响；事务中间结果可见性允许安全放宽 持久性：严格遵循 因为事务过程中，不是一致的，但事务会最终完成，最终达到一致，所以我们把分布式事务称为“最终一致”。 分布式事务目前主要有：可靠事件队列(两阶段提交)，SAGA，TCC. CAP理论中： 一致性（Consistency）：代表数据在任何时刻、任何分布式节点中所看到的都是符合预期的。一致性在分布式研究中是有严肃定义、有多种细分类型的概念，以后讨论分布式共识算法时，我们还会再提到一致性，那种面向副本复制的一致性与这里面向数据库状态的一致性严格来说并不完全等同，具体差别我们将在后续分布式共识算法中再作探讨。 可用性（Availability）：代表系统不间断地提供服务的能力，理解可用性要先理解与其密切相关两个指标：可靠性（Reliability）和可维护性（Serviceability）。可靠性使用平均无故障时间（Mean Time Between Failure，MTBF）来度量；可维护性使用平均可修复时间（Mean Time To Repair，MTTR）来度量。可用性衡量系统可以正常使用的时间与总时间之比，其表征为：A=MTBF/（MTBF+MTTR），即可用性是由可靠性和可维护性计算得出的比例值，譬如 99.9999%可用，即代表平均年故障修复时间为 32 秒。 分区容忍性（Partition Tolerance）: 代表分布式环境中部分节点因网络原因而彼此失联后，即与其他节点形成“网络分区”时，系统仍能正确地提供服务的能力。 由于 CAP 定理已有严格的证明，我们也不去探讨为何 CAP 不可兼得，而是直接分析如果舍弃 C、A、P 时所带来的不同影响。 如果放弃分区容忍性（CA without P）, 意味着我们将假设节点之间通信永远是可靠的。永远可靠的通信在分布式系统中必定不成立的，这不是你想不想的问题，而是只要用到网络来共享数据，分区现象就会始终存在。在现实中，最容易找到放弃分区容忍性的例子便是传统的关系数据库集群，这样的集群虽然依然采用由网络连接的多个节点来协同工作，但数据却不是通过网络来实现共享的。以 Oracle 的 RAC 集群为例，它的每一个节点均有自己独立的 SGA、重做日志、回滚日志等部件，但各个节点是通过共享存储中的同一份数据文件和控制文件来获取数据的，通过共享磁盘的方式来避免出现网络分区。因而 Oracle RAC 虽然也是由多个实例组成的数据库，但它并不能称作是分布式数据库。 如果放弃可用性（CP without A），意味着我们将假设一旦网络发生分区，节点之间的信息同步时间可以无限制地延长，此时，问题相当于退化到前面“全局事务”中讨论的一个系统使用多个数据源的场景之中，我们可以通过 2PC/3PC 等手段，同时获得分区容忍性和一致性。在现实中，选择放弃可用性的 CP 系统情况一般用于对数据质量要求很高的场合中，除了 DTP 模型的分布式数据库事务外，著名的 HBase 也是属于 CP 系统，以 HBase 集群为例，假如某个 RegionServer 宕机了，这个 RegionServer 持有的所有键值范围都将离线，直到数据恢复过程完成为止，这个过程要消耗的时间是无法预先估计的。 如果放弃一致性（AP without C），意味着我们将假设一旦发生分区，节点之间所提供的数据可能不一致。选择放弃一致性的 AP 系统目前是设计分布式系统的主流选择，因为 P 是分布式网络的天然属性，你再不想要也无法丢弃；而 A 通常是建设分布式的目的，如果可用性随着节点数量增加反而降低的话，很多分布式系统可能就失去了存在的价值，除非银行、证券这些涉及金钱交易的服务，宁可中断也不能出错，否则多数系统是不能容忍节点越多可用性反而越低的。目前大多数 NoSQL 库和支持分布式的缓存框架都是 AP 系统，以 Redis 集群为例，如果某个 Redis 节点出现网络分区，那仍不妨碍各个节点以自己本地存储的数据对外提供缓存服务，但这时有可能出现请求分配到不同节点时返回给客户端的是不一致的数据。 “选择放弃一致性的 AP 系统目前是设计分布式系统的主流选择”这个结论感到一丝无奈，“事务”原本的目的就是获得“一致性”，而在分布式环境中，“一致性”却不得不成为通常被牺牲、被放弃的那一项属性。但无论如何，我们建设信息系统，终究还是要确保操作结果至少在最终交付的时候是正确的。为此，人们又重新给一致性下了定义，将前面我们在 CAP、ACID 中讨论的一致性称为“强一致性”（Strong Consistency），有时也称为“线性一致性”（Linearizability，通常是在讨论共识算法的场景中），而把牺牲了 C 的 AP 系统又要尽可能获得正确的结果的行为称为追求“弱一致性”。 不过，如果单纯只说“弱一致性”那其实就是“不保证一致性”的意思……人类语言这东西真的是博大精深。在弱一致性里，人们又总结出了一种稍微强一点的特例，被称为“最终一致性”（Eventual Consistency），它是指：如果数据在一段时间之内没有被另外的操作所更改，那它最终将会达到与强一致性过程相同的结果，有时候面向最终一致性的算法也被称为“乐观复制算法”. 分布式事务目前主要有: 可靠事件队列： 最终一致性的概念是 eBay 的系统架构师 Dan Pritchett 在 2008 年在 ACM 发表的论文《Base: An Acid Alternative》中提出的，该论文总结了一种独立于 ACID 获得的强一致性之外的、使用 BASE 来达成一致性目的的途径。BASE 分别是基本可用性（Basically Available）、柔性事务（Soft State）和最终一致性（Eventually Consistent）的缩写。 BASE 这提法简直是把数据库科学家酷爱凑缩写的恶趣味发挥到淋漓尽致，不过有 ACID vs BASE这个朗朗上口的梗，该论文的影响力的确传播得足够快。虽然调侃它是恶趣味，但这篇论文本身作为最终一致性的概念起源，并系统性地总结了一种针对分布式事务的技术手段，是非常有价值的。 靠着持续重试来保证可靠性的解决方案谈不上是 Dan Pritchett 的首创或者独创，它在计算机的其他领域中已被频繁使用，也有了专门的名字叫作“最大努力交付”（Best-Effort Delive","date":"2022-10-09","objectID":"/go-interview-summary/:8:15","tags":["Go","Interview"],"title":"Go 面试问题汇总","uri":"/go-interview-summary/"},{"categories":["Computer Science"],"content":"排序算法可以分为内部排序和外部排序。 内部排序是数据记录在内存中进行排序。 而外部排序是因排序的数据很大，一次不能容纳全部的排序记录，在排序过程中需要访问外存。 常见的内部排序算法有：插入排序、希尔排序、选择排序、冒泡排序、归并排序、快速排序、堆排序、基数排序等。 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:0:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"稳定性 稳定的排序算法：冒泡排序、插入排序、归并排序和基数排序。 不是稳定的排序算法：选择排序、快速排序、希尔排序、堆排序。 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:1:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"冒泡排序 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:2:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"算法思想 从第一个和第二个开始比较，如果第一个比第二个大，则交换位置，然后比较第二个和第三个，逐渐往后，经过第一轮后最大的元素已经排在最后， 所以重复上述操作的话第二大的则会排在倒数第二的位置。那重复上述操作 n-1 次即可完成排序，因为最后一次只有一个元素所以不需要比较。 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:2:1","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"代码实现 // BubbleSort 冒泡排序 func BubbleSort(arr []int32) []int32 { // 第一层 for 表示循环的遍数 for i := 0; i \u003c len(arr)-1; i++ { // 第二层 for 表示具体比较哪两个元素 for j := i + 1; j \u003c len(arr); j++ { if arr[i] \u003e arr[j] { // 如果前面的大于后面的，则交换这两个元素的位置 arr[i], arr[j] = arr[j], arr[i] } } } return arr } ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:2:2","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"选择排序 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:3:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"算法思想 设第一个元素为比较元素，依次和后面的元素比较，比较完所有元素找到最小的元素，将它和第一个元素互换， 重复上述操作，我们找出第二小的元素和第二个位置的元素互换，以此类推找出剩余最小元素将它换到前面，即完成排序。 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:3:1","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"代码实现 // SelectionSort 选择排序 func SelectionSort(arr []int32) []int32 { // 第一层 for 表示循环选择的遍数 for i := 0; i \u003c len(arr)-1; i++ { // 起始元素设为最小的元素 minIndex := i // 第二层 for 表示最小元素与后面的元素逐个比较 for j := i + 1; j \u003c len(arr); j++ { if arr[j] \u003c arr[minIndex] { // 如果当前元素比最小元素小，则把当前元素角标记为最小元素角标 minIndex = j } } // 查找一遍后将最小元素与起始元素互换 arr[minIndex], arr[i] = arr[i], arr[minIndex] } return arr } ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:3:2","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"插入排序 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:4:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"算法思想 从第二个元素开始和前面的元素进行比较，如果前面的元素比当前元素大，则将前面元素 后移，当前元素依次往前，直到找到比它小或等于它的元素插入在其后面， 然后选择第三个元素，重复上述操作，进行插入，依次选择到最后一个元素，插入后即完成所有排序。 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:4:1","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"代码实现 // InsertionSort 插入排序 func InsertionSort(arr []int32) []int32 { // 第一层 for 表示循环插入的遍数 for i := 1; i \u003c len(arr); i++ { // 设置当前需要插入的元素 current := arr[i] // 与当前元素比较的比较元素 preIndex := i - 1 for preIndex \u003e= 0 \u0026\u0026 arr[preIndex] \u003e current { // 当比较元素大于当前元素则把比较元素后移 arr[preIndex+1] = arr[preIndex] // 往前选择下一个比较元素 preIndex -= 1 } // 当比较元素小于当前元素，则将当前元素插入在其后面 arr[preIndex+1] = current } return arr } ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:4:2","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"希尔排序 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:5:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"算法思想 希尔排序的整体思想是将固定间隔的几个元素之间排序，然后再缩小这个间隔。这样到最后数列就成为了基本有序数列。 具体步骤： 计算一个增量（间隔）值 对元素进行增量元素进行比较，比如增量值为 7，那么就对 0,7,14,21… 个元素进行插入排序 然后对 1,8,15… 进行排序，依次递增进行排序 所有元素排序完后，缩小增量比如为 3，然后又重复上述第 2，3 步 最后缩小增量至 1 时，数列已经基本有序，最后一遍普通插入即可 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:5:1","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"代码实现 // ShellSort 希尔排序 func ShellSort(arr []int32) []int32 { // 取整计算增量（间隔）值 gap := len(arr) / 2 for gap \u003e 0 { // 从增量值开始遍历比较 for i := gap; i \u003c len(arr); i++ { current := arr[i] // 元素与他同列的前面的每个元素比较，如果比前面的小则互换 preIndex := i for preIndex-gap \u003e= 0 \u0026\u0026 current \u003c arr[preIndex-gap] { arr[preIndex] = arr[preIndex-gap] preIndex -= gap } arr[preIndex] = current } // 缩小增量（间隔）值 gap /= 2 } return arr } ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:5:2","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"归并排序 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:6:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"算法思想 归并排序是分治法的典型应用。分治法（Divide-and-Conquer）：将原问题划分成 n 个规模较小而结构与原问题相似的子问题；递归地解决这些问题，然后再合并其结果，就得到原问题的解，分解后的数列很像一个二叉树。 具体实现步骤： 使用递归将源数列使用二分法分成多个子列 申请空间将两个子列排序合并然后返回 将所有子列一步一步合并最后完成排序 注：先分解再归并 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:6:1","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"代码实现 // MergeSort 归并排序 func MergeSort(arr []int32) []int32 { if len(arr) == 1 { return arr } // 使用二分法将数列分两个 mid := len(arr) / 2 left := arr[:mid] right := arr[mid:] // 使用递归运算 return merge(MergeSort(left), MergeSort(right)) } // 排序合并 func merge(left []int32, right []int32) []int32 { // 排序合并两个数列 result := make([]int32, 0) // 两个数列都有值 for len(left) \u003e 0 \u0026\u0026 len(right) \u003e 0 { // 左右两个数列第一个最小放前面 if left[0] \u003c= right[0] { head := left[0] result = append(result, head) left = left[1:] } else { head := right[0] result = append(result, head) right = right[1:] } } // 只有一个数列中还有值，直接添加 result = append(result, left...) result = append(result, right...) return result } ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:6:2","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"快速排序 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:7:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"算法思想 找出基线条件，这种条件必须尽可能简单，不断将问题分解（或者说缩小规模），直到符合基线条件。 具体实现步骤： 先从数列中取出一个数作为基准数。 分区过程，将比这个数大的数全放到它的右边，小于或等于它的数全放到它的左边。 再对左右区间重复第二步，直到各区间只有一个数。 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:7:1","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"代码实现 // QuickSort 快速排序 func QuickSort(arr []int32) []int32 { if len(arr) \u003c 2 { // 基线条件：为空或只包含一个元素的数组是 “有序” 的 return arr } // 递归条件 pivot := arr[0] var less, greater []int32 for i, val := range arr { if i == 0 { continue } if val \u003c= pivot { // 由所有小于基准值的元素组成的子数组 less = append(less, val) } else { // 由所有大于基准值的元素组成的子数组 greater = append(greater, val) } } return append(append(QuickSort(less), pivot), QuickSort(greater)...) } ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:7:2","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"堆排序 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:8:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"算法思想 堆分为最大堆和最小堆，是完全二叉树。堆排序就是把堆顶的最大数取出，将剩余的堆继续调整为最大堆，具体过程在第二块有介绍，以递归实现， 剩余部分调整为最大堆后，再次将堆顶的最大数取出，再将剩余部分调整为最大堆，这个过程持续到剩余数只有一个时结束。 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:8:1","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"代码实现 // HeapSort 堆排序 func HeapSort(arr []int32) []int32 { for i := 0; i \u003c len(arr); i++ { // 第一下 使得第一个元素最小，接下来就从第二个来构造，使得下一个最小， heapify(arr[i:]) } return arr } // 建堆 // 每次操作完毕 堆顶的元素就是最小的，由于堆的特性，我们只需要从倒数第二层开始就可以了 func heapify(arr []int32) { length := len(arr) floor := length/2 - 1 for i := floor; i \u003e= 0; i-- { // 然后比较的每一个节点与其两个孩子节点的大小，使得根节点永远是最小的 // 有一种特殊情况，就是最后一个节点的孩子节点可能不存在，和可能只有一个，所以需要加上一个判断 root := i left := 2*i + 1 // 左节点 right := 2*i + 2 // 右节点 if right \u003c length \u0026\u0026 arr[right] \u003c arr[root] { root = right } if left \u003c length \u0026\u0026 arr[left] \u003c arr[root] { root = left } // 把父节点换下去并向下调整 if root != i { arr[root], arr[i] = arr[i], arr[root] } } } ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:8:2","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"计数排序 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:9:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"算法思想 对每一个输入元素 x，确定小于 x 的元素个数。利用这一信息，就可以直接把 x 放在它在输出数组上的位置上了，运行时间为 O (n)，但其需要的空间不一定，空间浪费大。 具体实现步骤： 找出待排序的数组中最大和最小的元素 统计数组中每个值为i的元素出现的次数，存入数组C的第i项 对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加） 反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:9:1","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"代码实现 // CountingSort 计数排序 func CountingSort(arr []int32) []int32 { max := getMax(arr) // 获取最大值 counter := make([]int32, max+1) // 用于统计个数的空数组 sortedIndex := 0 // 桶内索引值 for i := 0; i \u003c len(arr); i++ { counter[arr[i]] += 1 // 统计每个元素出现的次数 } for j := 0; j \u003c len(counter); j++ { for counter[j] \u003e 0 { arr[sortedIndex] = int32(j) // 取出元素 sortedIndex += 1 counter[j] -= 1 } } return arr } // 获取最大值 func getMax(arr []int32) int32 { max := arr[0] for _, val := range arr { if val \u003e max { max = val } } return max } ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:9:2","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"桶排序 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:10:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"算法思想 为了节省空间和时间，我们需要指定要排序的数据中最小以及最大的数字的值，来方便桶排序算法的运算。 桶排序的算法原理： 设置一个定量的数组当作空桶子. 寻访序列，并且把项目一个一个放到对应的桶子去. 对每个不是空的桶子进行排序. 从不是空的桶子里把项目再放回原来的序列中. ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:10:1","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"代码实现 // BucketSort 桶排序 func BucketSort(arr []int) []int { // 原数列的最大值与最小值 max, min := arr[0], arr[0] for _, val := range arr { if val \u003e max { max = val } if val \u003c min { min = val } } // 这里设置每个桶的容量是5 bucketCapacity := 5 // 计算所需桶的个数 bucketCount := (max - min + bucketCapacity) / bucketCapacity buckets := make([][]int, 0, bucketCount) for i := 0; i \u003c bucketCount; i += 1 { bucket := make([]int, 0) // 初始化桶 buckets = append(buckets, bucket) } for _, n := range arr { // 映射函数 k := (n - min) / bucketCapacity // 分配入桶 buckets[k] = append(buckets[k], n) } p := 0 for _, bucket := range buckets { if len(bucket) == 0 { continue } // 给每个桶中的元素排序 sort.Ints(bucket) // 将桶中的元素赋值到原序列 for _, n := range bucket { arr[p] = n p += 1 } } return arr } ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:10:2","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"基数排序 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:11:0","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"算法思想 基数排序（radix sort）属于 “分配式排序”（distribution sort），又称 “桶子法”（bucket sort）或 bin sort， 顾名思义，它是透过键值的部份资讯，将要排序的元素分配至某些 “桶” 中，藉以达到排序的作用，基数排序法是属于稳定性的排序， 其时间复杂度为 O (nlog (r) m)，其中 r 为所采取的基数，而 m 为堆数，在某些时候，基数排序法的效率高于其它的稳定性排序法。 ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:11:1","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["Computer Science"],"content":"代码实现 // RadixSort 基数排序 func RadixSort(array []int) []int { digit := 1 max := math.MinInt32 for _, val := range array { if val \u003c 0 { // 输入可能为负数，取绝对值 val = -val } // 取最大值来判断目前是否已经看到最高位数 if max \u003c val { max = val } } for digit \u003c= max { radixCountingSort(array, digit) // 递增位数 digit *= 10 } return array } // 计数排序法（递增） func radixCountingSort(array []int, digit int) { // 一个位数可能的值为-9 ~ +9，共19种可能 var counter [19]int for _, n := range array { // 先计算出目前要看的位数的值之后，再进行-(-9)的索引值位移 k := ((n / digit) % 10) + 9 counter[k] += 1 } for i := 1; i \u003c 19; i += 1 { counter[i] += counter[i-1] } length := len(array) origin := make([]int, length) copy(origin, array) for i := length - 1; i \u003e= 0; i -= 1 { n := origin[i] k := ((n / digit) % 10) + 9 counter[k]-- array[counter[k]] = n } } ","date":"2022-10-08","objectID":"/classic-sorting-algorithms/:11:2","tags":["Sorting","Algorithms"],"title":"经典排序算法","uri":"/classic-sorting-algorithms/"},{"categories":["DevOps"],"content":"Git代码状态转换图 其中： 未被Git跟踪的状态为unstage状态； 已被Git跟踪的状态为stage状态（stage：阶段），因此包括staging状态和staged状态。 untrack files：是指尚未被git所管理的文件； changed but not updated：是指文件被git管理，并且发生了改变，但改动还没被git管理； 这两种状态，都可以看成是改动还没被git管理的状态，我们这里称unstage状态。 staging是commit和未管理之间的一个状态，也有别名叫index状态，也就是git已经管理了这些改动，但是还没完成提交。 changes to be committed是指进入staged状态的文件。 .gitignore中的文件，不会出现在以上三个状态中。 注： 这个图也解释了为啥从远端库拉代码，不需要add、commit。 代码一旦修改，就会成为未被git库跟踪的状态。需要add、commit。 ","date":"2022-09-29","objectID":"/git-and-operations/:1:0","tags":["Git"],"title":"Git 基本知识与常用指令","uri":"/git-and-operations/"},{"categories":["DevOps"],"content":"大白话Git Git 管理代码，保证代码版本迭代连续性，即：向A分支merge或者push代码时，A分支代码必须是当前代码的上一个版本，不然会产生冲突。 （换句话说：Git 确保当前的本地的代码为最新） Git有修改就有提交，就有新的代码版本，git管理维护的是修改。 Git分支存储的是代码副本。 push: 实际上就是将本地分支合并到远端库分支；pull:实际就是将远端分支合并到本地分支。 ","date":"2022-09-29","objectID":"/git-and-operations/:2:0","tags":["Git"],"title":"Git 基本知识与常用指令","uri":"/git-and-operations/"},{"categories":["DevOps"],"content":"Git 本地常用操作指令 创建git库 git init #在当前目录中生成一个.git 目录（含有.git目录的目录即是git仓库） 注册Git用户(用户信息配置) —\u003e用于在团队合作开发中，表明代码作者。 git config --global user.name XXX # 用户名 git config --global user.email XXX # 用户邮箱 git config --list # 查看用户信息 注：加 –global,表示全局设置。 向git库添加修改 git add [path］ # 会把对应目录或文件，添加到stage状态 git add . # 会把当前所有的untrack files和changed but not updated添加到stage状态 实际上是为修改内容添加index索引。 向版本库提交修改 git commit –m \"XXXX\" # 提交修改,添加注释,文件描述 注：git 提示： 未有 add 红色字体，未有commit绿色字体，已提交则worktree是干净的 查看当前代码库的状态 git status 查看版本信息 —\u003e实际是查看修改提交信息 git log git log --graph #以图形化（节点）展示当前git库的提交信息。 查看指定版本信息 git show sdjf974654dd…. #(show后面为每次提交系统自动生成的一串哈希值) git show sdji97 #一般只使用版本号的前几个字符即可 撤销修改 撤销整体修改 git reset --hard #回到原来编辑的地方,改动会丢失。（同样适用于团队对于其他人的修改恢复） git reset --hard sdv143kvf... #可回到指定的版本#(hard后面为每次提交系统自动生成的一串哈希值) git reset [path] # 会改变path指定的文件或目录的stage状态，到非stage状态。 git reset # 会将所有stage的文件状态，都改变成非stage状态。 撤销某次修改 回退1个change的写法就是: git reset HEAD^ git reset --soft HEAD~1 # 删除最近的提交，保留你已经完成的工作 git reset --hard HEAD~1 # 删除最近的提交，销毁你已经完成的工作 2个为HEAD^^， 3个为HEAD~3，以此类推。 向远端库推送修改（提交修改） git push origin 分支名 暂存修改 git stash # 可以把当前的改动（stage和unstage，但不包括untrack的文件）暂存。 git stash list # 查看暂存的改动 git stash apply # 重新取出暂存的改动。但apply之前要保证worktree是干净的。 ","date":"2022-09-29","objectID":"/git-and-operations/:3:0","tags":["Git"],"title":"Git 基本知识与常用指令","uri":"/git-and-operations/"},{"categories":["DevOps"],"content":"Git 团队开发常用操作指令 获取远端库项目 git clone/pull xxx.git 团队开发的基本流程（多分支合并一个分支） git add . #添加改动的文件 git commit #（提交至本地） git pull --rebase #（将服务器项目与本地项目合并） git push #（将本地项目上传至远端库） 在提交前要git pull –rebase 一下，确保当前的本地的代码为最新。 更新 fork 的仓库 git remote add upstream xxx.git # 添加远程，称其为 \"上游\" git fetch upstream # 把该远程的所有分支取到远程跟踪分支中去 git checkout master # 确保你在你的主分支上 git rebase upstream/master # 重写你的主分支，这样任何不在上游/主分支中的提交都会在其他分支的顶部重放。 git pull upstream xx # 把本地的xx分支与被fork仓库xx分支同步 git push upstream xx:xxx # 推送本地xx分支到被fork仓库的xxx分支上 tag 的创建与删除 创建 tag # To create a tag on your current branch, run this: git tag \u003ctagname\u003e # 在当前分支上创建一个标签 # If you want to include a description with your tag, add -a to create an annotated tag: git tag \u003ctagname\u003e -a # 如果你想在标签中加入描述，可以添加 -a 来创建一个带注释的标签。 # This will create a local tag with the current state of the branch you are on. When pushing to your remote repo, tags are NOT included by default. You will need to explicitly say that you want to push your tags to your remote repo: git push origin --tags # 这将创建一个本地标签，包含你所在的分支的当前状态。当推送到远程版本时，默认不包括标签。你需要明确地说，你想把你的标签推送到你的远程 repo # Or if you just want to push a single tag: git push origin \u003ctag\u003e # 或者如果你只想推送一个标签 eg: git tag v1.0.0 git push origin v1.0.0 删除 tag # use the --delete option (or -d if your git version is older than 1.8.0): git push --delete origin tagname # delete the local tag, use: git tag --delete tagname # 删除本地标签 eg: git git push --delete origin v1.0.0 ","date":"2022-09-29","objectID":"/git-and-operations/:4:0","tags":["Git"],"title":"Git 基本知识与常用指令","uri":"/git-and-operations/"},{"categories":["DevOps"],"content":"Git 分支管理 建立分支 git branch AAA #建立分支AAA 分支切换 git checkout AAA #从当前分支切换到AAA分支 (若AAA分支不存在，则自动新建) 将分支与主枝master合并 git checkout master #（首先切换回主枝） git merge AAA #（将分支AAA与主枝合并） 注：git merge：默认情况下，Git执行\"快进式合并\"（fast-farward merge），会直接将Master分支指向Develop分支。 使用–no-ff参数后，会执行正常合并，在Master分支上生成一个新节点。为了保证版本演进的清晰（保持提交曲线为直线），建议采用这种方法。 当前分支查看 git branch #默认有master（也称为主枝） git branch -r #查看远端库分支 git branch –a #查看当前所有分支（包括本地分支和远端库分支） 删除分支 git branch –d AAA #删除分支AAA git push origin --delete AAA #删除远程分支AAA 切下远端库A分支到本地库A分支 git checkout -b A origin/A # （若本地A分支不存在，则自动新建） 注：上面只是一些基本的操作命令，更多的命令可通过帮助文档查询。 帮助文档的使用： man git-\u003c需查询的指令\u003e #（git后面有“-”） 如commit的查询为 man git-commit ","date":"2022-09-29","objectID":"/git-and-operations/:5:0","tags":["Git"],"title":"Git 基本知识与常用指令","uri":"/git-and-operations/"},{"categories":["DevOps"],"content":"本地代码上传GitHub GitHub上建立远端仓库，复制下载链接。 本地指定目录下，GitBash 粘贴远端仓库下载链接拉取远端仓库代码。 复制本地需要提交的代码到远端仓库目录。 Git add、commit、push 提交本地代码至Github远端仓库。 ","date":"2022-09-29","objectID":"/git-and-operations/:6:0","tags":["Git"],"title":"Git 基本知识与常用指令","uri":"/git-and-operations/"},{"categories":["DevOps"],"content":"参考文档 Git 基本知识与常用指令 ","date":"2022-09-29","objectID":"/git-and-operations/:7:0","tags":["Git"],"title":"Git 基本知识与常用指令","uri":"/git-and-operations/"},{"categories":[],"content":"为什么一定要设一个主键？ 不管设置不设置主键，innodb 也会生成一个隐藏列，作为自增主键。所以啦，反正都要生成一个主键，那你还不如自己指定一个主键，在有些情况下，就能显式的用上主键索引，提高查询效率！ ","date":"2022-09-29","objectID":"/mysql-table-design/:1:0","tags":[],"title":"MySQL 表设计注意问题","uri":"/mysql-table-design/"},{"categories":[],"content":"主键是用自增还是 UUID? 自增。数据在物理结构上是顺序存储，性能最好。 ","date":"2022-09-29","objectID":"/mysql-table-design/:2:0","tags":[],"title":"MySQL 表设计注意问题","uri":"/mysql-table-design/"},{"categories":[],"content":"自增主键用完后怎么办？ 把自增主键改成 bigint 类型就好了。不过一般 int 类型用不到最大值，就分表分库了。 ","date":"2022-09-29","objectID":"/mysql-table-design/:3:0","tags":[],"title":"MySQL 表设计注意问题","uri":"/mysql-table-design/"},{"categories":[],"content":"主键为什么不推荐有业务含义？ 因为任何有业务含义的列都有改变的可能性，主键一旦带上了业务含义，那么主键就有可能发生变更。主键一旦发生变更，该数据在磁盘上的存储位置就会发生变更，有可能会引发页分裂，产生空间碎片。 带有业务含义的主键，不一定是顺序自增的。那么就会导致数据的插入顺序，并不能保证后面插入数据的主键一定比前面的数据大。如果出现了，后面插入数据的主键比前面的小，就有可能引发页分裂，产生空间碎片。 ","date":"2022-09-29","objectID":"/mysql-table-design/:4:0","tags":[],"title":"MySQL 表设计注意问题","uri":"/mysql-table-design/"},{"categories":[],"content":"表示枚举的字段为什么不用 enum 类型？ ENUM 类型的 ORDER BY 操作效率低，需要额外操作 如果枚举值是数值，有陷阱 ","date":"2022-09-29","objectID":"/mysql-table-design/:5:0","tags":[],"title":"MySQL 表设计注意问题","uri":"/mysql-table-design/"},{"categories":[],"content":"货币字段用什么类型？ 如果货币单位是分，可以用 Int 类型。如果坚持用元，用 Decimal [ˈdes (ə) məl]。 千万不要答 float 和 double，因为 float 和 double 是以二进制存储的，所以有一定的误差。 ","date":"2022-09-29","objectID":"/mysql-table-design/:6:0","tags":[],"title":"MySQL 表设计注意问题","uri":"/mysql-table-design/"},{"categories":[],"content":"时间字段用什么类型？ varchar，如果用 varchar 类型来存时间，优点在于显示直观。但是坑的地方也是挺多的。比如，插入的数据没有校验，你可能某天就发现一条数据为 2013111 的数据，请问这是代表 2013 年 1 月 11 日，还是 2013 年 11 月 1 日？ 其次，做时间比较运算，你需要用 STR_TO_DATE 等函数将其转化为时间类型，你会发现这么写是无法命中索引的。数据量一大，是个坑！ timestamp，该类型是四个字节的整数，它能表示的时间范围为 1970-01-01 08:00:01 到 2038-01-19 11:14:07。2038 年以后的时间，是无法用 timestamp 类型存储的。 但是它有一个优势，timestamp 类型是带有时区信息的。一旦你系统中的时区发生改变，例如你修改了时区 SET TIME_ZONE = “america/new_york”; 你会发现，项目中的该字段的值自己会发生变更。这个特性用来做一些国际化大项目，跨时区的应用时，特别注意！ datetime，datetime 储存占用 8 个字节，它存储的时间范围为 1000-01-01 00:00:00 ~ 9999-12-31 23:59:59。显然，存储时间范围更大。但是它坑的地方在于，他存储的是时间绝对值，不带有时区信息。如果你改变数据库的时区，该项的值不会自己发生变更！ bigint，也是 8 个字节，自己维护一个时间戳，表示范围比 timestamp 大多了，就是要自己维护，不大方便。 ","date":"2022-09-29","objectID":"/mysql-table-design/:7:0","tags":[],"title":"MySQL 表设计注意问题","uri":"/mysql-table-design/"},{"categories":[],"content":"为什么不直接存储图片、音频、视频等大容量内容？ 我们在实际应用中，都是用 HDFS 来存储文件。然后 mysql 中，只存文件的存放路径。mysql 中有两个字段类型被用来设计存放大容量文件，也就是 text 和 blob 类型。但是，我们在生产中，基本不用这两个类型！ Mysql 内存临时表不支持 TEXT、BLOB 这样的大数据类型，如果查询中包含这样的数据，在排序等操作时，就不能使用内存临时表，必须使用磁盘临时表进行。导致查询效率缓慢 binlog 内容太多。因为你数据内容比较大，就会造成 binlog 内容比较多。大家也知道，主从同步是靠 binlog 进行同步，binlog 太大了，就会导致主从同步效率问题！ ","date":"2022-09-29","objectID":"/mysql-table-design/:8:0","tags":[],"title":"MySQL 表设计注意问题","uri":"/mysql-table-design/"},{"categories":[],"content":"字段为什么要定义为 NOT NULL? 索引性能不好 查询会出现一些不可预料的结果 create table table_2 ( `id` INT (11) NOT NULL, name varchar(20) NOT NULL ) 表 table_2 数据是这样的: id name 1 王二 3 5 麻子 7 你执行语句 select count(name) from table_2; 会发现结果为 2，但是实际上是有四条数据的！ ","date":"2022-09-29","objectID":"/mysql-table-design/:9:0","tags":[],"title":"MySQL 表设计注意问题","uri":"/mysql-table-design/"},{"categories":["DevOps"],"content":"MySQL, Oracle，SQL Server 的区别 Oracle 没有自动增长类型，MySQL 和 SQL Server 一般使用自动增长类型 做分页的话，MySQL 使用 Limit，SQL Server 使用 top，Oracle 使用 row Oracle 支持多用户不同权限来进行操作，而 MySQL 只要有登录权限就可操作全部数据库 ","date":"2022-09-29","objectID":"/mysql/:1:0","tags":["MySQL"],"title":"MySQL","uri":"/mysql/"},{"categories":["DevOps"],"content":"数据库三大范式 第一范式：每个列都不可以再拆分。 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。 ","date":"2022-09-29","objectID":"/mysql/:2:0","tags":["MySQL"],"title":"MySQL","uri":"/mysql/"},{"categories":["DevOps"],"content":"Mysql 事务 事务 (transaction) 是作为一个单元的一组有序的数据库操作 (sql 语句分组)。 ","date":"2022-09-29","objectID":"/mysql/:3:0","tags":["MySQL"],"title":"MySQL","uri":"/mysql/"},{"categories":["DevOps"],"content":"事务的四大特性 (ACID) 原子性 (Atomicity)：事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性 (Consistency)：执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性 (isolation)：并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性 (Durability)：一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 ","date":"2022-09-29","objectID":"/mysql/:4:0","tags":["MySQL"],"title":"MySQL","uri":"/mysql/"},{"categories":["DevOps"],"content":"什么是脏读？幻读？不可重复读？ 脏读：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个 RollBack 了操作，则后一个事务所读取的数据就会是不正确的。 不可重复读：不可重复读 (Non-repeatable read): 在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。 幻读：在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列 (Row) 数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。 ","date":"2022-09-29","objectID":"/mysql/:5:0","tags":["MySQL"],"title":"MySQL","uri":"/mysql/"},{"categories":["DevOps"],"content":"事务的隔离级别 SQL 标准定义了 4 类隔离级别，包括了一些具体规则，用来限定事务内外的哪些改变是可见的，哪些是不可见的。 读取未提交内容 (Read Uncommitted)：最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 读取提交内容 (Read Committed)：允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 可重复读 (Repeatable Read)：对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 串行读 (Serializable)：最高的隔离级别。通过强制事务排序，使之不能互相冲突，从而解决幻读问题。简言之，在每个读的数据行上加上共享锁。在这个级别时，可能导致大量的超时现象和锁竞争。 ","date":"2022-09-29","objectID":"/mysql/:6:0","tags":["MySQL"],"title":"MySQL","uri":"/mysql/"},{"categories":["DevOps"],"content":"SQL 语句 ","date":"2022-09-29","objectID":"/mysql/:7:0","tags":["MySQL"],"title":"MySQL","uri":"/mysql/"},{"categories":["DevOps"],"content":"六种关联查询 交叉连接 内连接 外连接 联合查询 全连接 交叉连接 ","date":"2022-09-29","objectID":"/mysql/:7:1","tags":["MySQL"],"title":"MySQL","uri":"/mysql/"},{"categories":["DevOps"],"content":"SQL 优化 选取最适合的字段属性 例如 在定义邮政编码这个字段的时候，如果设置为 char (255)，就会给数据库增加不必要的空间，甚至使用 varchar 这种类型也是多余的，char (6) 就够了 尽量把字段设置为 not null 这样执行查询时，数据库不会去比较 Null 值 优化查询语句 使用索引 查询时使用 join 代替子查询 尽量使用一条或者少数几条语句完成 使用外键 ","date":"2022-09-29","objectID":"/mysql/:8:0","tags":["MySQL"],"title":"MySQL","uri":"/mysql/"},{"categories":["DevOps"],"content":"基本查询 SELECT * FROM students; ","date":"2022-09-29","objectID":"/mysql-operations/:1:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"条件查询 SELECT * FROM students where score \u003e= 80 AND gender='M'; score 和 gender 是查询条件 SELECT * FROM students WHERE NOT class_id = 2; 按 NOT 条件查询 students，查找条件不为 id=2 的数据 SELECT * FROM students WHERE score \u003c 80 OR score \u003e 90 AND gender = 'M'; 如果不加括号，条件运算按照 NOT、AND、OR 的优先级进行，即 NOT 优先级最高，其次是 AND，最后是 OR。加上括号可以改变优先级。 ","date":"2022-09-29","objectID":"/mysql-operations/:2:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"投影查询 SELECT id, score points, name FROM students; SELECT 语句将列名 score 重命名为 points，而 id 和 name 列名保持不变 ","date":"2022-09-29","objectID":"/mysql-operations/:3:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"排序 SELECT id, name, gender, score FROM students ORDER BY score; 默认按 score 从低到高 SELECT id, name, gender, score FROM students ORDER BY score DESC; 按照成绩从高到底排序，我们可以加上 DESC 表示 “倒序” SELECT id, name, gender, score FROM students ORDER BY score DESC, gender; 如果 score 列有相同的数据，要进一步排序，可以继续添加列名。例如，使用 ORDER BY score DESC, gender 表示先按 score 列倒序，如果有相同分数的，再按 gender 列排序 ","date":"2022-09-29","objectID":"/mysql-operations/:4:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"分页查询 SELECT id, name, gender, score FROM students ORDER BY score DESC LIMIT 3 OFFSET 0; 我们把结果集分页，每页 3 条记录。要获取第 1 页的记录，可以使用 LIMIT 3 OFFSET 0 LIMIT3 表示每页最多 3 条记录，0 是结果集从 0 号记录开始。如果想查看第二页的记录，将 offset 设为 3. ","date":"2022-09-29","objectID":"/mysql-operations/:5:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"聚合查询 对于统计总数、平均数这类计算，SQL 提供了专门的聚合函数，使用聚合函数进行查询，就是聚合查询，它可以快速获得结果 SELECT COUNT(*) num FROM students; COUNT () 表示查询所有列的行数，要注意聚合的计算结果虽然是一个数字，但查询的结果仍然是一个二维表，只是这个二维表只有一行一列，并且列名是 COUNT ()。 通常，使用聚合查询时，我们应该给列名设置一个别名（num），便于处理结果 除了 COUNT () 函数外，SQL 还提供了如下聚合函数 函数 说明 SUM 计算某一列的合计值，该列必须为数值类型 AVG 计算某一列的平均值，该列必须为数值类型 MAX 计算某一列的最大值 MIN 计算某一列的最小值 注意，MAX () 和 MIN () 函数并不限于数值类型。如果是字符类型，MAX () 和 MIN () 会返回排序最后和排序最前的字符 SELECT AVG(score) average FROM students WHERE gender = 'M'; 使用聚合查询计算男生平均成绩 ","date":"2022-09-29","objectID":"/mysql-operations/:6:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"分组查询 SELECT COUNT(*) num FROM students GROUP BY class_id; 按 class_id 分组 SELECT class_id, COUNT(*) num FROM students GROUP BY class_id; 但是这 3 行结果分别是哪三个班级的，不好看出来，所以我们可以把 class_id 列也放入结果集中 ","date":"2022-09-29","objectID":"/mysql-operations/:7:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"多表查询 SELECT * FROM students, classes; 利用投影查询的 “设置列的别名” 来给两个表各自的 id 和 name 列起别名 SELECT students.id sid, students.name, students.gender, students.score, classes.id cid, classes.name cname FROM students, classes; 给表名取别名 SELECT s.id sid, s.name, s.gender, s.score, c.id cid, c.name cname FROM students s, classes c; ","date":"2022-09-29","objectID":"/mysql-operations/:8:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"连接查询 连接查询是另一种类型的多表查询。连接查询对多个表进行 JOIN 运算，简单地说，就是先确定一个主表作为结果集，然后，把其他表的行有选择性地 “连接” 在主表结果集上。 ","date":"2022-09-29","objectID":"/mysql-operations/:9:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"内连接 INNER JOIN 先确定主表，仍然使用 FROM \u003c表 1\u003e 的语法； 再确定需要连接的表，使用 INNER JOIN \u003c表 2\u003e 的语法； 然后确定连接条件，使用 ON \u003c条件…\u003e，这里的条件是 s.class_id = c.id，表示 students 表的 class_id 列与 classes 表的 id 列相同的行需要连接； 可选：加上 WHERE 子句、ORDER BY 等子句。 SELECT s.id, s.name, s.class_id, c.name class_name, s.gender, s.score FROM students s INNER JOIN classes c ON s.class_id = c.id; 选出所有学生，同时返回班级名称 ","date":"2022-09-29","objectID":"/mysql-operations/:10:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"外连接 OUTER JOIN SELECT s.id, s.name, s.class_id, c.name class_name, s.gender, s.score FROM students s RIGHT OUTER JOIN classes c ON s.class_id = c.id; ","date":"2022-09-29","objectID":"/mysql-operations/:11:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"查询范式 SELECT ... FROM tableA ??? JOIN tableB ON tableA.column1 = tableB.column2; ","date":"2022-09-29","objectID":"/mysql-operations/:12:0","tags":["MySQL"],"title":"MySQL 数据库操作","uri":"/mysql-operations/"},{"categories":["DevOps"],"content":"Redis 是什么？ Redis 本质上是一个 Key-Value 类型的内存数据库，很像 memcached，整个数据库统统加载在内存当中进行操作。 因为是纯内存操作，Redis 的性能非常出色，每秒可以处理超过 10 万次读写操作，是已知性能最快的 Key-Value DB。 Redis 可以用来实现很多有用的功能: 用它的 List 来做 FIFO 双向链表，实现一个轻量级的高性能消息队列服务 用它的 Set 可以做高性能的 tag 系统 Redis 也可以对存入的 Key-Value 设置 expire 时间，因此也可以被当作一 个功能加强版的 memcached 来用 ","date":"2022-09-29","objectID":"/redis/:1:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"使用 redis 有哪些好处？ 速度快，因为数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是 O (1) 支持丰富数据类型，支持 string，list，set，zset，hash 支持事务，操作都是原子性，所谓的原子性就是对数据的更改要么全部执行，要么全部不执行 丰富的特性：可用于缓存，消息，按 key 设置过期时间，过期后将会自动删除 ","date":"2022-09-29","objectID":"/redis/:2:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"Memcache 与 Redis 的区别都有哪些？ 存储方式: Memcache 把数据全部存在内存之中，断电后会挂掉，数据不能超过内存大小。 Redis 有部份存在硬盘上，这样能保证数据的持久性。 数据支持类型: Memcache 对数据类型支持相对简单。 Redis 有复杂的数据类型。 ","date":"2022-09-29","objectID":"/redis/:3:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"redis 相比 memcached 有哪些优势？ memcached 所有的值均是简单的字符串，redis 作为其替代者，支持更为丰富的数据类型 redis 的速度比 memcached 快很多 redis 可以持久化其数据 ","date":"2022-09-29","objectID":"/redis/:4:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"为什么用 redis 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 ","date":"2022-09-29","objectID":"/redis/:5:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"Redis 的持久化机制是什么？各自的优缺点？ ","date":"2022-09-29","objectID":"/redis/:6:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"RDB：是 Redis DataBase 缩写快照 RDB 是 Redis 默认的持久化方式。按照一定的时间将内存的数据以快照的形式保存到硬盘中，对应产生的数据文件为 dump.rdb。通过配置文件中的 save 参数来定义快照的周期。 只有一个文件 dump.rdb，方便持久化。 容灾性好，一个文件可以保存到安全的磁盘 ","date":"2022-09-29","objectID":"/redis/:6:1","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"AOF：持久化： AOF 持久化 (即 Append Only File 持久化)，则是将 Redis 执行的每次写命令记录到单独的日志文件中，当重启 Redis 会重新将持久化的日志中文件恢复数据。 ","date":"2022-09-29","objectID":"/redis/:6:2","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"Redis 的过期策略和内存淘汰机制 ","date":"2022-09-29","objectID":"/redis/:7:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"过期策略 redis 采用的是定期删除 + 惰性删除策略 定期删除，redis 默认每个 100ms 检查，是否有过期的 key, 有过期 key 则删除。需要说明的是，redis 不是每个 100ms 将所有的 key 检查一次，而是随机抽取进行检查 (如果每隔 100ms, 全部 key 进行检查，redis 岂不是卡死)。因此，如果只采用定期删除策略，会导致很多 key 到时间没有删除。 于是，惰性删除派上用场。也就是说在你获取某个 key 的时候，redis 会检查一下，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除。 ","date":"2022-09-29","objectID":"/redis/:7:1","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"内存淘汰机制 如果定期删除没删除 key。然后你也没即时去请求 key，也就是说惰性删除也没生效。这样，redis 的内存会越来越高。那么就应该采用内存淘汰机制。 在 redis.conf 中有一行配置 # maxmemory-policy volatile-lru 该配置就是配内存淘汰策略的 noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。应该没人用吧。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key。推荐使用，目前项目在用这种。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key。应该也没人用吧，你不删最少使用 Key, 去随机删。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key。这种情况一般是把 redis 既当缓存，又做持久化存储的时候才用。不推荐 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。依然不推荐 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。不推荐 ","date":"2022-09-29","objectID":"/redis/:7:2","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"Redis 主要消耗什么物理资源？ 内存 ","date":"2022-09-29","objectID":"/redis/:8:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"Redis 的内存用完了会发生什么？ 如果达到设置的上限，Redis 的写命令会返回错误信息（但是读命令还可以正常返回。）或者你可以配置内存淘汰机制，当 Redis 达到内存上限时会冲刷掉旧的内容。 ","date":"2022-09-29","objectID":"/redis/:9:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"缓存穿透和缓存雪崩 ","date":"2022-09-29","objectID":"/redis/:10:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"缓存穿透 缓存穿透，即黑客故意去请求缓存中不存在的数据，导致所有的请求都怼到数据库上，从而数据库连接异常。 ","date":"2022-09-29","objectID":"/redis/:10:1","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"缓存雪崩 即缓存同一时间大面积的失效，这个时候又来了一波请求，结果请求都怼到数据库上，从而导致数据库连接异常。 解决方法： 给缓存的失效时间，加上一个随机值，避免集体失效。 ","date":"2022-09-29","objectID":"/redis/:10:2","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"MySQL 里有 2000w 数据，Redis 中只存 20w 的数据，如何保证 Redis 中的数据都是热点数据 redis 内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略（回收策略）。 可以设置成最近最少使用的数据淘汰。 ","date":"2022-09-29","objectID":"/redis/:11:0","tags":["Redis"],"title":"Redis","uri":"/redis/"},{"categories":["DevOps"],"content":"Docker 安装与卸载 ","date":"2022-09-29","objectID":"/docker-usage/:1:0","tags":["Docker"],"title":"Docker 安装卸载与镜像容器管理","uri":"/docker-usage/"},{"categories":["DevOps"],"content":"安装与验证 更新数据源： sudo apt update 安装一些必备软件包，让 apt 通过 HTTPS 使用软件包。 sudo apt install apt-transport-https ca-certificates curl software-properties-common 将官方 Docker 版本库的 GPG 密钥添加到系统中： curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 将 Docker 版本库添加到APT源： sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu focal stable\" 用新添加的 Docker 软件包来进行升级更新。 sudo apt update 确保要从 Docker 版本库，而不是默认的 Ubuntu 版本库进行安装： apt-cache policy docker-ce # 执行后可以看到 docker-ce 来自 Docker 官方版本库。 安装 Docker sudo apt install docker-ce 检查 Docker 是否正在运行： sudo systemctl status docker ","date":"2022-09-29","objectID":"/docker-usage/:1:1","tags":["Docker"],"title":"Docker 安装卸载与镜像容器管理","uri":"/docker-usage/"},{"categories":["DevOps"],"content":"卸载 Docker 在卸载 Docker 之前，你最好移除所有的容器，镜像，卷和网络。 运行下面的命令停止所有正在运行的容器，并且移除所有的 docker 对象 docker container stop $(docker container ls -aq) docker system prune -a --volumes 接下来你可以使用 apt 命令来卸载 Docker sudo apt purge docker-ce sudo apt autoremove ","date":"2022-09-29","objectID":"/docker-usage/:1:2","tags":["Docker"],"title":"Docker 安装卸载与镜像容器管理","uri":"/docker-usage/"},{"categories":["DevOps"],"content":"Docker 删除容器和镜像 ","date":"2022-09-29","objectID":"/docker-usage/:2:0","tags":["Docker"],"title":"Docker 安装卸载与镜像容器管理","uri":"/docker-usage/"},{"categories":["DevOps"],"content":"管理容器 docker ps -aq # 列出所有容器 ID docker ps -a # 查看所有运行或者不运行容器 docker stop $(docker ps -a -q) # 停止所有的 container（容器），这样才能够删除其中的 images docker rm $(docker ps -a -q) # 如果想要删除所有 container（容器）的话再加一个指令 docker container prune # 删除所有停止的容器 docker stop Name或者ID # 停止一个容器 docker start Name或者ID # 启动一个容器 docker kill Name或者ID # 杀死一个容器 docker restart name或者ID # 重启一个容器 ","date":"2022-09-29","objectID":"/docker-usage/:2:1","tags":["Docker"],"title":"Docker 安装卸载与镜像容器管理","uri":"/docker-usage/"},{"categories":["DevOps"],"content":"管理镜像 docker images # 查看当前有些什么 images docker rmi imageid # 删除 images（镜像），通过 image 的 id 来指定删除谁 docker rmi $(docker images | grep \"^\u003cnone\u003e\" | awk \"{print $3}\") # 想要删除 untagged images，也就是那些 id 为的 image 的话可以用 docker rmi $(docker images -q) # 要删除全部 image（镜像）的话 docker rmi -f $(docker images -q) # 强制删除全部 image 的话 ","date":"2022-09-29","objectID":"/docker-usage/:2:2","tags":["Docker"],"title":"Docker 安装卸载与镜像容器管理","uri":"/docker-usage/"},{"categories":null,"content":"个人介绍 一个已经工作一年多的后端程序员，工作中主要使用 Go 和 Python。 个人邮箱：liuguang@duck.com Github: https://github.com/lqgl BiliBili: https://space.bilibili.com/289855487 ","date":"2022-09-27","objectID":"/about/:0:0","tags":null,"title":"关于自己","uri":"/about/"},{"categories":["Go Blog"],"content":"英文原文：When To Use Generics ","date":"2022-09-19","objectID":"/go-when-generics/:0:0","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"介绍 Go 1.18版本增加了一个主要的新语言特性:支持泛型编程。在本文中，我不打算描述泛型是什么，也不打算描述如何使用泛型。 本文讨论的是在Go代码中何时使用泛型，何时不使用泛型。 需要明确的是，我将提供一般的指导方针，而不是硬性规定。用你自己的判断。但如果您不确定，我建议使用这里讨论的指导方针。 ","date":"2022-09-19","objectID":"/go-when-generics/:1:0","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"编写代码 让我们从编写Go程序的一般指导原则开始:通过编写代码来编写Go程序，而不是通过定义类型。 谈到泛型，如果您通过定义类型参数约束开始编写程序，那么您可能走错了路。从编写函数开始。 很容易在以后添加类型参数，因为它们显然是有用的。 ","date":"2022-09-19","objectID":"/go-when-generics/:2:0","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"什么时候类型参数有用? 也就是说，让我们看看类型参数在哪些情况下是有用的。 ","date":"2022-09-19","objectID":"/go-when-generics/:3:0","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"当使用语言定义的容器类型时 一种情况是编写操作语言定义的特殊容器类型(切片、映射和通道)的函数。 如果一个函数具有这些类型的形参，并且函数代码没有对元素类型做任何特定的假设，那么使用类型形参可能会很有用。 例如，下面是一个函数，它返回任意类型映射中所有键的切片: // MapKeys returns a slice of all the keys in m. // The keys are not returned in any particular order. func MapKeys[Key comparable, Val any](m map[Key]Val) []Key { s := make([]Key, 0, len(m)) for k := range m { s = append(s, k) } return s } 这段代码没有对map键类型做任何假设，也根本不使用map值类型。它适用于任何map类型。这使得它成为使用类型参数的一个很好的候选者。 这类函数的类型参数的替代方案通常是使用反射，但这是一种更笨拙的编程模型，在构建时不进行静态类型检查，并且在运行时通常更慢。 ","date":"2022-09-19","objectID":"/go-when-generics/:3:1","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"通用数据结构 类型参数可能有用的另一种情况是用于通用数据结构。通用数据结构类似于切片或映射，但不是内置在语言中，如链表或二叉树。 今天，需要这种数据结构的程序通常会做两件事中的一件:用特定的元素类型编写它们，或者使用接口类型。 用类型参数替换特定的元素类型可以生成更通用的数据结构，可以在程序的其他部分或由其他程序使用。 用类型参数替换接口类型可以更有效地存储数据，节省内存资源;它还允许代码避免类型断言，并在构建时进行完全的类型检查。 例如，下面是使用类型参数的二叉树数据结构的一部分: // Tree is a binary tree. type Tree[T any] struct { cmp func(T, T) int root *node[T] } // A node in a Tree. type node[T any] struct { left, right *node[T] val T } // find returns a pointer to the node containing val, // or, if val is not present, a pointer to where it // would be placed if added. func (bt *Tree[T]) find(val T) **node[T] { pl := \u0026bt.root for *pl != nil { switch cmp := bt.cmp(val, (*pl).val); { case cmp \u003c 0: pl = \u0026(*pl).left case cmp \u003e 0: pl = \u0026(*pl).right default: return pl } } return pl } // Insert inserts val into bt if not already there, // and reports whether it was inserted. func (bt *Tree[T]) Insert(val T) bool { pl := bt.find(val) if *pl != nil { return false } *pl = \u0026node[T]{val: val} return true } 树中的每个节点都包含类型参数T的值。当使用特定的类型参数实例化树时，该类型的值将直接存储在节点中。它们不会被存储为接口类型。 这是对类型参数的合理使用，因为Tree数据结构(包括方法中的代码)在很大程度上独立于元素类型T。 Tree数据结构确实需要知道如何比较元素类型T的值;它使用了一个传入的比较函数。 您可以在find方法的第四行看到这一点，在调用bt.cmp中。除此之外，类型参数根本不重要。 ","date":"2022-09-19","objectID":"/go-when-generics/:3:2","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"对于类型参数来说，宁可选择函数，也不选择方法，这一点都不重要。 Tree示例说明了另一个通用准则:当您需要比较函数之类的东西时，首选函数而不是方法。 我们可以定义Tree类型，这样元素类型就需要有一个 Compare 或 Less 方法。 这可以通过编写一个需要该方法的约束来实现，这意味着任何用于实例化Tree类型的类型参数都需要有该方法。 结果是，任何想要将Tree与简单数据类型(如int)一起使用的人都必须定义自己的整数类型并编写自己的比较方法。 如果我们定义Tree来接受一个比较函数，如上面所示的代码，那么很容易传入所需的函数。编写比较函数和编写方法一样容易。 如果Tree元素类型碰巧已经有一个Compare方法，那么我们可以简单地使用ElementType这样的方法表达式。Compare作为比较函数。 换句话说，将方法转换为函数要比向类型添加方法简单得多。因此，对于通用的数据类型，最好使用函数，而不是编写需要方法的约束。 ","date":"2022-09-19","objectID":"/go-when-generics/:3:3","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"实现一个通用方法 类型参数有用的另一种情况是，不同的类型需要实现一些公共方法，而不同类型的实现看起来都一样。 例如，考虑标准库的sort.Interface。它要求一个类型实现三个方法:Len、Swap和Less。 下面是实现排序的泛型类型SliceFn的一个示例。用于任何片类型的接口: // SliceFn implements sort.Interface for a slice of T. type SliceFn[T any] struct { s []T less func(T, T) bool } func (s SliceFn[T]) Len() int { return len(s.s) } func (s SliceFn[T]) Swap(i, j int) { s.s[i], s.s[j] = s.s[j], s.s[i] } func (s SliceFn[T]) Less(i, j int) bool { return s.less(s.s[i], s.s[j]) } 对于任何切片类型，Len和Swap方法完全相同。Less方法需要进行比较，这是名称SliceFn的Fn部分。与前面的树示例一样，我们将在创建SliceFn时传入一个函数。 下面是如何使用SliceFn使用比较函数对任何切片进行排序: // SortFn sorts s in place using a comparison function. func SortFn[T any](s []T, less func(T, T) bool) { sort.Sort(SliceFn[T]{s, less}) } 这类似于标准库函数sort.Slice，但是比较函数是使用值而不是slice索引来编写的。 对这类代码使用类型参数是合适的，因为所有切片类型的方法看起来完全相同。 (我应该提到的是，Go 1.19而不是1.18很可能包含一个泛型函数，使用比较函数对切片进行排序，而该泛型函数很可能不会使用sort.interface。 见建议#47619。 但是，即使这个特定的例子很可能没有用，但总的观点仍然是正确的:当您需要实现对所有相关类型看起来相同的方法时，使用类型参数是合理的。) ","date":"2022-09-19","objectID":"/go-when-generics/:3:4","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"什么情况下类型参数没有用? 现在让我们讨论问题的另一面:什么时候不使用类型参数。 ","date":"2022-09-19","objectID":"/go-when-generics/:4:0","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"不要用类型参数替换接口类型 众所周知，Go有接口类型。接口类型允许一种泛型编程。 例如，广泛使用的io.Reader接口提供了一种通用机制，用于从包含信息(例如文件)或产生信息(例如随机数生成器)的任何值读取数据。 如果对某种类型的值所需要做的只是调用该值的方法，那么请使用接口类型，而不是类型参数。 io.Reader易于阅读，效率高，效果好。不需要使用类型参数通过调用read方法从值中读取数据。 例如，可能很容易将这里的第一个函数签名(只使用接口类型)更改为第二个版本(使用类型参数)。 func ReadSome(r io.Reader) ([]byte, error) func ReadSome[T io.Reader](r T) ([]byte, error) 不要做那种改变。省略type参数可以使函数更容易编写和读取，并且执行时间可能是相同的。 最后一点值得强调。虽然可以通过几种不同的方式实现泛型，而且实现会随着时间的推移而改变和改进，但在Go 1.18中使用的实现在很多情况下会将类型为类型参数的值与类型为接口类型的值处理得非常相似。 这意味着使用类型参数通常不会比使用接口类型快。因此，不要仅仅为了速度而从接口类型更改到类型参数，因为它可能不会运行得更快。 ","date":"2022-09-19","objectID":"/go-when-generics/:4:1","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"如果方法实现不同，不要使用类型参数 在决定是使用类型参数还是使用接口类型时，请考虑方法的实现。前面我们说过，如果一个方法的实现对于所有类型都是相同的，那么就使用类型参数。 相反，如果每个类型的实现都不同，那么就使用接口类型并编写不同的方法实现，而不要使用类型参数。 例如，从文件中读取的实现与从随机数生成器中读取的实现完全不同。这意味着我们应该编写两个不同的Read方法，并使用io.Reader这样的接口类型。 ","date":"2022-09-19","objectID":"/go-when-generics/:4:2","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"在适当的地方使用反射 Go有run time reflection。反射允许一种泛型编程，因为它允许您编写适用于任何类型的代码。 如果某些操作必须支持甚至没有方法的类型(因此接口类型没有帮助)，并且如果每个类型的操作不同(因此类型参数不合适)，则使用反射。 一个例子是encoding/json包。 我们不希望要求编码的每个类型都有MarshalJSON方法，因此不能使用接口类型。 但是对接口类型进行编码与对结构类型进行编码完全不同，因此不应该使用类型参数。 相反，这个包使用反射。代码并不简单，但它可以工作。详细信息请参见源代码。 ","date":"2022-09-19","objectID":"/go-when-generics/:4:3","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"一个简单的指南 最后，关于何时使用泛型的讨论可以简化为一个简单的指导原则。 如果您发现自己多次编写完全相同的代码，而副本之间的唯一区别是代码使用了不同的类型，那么请考虑是否可以使用类型参数。 另一种说法是，在注意到将要多次编写完全相同的代码之前，应该避免使用类型参数。 ","date":"2022-09-19","objectID":"/go-when-generics/:5:0","tags":["Go"],"title":"When Generics","uri":"/go-when-generics/"},{"categories":["Go Blog"],"content":"英文原文:Go maps in action ","date":"2022-09-16","objectID":"/go-maps/:0:0","tags":["Go"],"title":"Go Maps","uri":"/go-maps/"},{"categories":["Go Blog"],"content":"介绍 哈希表是计算机科学中最有用的数据结构之一。许多哈希表实现具有不同的属性，但通常它们提供快速查找、添加和删除。Go提供了实现哈希表的内置映射(map)类型。 ","date":"2022-09-16","objectID":"/go-maps/:1:0","tags":["Go"],"title":"Go Maps","uri":"/go-maps/"},{"categories":["Go Blog"],"content":"声明和初始化 Go 映射(map)类型看起来像这样: map[KeyType]ValueType 其中 KeyType 可以是任何comparable的类型(稍后详细介绍)， ValueType 可以是任何类型，包括另一个映射(map)! 变量 m 是字符串键到 int 值的映射: var m map[string]int map 类型是引用类型，比如指针或切片，所以上面 m 的值为nil;它不指向一个初始化的映射。 空映射(nil map)在读取时表现得像空映射(empty map)，但试图写入空映射(nil map)将导致运行时panic; 不要这样做。要初始化一个map ，使用内置的make函数: m = make(map[string]int) make 函数分配并初始化一个散列映射(hash map)数据结构，并返回指向它的映射(map)值。该数据结构的细节是运行时的实现细节，并不是由语言本身指定的。 在本文中，我们将重点讨论映射(maps)的使用，而不是它们的实现。 ","date":"2022-09-16","objectID":"/go-maps/:2:0","tags":["Go"],"title":"Go Maps","uri":"/go-maps/"},{"categories":["Go Blog"],"content":"使用映射(maps) Go 为处理映射(maps)提供了一种熟悉的语法。该语句将键“route”的值设置为66: m[\"route\"] = 66 该语句检索存储在键“route”下的值，并将其赋值给一个新变量i: i := m[\"route\"] 如果请求的键不存在，则得到值类型的零值。在本例中，值类型是int，所以0值是0: j := m[\"root\"] // j == 0 内置的 len 函数根据映射中的项数返回: n := len(m) 内置的 delete 函数从映射中删除一个条目: delete(m, \"route\") delete 函数不返回任何内容，如果指定的键不存在，则不执行任何操作。 双值赋值测试键是否存在: i, ok := m[\"route\"] 在这个语句中，第一个值(i)被分配到存储在键“route”下的值。如果该键不存在，i是值类型的零值(0)。 第二个值(ok)是一个bool值，如果该键在映射中存在则为true，如果不存在则为false。 要在不检索值的情况下测试键，在第一个值处使用下划线: _, ok := m[\"route\"] 要遍历映射的内容，使用range关键字: for key, value := range m { fmt.Println(\"Key:\", key, \"Value:\", value) } 要用一些数据初始化一个映射，使用一个映射字面量: commits := map[string]int{ \"rsc\": 3711, \"r\": 2138, \"gri\": 1908, \"adg\": 912, } 可以使用相同的语法初始化一个空映射(empty map)，在功能上与使用 make 函数相同: m = map[string]int{} ","date":"2022-09-16","objectID":"/go-maps/:3:0","tags":["Go"],"title":"Go Maps","uri":"/go-maps/"},{"categories":["Go Blog"],"content":"利用零值 当键不存在时，映射检索产生零值是很方便的。 例如，布尔值的映射可以用作类似集合的数据结构(回想一下布尔类型的零值为false)。 这个示例遍历一个节点链表并打印它们的值。它使用Node指针的映射来检测列表中的循环。 type Node struct { Next *Node Value interface{} } var first *Node visited := make(map[*Node]bool) // key line for n := first; n != nil; n = n.Next { if visited[n] { // key line fmt.Println(\"cycle detected\") break } visited[n] = true // key line fmt.Println(n.Value) } 表达式 visited[n] 如果访问过n则为真，如果n不存在则为假。没有必要使用二值形式来测试映射中是否存在n;默认的0值为我们做了这些。 另一个有用的零值实例是切片的映射。向nil切片添加值只是分配一个新切片，所以向切片的映射添加值是一行程序;不需要检查键是否存在。 在下面的示例中，People切片使用Person值填充。每个人都有一个名字和一个点赞切片。该示例创建了一个映射，将每个点赞与喜欢它的人的一个切片联系起来。 type Person struct { Name string Likes []string } var people []*Person likes := make(map[string][]*Person) // key line for _, p := range people { for _, l := range p.Likes { likes[l] = append(likes[l], p) // key line } } 打印一个喜欢奶酪的人的列表: for _, p := range likes[\"cheese\"] { fmt.Println(p.Name, \"likes cheese.\") } 打印喜欢培根的人的数量: fmt.Println(len(likes[\"bacon\"]), \"people like bacon.\") 请注意，由于range和len都将nil切片视为零长度的切片，所以即使没有人喜欢奶酪或培根，最后两个例子也可以工作(不管这种情况有多不可能)。 ","date":"2022-09-16","objectID":"/go-maps/:4:0","tags":["Go"],"title":"Go Maps","uri":"/go-maps/"},{"categories":["Go Blog"],"content":"键类型 如前所述，映射键可以是任何可比较的类型。 语言规范对此进行了精确的定义，但简而言之，可比较的类型是布尔型、数值型、字符串型、指针型、通道型和接口型，以及只包含这些类型的结构体或数组。 值得注意的是，列表中没有切片、映射和函数;这些类型不能使用==进行比较，也不能用作映射键。 显然，字符串、int和其他基本类型应该作为映射键可用，但结构体键(struct keys)可能出乎意料。Struct可用于多个维度的键数据。 例如，这个map of maps可以用来按国家统计网页点击量: hits := make(map[string]map[string]int) 这是string到(string到int)的映射。外部映射的每个键都是到具有自己内部映射的网页的路径。 每个内部映射键都是两个字母的国家代码。这个表达式获取一个澳大利亚人加载文档页面的次数: n := hits[\"/doc/\"][\"au\"] 不幸的是，这种方法在添加数据时变得笨拙，因为对于任何给定的外部键，你必须检查内部映射是否存在，并在需要时创建它: func add(m map[string]map[string]int, path, country string) { mm, ok := m[path] if !ok { mm = make(map[string]int) m[path] = mm } mm[country]++ } add(hits, \"/doc/\", \"au\") 另一方面，使用一个带有struct键的单一映射的设计消除了所有的复杂性: type Key struct { Path, Country string } hits := make(map[Key]int) 当一个越南人访问主页时，增加(可能创建)相应的计数器是一行代码: hits[Key{\"/\", \"vn\"}]++ 同样，也很容易看出有多少瑞士人读过spec: n := hits[Key{\"/ref/spec\", \"ch\"}] ","date":"2022-09-16","objectID":"/go-maps/:5:0","tags":["Go"],"title":"Go Maps","uri":"/go-maps/"},{"categories":["Go Blog"],"content":"并发 映射对于并发使用是不安全的:它没有定义当您同时读写它们时会发生什么。 如果需要从并发执行的goroutine中对映射进行读写，则访问必须通过某种同步机制进行调节。 保护映射的一种常用方法是使用sync.RWMutex。 这条语句声明了一个计数器变量，它是一个匿名结构体，包含一个映射和一个嵌入的sync.RWMutex。 var counter = struct{ sync.RWMutex m map[string]int }{m: make(map[string]int)} 要从计数器读取，取读锁: counter.RLock() n := counter.m[\"some_key\"] counter.RUnlock() fmt.Println(\"some_key:\", n) 要写入计数器，取写锁: counter.Lock() counter.m[\"some_key\"]++ counter.Unlock() ","date":"2022-09-16","objectID":"/go-maps/:6:0","tags":["Go"],"title":"Go Maps","uri":"/go-maps/"},{"categories":["Go Blog"],"content":"迭代顺序 当使用range循环在映射上迭代时，不会指定迭代顺序，也不能保证从一个迭代到下一个迭代是相同的。 如果需要稳定的迭代顺序，则必须维护指定该顺序的独立数据结构。 下面的例子使用一个单独的排序的键切片按键的顺序打印一个map[int]字符串: import \"sort\" var m map[int]string var keys []int for k := range m { keys = append(keys, k) } sort.Ints(keys) for _, k := range keys { fmt.Println(\"Key:\", k, \"Value:\", m[k]) } ","date":"2022-09-16","objectID":"/go-maps/:7:0","tags":["Go"],"title":"Go Maps","uri":"/go-maps/"},{"categories":["Go Blog"],"content":"英文原文: Share Memory By Communicating 传统的线程模型（例如，在编写Java、C++和Python程序时通常使用）要求程序员使用共享内存在线程之间进行通信。 通常情况下，共享数据结构受到锁的保护，而线程将争夺这些锁来访问数据。在某些情况下，使用线程安全的数据结构(如Python的Queue)会使这更容易。 Go的并发原语 - goroutines和channels - 为构造并发软件提供了一种优雅而独特的方法。(这些概念有一个有趣的历史， 始于C.A.R.Hoare的 Communicating Sequential Processes）。 Go鼓励使用通道在goroutine之间传递对数据的引用，而不是明确地使用锁来调解对共享数据的访问。这种方法可以确保在给定时间内只有一个goroutine可以访问数据。 这个概念在Effective Go文件中得到了总结（任何Go程序员都必须阅读）: 不要通过共享内存来进行通信，相反，通过通信来共享内存 考虑一个程序，它轮询一个url列表。在传统的线程环境中，数据的结构可能是这样的: type Resource struct { url string polling bool lastPolled int64 } type Resources struct { data []*Resource lock *sync.Mutex } 然后一个轮询器函数(其中许多会在单独的线程中运行)可能看起来像这样: func Poller(res *Resources) { for { // get the least recently-polled Resource // and mark it as being polled res.lock.Lock() var r *Resource for _, v := range res.data { if v.polling { continue } if r == nil || v.lastPolled \u003c r.lastPolled { r = v } } if r != nil { r.polling = true } res.lock.Unlock() if r == nil { continue } // poll the URL // update the Resource's polling and lastPolled res.lock.Lock() r.polling = false r.lastPolled = time.Nanoseconds() res.lock.Unlock() } } 这个函数大约有一页那么长，需要更多的细节来完成它。它甚至不包括URL轮询逻辑(它本身只有几行)，也不会优雅地处理耗尽资源池的问题。 让我们来看看使用Go习语实现的相同功能。在本例中，Poller是一个函数，它从输入通道接收要轮询的资源，并在完成时将它们发送到输出通道。 type Resource string func Poller(in, out chan *Resource) { for r := range in { // poll the URL // send the processed Resource to out out \u003c- r } } 前面示例中微妙的逻辑明显不存在了，而且我们的Resource数据结构不再包含记账数据。事实上，剩下的都是重要的部分。这应该会让您对这些简单的语言特性的功能有一个初步的了解。 上面的代码片段有许多遗漏。有关使用这些思想的完整的、惯用的Go程序的演练，请参见Codewalk Share Memory By Communicating。 ","date":"2022-09-16","objectID":"/go-communication/:0:0","tags":["Go"],"title":"Share Memory By Communicating","uri":"/go-communication/"},{"categories":["Go Blog"],"content":"英文原文: Defer, Panic, and Recover ","date":"2022-09-16","objectID":"/go-defer-panic-and-recover/:0:0","tags":["Go"],"title":"Defer Panic and Recover","uri":"/go-defer-panic-and-recover/"},{"categories":["Go Blog"],"content":"介绍 Go有常见的控制流机制：if、for、switch、goto。它也有go语句来运行单独的goroutine中的代码。这里我想讨论一些不太常见的机制：defer、panic和recover。 ","date":"2022-09-16","objectID":"/go-defer-panic-and-recover/:1:0","tags":["Go"],"title":"Defer Panic and Recover","uri":"/go-defer-panic-and-recover/"},{"categories":["Go Blog"],"content":"defer defer语句将一个函数调用推到一个列表中。保存的调用列表在周围的函数返回后被执行。defer通常被用来简化执行各种清理动作的函数。 例如，让我们看看一个打开两个文件并将一个文件的内容复制到另一个文件的函数: func CopyFile(dstName, srcName string) (written int64, err error) { src, err := os.Open(srcName) if err != nil { return } dst, err := os.Create(dstName) if err != nil { return } written, err = io.Copy(dst, src) dst.Close() src.Close() return } 这可以工作，但有一个错误。如果对os.Create的调用失败，该函数将在没有关闭源文件的情况下返回。这可以通过在第二个返回语句之前调用 src.Close 来轻松解决，但如果这个函数更复杂，问题可能就不会那么容易被注意和解决了。通过引入defer语句，我们可以确保文件始终被关闭: func CopyFile(dstName, srcName string) (written int64, err error) { src, err := os.Open(srcName) if err != nil { return } defer src.Close() dst, err := os.Create(dstName) if err != nil { return } defer dst.Close() return io.Copy(dst, src) } 延迟语句允许我们考虑在打开每个文件后立即关闭它，确保无论函数中返回语句的数量如何，文件都将被关闭。 延迟语句的行为是直接的和可预测的。有三个简单的规则: 1. 当对defer语句求值时，对deferred函数的实参进行求值。 在本例中，当Println调用被延迟时，表达式“i”被求值。延迟调用将在函数返回后打印“0”。 func a() { i := 0 defer fmt.Println(i) i++ return } 2. 延迟的函数调用在周围的函数返回后按照后进先出的顺序执行 这个函数输出\"3210\"： func b() { for i := 0; i \u003c 4; i++ { defer fmt.Print(i) } } 3. 延迟函数可以读取并赋值给返回函数的命名返回值。 在本例中，延迟函数在周围的函数返回后将返回值i加1。因此，这个函数返回2: func c() (i int) { defer func() { i++ }() return 1 } 这便于修改函数的错误返回值;稍后我们将看到一个这样的例子。 ","date":"2022-09-16","objectID":"/go-defer-panic-and-recover/:1:1","tags":["Go"],"title":"Defer Panic and Recover","uri":"/go-defer-panic-and-recover/"},{"categories":["Go Blog"],"content":"Panic Panic 是一个内置函数，它会停止正常的控制流程，开始panic。当函数F调用panic时，F的执行将停止，F中的任何defer函数将正常执行，然后F返回给调用者。对于调用者来说，F的行为就像是在呼叫panic。该过程沿着堆栈向上继续，直到当前goroutine中的所有函数都返回，此时程序将崩溃。panic可以通过直接调用panic而引发。它们也可能由运行时错误引起，例如越界数组访问。 ","date":"2022-09-16","objectID":"/go-defer-panic-and-recover/:1:2","tags":["Go"],"title":"Defer Panic and Recover","uri":"/go-defer-panic-and-recover/"},{"categories":["Go Blog"],"content":"Recover Recover 是一个内置函数，它可以重新控制一个处于panic状态的goroutine。Recover只在延迟函数中有用。在正常执行期间，调用恢复将返回nil，没有其他效果。如果当前goroutine处于panic状态，则调用recover将捕获给panic的值并恢复正常执行。 下面是一个演示panic和defer机制的示例程序: package main import \"fmt\" func main() { f() fmt.Println(\"Returned normally from f.\") } func f() { defer func() { if r := recover(); r != nil { fmt.Println(\"Recovered in f\", r) } }() fmt.Println(\"Calling g.\") g(0) fmt.Println(\"Returned normally from g.\") } func g(i int) { if i \u003e 3 { fmt.Println(\"Panicking!\") panic(fmt.Sprintf(\"%v\", i)) } defer fmt.Println(\"Defer in g\", i) fmt.Println(\"Printing in g\", i) g(i + 1) } 函数g接受int i，如果i大于3，它会发生panic，否则它调用自身，参数是i+1。函数f延迟调用recover的函数并打印恢复的值(如果它是非空值)。在继续阅读之前，试着想象一下这个程序的输出可能是什么。 该程序将输出： Calling g. Printing in g 0 Printing in g 1 Printing in g 2 Printing in g 3 Panicking! Defer in g 3 Defer in g 2 Defer in g 1 Defer in g 0 Recovered in f 4 Returned normally from f. 如果从f中删除deferred函数，则panic没有恢复，并且到达goroutine的调用堆栈的顶部，从而终止程序。修改后的程序将输出: Calling g. Printing in g 0 Printing in g 1 Printing in g 2 Printing in g 3 Panicking! Defer in g 3 Defer in g 2 Defer in g 1 Defer in g 0 panic: 4 panic PC=0x2a9cd8 [stack trace omitted] ","date":"2022-09-16","objectID":"/go-defer-panic-and-recover/:1:3","tags":["Go"],"title":"Defer Panic and Recover","uri":"/go-defer-panic-and-recover/"},{"categories":["Go Blog"],"content":"More 关于panic和recover的真实示例，请参阅Go标准库中的json包。它用一组递归函数对接口进行编码。如果在遍历值时发生错误，将调用panic以将堆栈展开到顶层函数调用，该函数从panic中恢复并返回适当的错误值(请参阅encode.go中encodeState类型的’ error ‘和’ marshal ‘方法)。 Go库中的惯例是，即使一个包在内部使用panic，它的其他外部API仍然显式的错误返回值。 defer 的其他用途（除了前面给出的file.Close例子之外）包括释放一个mutex： mu.Lock() defer mu.Unlock() 打印页脚： printHeader() defer printFooter() 以及更多。 总之，defer语句(带或不带panic和recover)为控制流提供了一种不同寻常的强大机制。它可以用于建模由其他编程语言中的特殊目的结构实现的许多特性。试一下。 ","date":"2022-09-16","objectID":"/go-defer-panic-and-recover/:1:4","tags":["Go"],"title":"Defer Panic and Recover","uri":"/go-defer-panic-and-recover/"},{"categories":["Go Blog"],"content":"英文原文:Go Slices: usage and internals ","date":"2022-09-16","objectID":"/go-slice/:0:0","tags":["Go"],"title":"Go Slice","uri":"/go-slice/"},{"categories":["Go Blog"],"content":"介绍 Go 的切片类型为处理类型化数据的序列提供了一种方便而有效的方法。切片类似于其他语言中的数组，但有一些不寻常的特性。本文将探讨什么是切片以及如何使用切片。 ","date":"2022-09-16","objectID":"/go-slice/:1:0","tags":["Go"],"title":"Go Slice","uri":"/go-slice/"},{"categories":["Go Blog"],"content":"数组(Arrays) 切片类型是建立在Go的数组类型之上的一个抽象，因此要理解切片，我们必须先理解数组。 一个数组类型定义指定了一个长度和一个元素类型。例如，[4]int类型表示一个包含四个整数的数组。一个数组的大小是固定的；它的长度是其类型的一部分（[4]int和[5]int是不同的，不兼容的类型）。数组可以用常规方式进行索引，所以表达式s[n]可以访问第n个元素，从0开始。 var a [4]int a[0] = 1 i := a[0] // i == 1 数组不需要显式初始化，数组的零值是一个随时可以使用的数组，其元素本身已经归零: // a[2] == 0, the zero value of the int type [4]int的内存表示只是四个按顺序排列的整数值。 Go 的数组是数值。一个数组变量表示整个数组；它不是指向第一个数组元素的指针（在C语言中是这样的）。这意味着当你分配或传递一个数组的值时，你将复制其内容。(为了避免复制，你可以传递一个指向数组的指针，但那是一个指向数组的指针，而不是一个数组。) 考虑数组的一种方法是作为一种结构，但具有索引而不是命名字段：一个固定大小的复合值. 数组字面值可以这样指定： b := [2]string{\"Penn\", \"Teller\"} 或者，您可以让编译器为您计算数组元素: b := [...]string{\"Penn\", \"Teller\"} 在这两种情况下，b的类型都是[2]string ","date":"2022-09-16","objectID":"/go-slice/:2:0","tags":["Go"],"title":"Go Slice","uri":"/go-slice/"},{"categories":["Go Blog"],"content":"切片(Slices) 数组有自己的位置，但它们有点不灵活，所以在Go代码中不太常见。然而，切片随处可见。它们以数组为基础，提供强大的力量和便利。 切片的类型规范是[]T，其中T是切片元素的类型。与数组类型不同，切片类型没有指定的长度。 slice 字面量的声明就像数组字面量一样，只是省略了元素计数： letters := []string{\"a\", \"b\", \"c\", \"d\"} 切片可以用内置的make函数创建，它的签名是： func make([]T, len, cap) []T T 代表要创建切片的元素类型。make函数接收元素类型，长度和可选容量。调用时，make 将分配一个数组并返回指向该数组的切片。 var s []byte s = make([]byte, 5, 5) // s == []byte{0, 0, 0, 0, 0} 当省略容量参数时，默认为指定的长度。这是相同代码的更简洁版本： s := make([]byte, 5) 可以使用内置的len和cap函数检查切片的长度和容量 len(s) == 5 cap(s) == 5 接下来的两个部分讨论长度和容量之间的关系。 切片的零值为nil。len和cap函数对于nil切片都将返回0。 切片也可以通过“切片”现有的切片或数组来形成。切片是通过指定一个半开的区间和两个用冒号分隔的索引来完成的。例如，表达式b[1:4]创建了一个包含b的元素1到3的切片(结果切片的索引将是0到2)。 b := []byte{'g', 'o', 'l', 'a', 'n', 'g'} // b[1:4] == []byte{'o', 'l', 'a'}, sharing the same storage as b 切片表达式的开始和结束索引是可选的;它们的默认值分别为0和slice的长度: // b[:2] == []byte{'g', 'o'} // b[2:] == []byte{'l', 'a', 'n', 'g'} // b[:] == b 这也是在给定数组的情况下创建切片的语法: x := [3]string{\"Лайка\", \"Белка\", \"Стрелка\"} s := x[:] // a slice referencing the storage of x ","date":"2022-09-16","objectID":"/go-slice/:3:0","tags":["Go"],"title":"Go Slice","uri":"/go-slice/"},{"categories":["Go Blog"],"content":"切片内部(Slice internals) 一个切片是一个数组段的描述符。它由指向数组的指针、段的长度和它的容量(段的最大长度)组成。 之前由 make([]byte, 5) 创建的变量s的结构是这样的: 长度是切片引用的元素的数量。容量是底层数组中的元素数量(从切片指针引用的元素开始)。在接下来的几个示例中，我们将清楚地说明长度和容量之间的区别。 当我们对s切片时，观察切片数据结构的变化及其与基础数组的关系： s = s[2:4] 切片操作不会复制原始切片的数据。它创建一个指向原始数组的新切片值。这使得切片操作与操作数组下标一样高效。因此，修改新切片的元素(而不是切片本身)会修改原始切片的元素: d := []byte{'r', 'o', 'a', 'd'} e := d[2:] // e == []byte{'a', 'd'} e[1] = 'm' // e == []byte{'a', 'm'} // d == []byte{'r', 'o', 'a', 'm'} 之前我们将s切成比其容量更短的长度。我们可以通过再次切片使s增长到它的容量： s = s[:cap(s)] 一个切片不能超过它的容量。尝试这样做将导致运行时恐慌，就像在切片或数组边界外进行索引一样。同样，不能将切片重新切到0以下以访问数组中较早的元素。 ","date":"2022-09-16","objectID":"/go-slice/:4:0","tags":["Go"],"title":"Go Slice","uri":"/go-slice/"},{"categories":["Go Blog"],"content":"Growing slices (the copy and append functions) 要增长一个切片的容量，必须创建一个新的、更大的切片，并将原始切片的内容复制到其中。这种技术就是来自其他语言的动态数组实现在幕后工作的方式。下面的例子通过创建一个新的切片t，将s的内容复制到t中，然后将切片值t赋给s，从而使s的容量翻倍: t := make([]byte, len(s), (cap(s)+1)*2) // +1 in case cap(s) == 0 for i := range s { t[i] = s[i] } s = t 内置的copy函数简化了这个常见操作的循环部分。顾名思义，copy将数据从源片复制到目标片。它返回复制的元素数量。 func copy(dst, src []T) int copy 函数支持在不同长度的切片之间进行复制(它只复制较少数量的元素)。此外，copy函数可以处理共享相同底层数组的源片和目标片，正确处理重叠的切片。 使用copy，我们可以简化上面的代码片段: t := make([]byte, len(s), (cap(s)+1)*2) copy(t, s) s = t 一种常见的操作是将数据追加到一个切片的末尾。这个函数将字节元素追加到字节的切片中，必要时增加切片，并返回更新后的切片值： func AppendByte(slice []byte, data ...byte) []byte { m := len(slice) n := m + len(data) if n \u003e cap(slice) { // if necessary, reallocate // allocate double what's needed, for future growth. newSlice := make([]byte, (n+1)*2) copy(newSlice, slice) slice = newSlice } slice = slice[0:n] copy(slice[m:n], data) return slice } 可以这样使用 AppendByte： p := []byte{2, 3, 5} p = AppendByte(p, 7, 11, 13) // p == []byte{2, 3, 5, 7, 11, 13} 像 AppendByte 这样的函数很有用，因为它们提供了对切片增长方式的完全控制。根据程序的特点，可能需要分配更小或更大的块，或对重新分配的大小设置一个上限。 但是大多数程序不需要完全控制，所以Go提供了一个内置的 append 函数，这对大多数目的都很好;它有签名: func append(s []T, x ...T) []T append 函数将元素x追加到切片s的末尾，如果需要更大的容量，则会增长切片。 a := make([]int, 1) // a == []int{0} a = append(a, 1, 2, 3) // a == []int{0, 1, 2, 3} 要将一个切片附加到另一个切片，使用…将第二个参数拓展为一个参数列表。 a := []string{\"John\", \"Paul\"} b := []string{\"George\", \"Ringo\", \"Pete\"} a = append(a, b...) // equivalent to \"append(a, b[0], b[1], b[2])\" // a == []string{\"John\", \"Paul\", \"George\", \"Ringo\", \"Pete\"} 因为切片的零值(nil)就像一个零长度的切片，你可以声明一个切片变量，然后在循环中添加它: // Filter returns a new slice holding only // the elements of s that satisfy fn() func Filter(s []int, fn func(int) bool) []int { var p []int // == nil for _, v := range s { if fn(v) { p = append(p, v) } } return p } ","date":"2022-09-16","objectID":"/go-slice/:5:0","tags":["Go"],"title":"Go Slice","uri":"/go-slice/"},{"categories":["Go Blog"],"content":"一个可能的“陷阱” 如前所述，重新切片并不会生成底层数组的副本。整个数组将保存在内存中，直到它不再被引用。有时，这可能导致程序将所有数据保存在内存中，而实际上只需要一小部分数据。 例如，FindDigits 函数将一个文件加载到内存中，并在其中搜索第一组连续的数字，将它们作为一个新片返回。 var digitRegexp = regexp.MustCompile(\"[0-9]+\") func FindDigits(filename string) []byte { b, _ := ioutil.ReadFile(filename) return digitRegexp.Find(b) } 此代码的行为与所宣传的一样，但返回的[]byte指向包含整个文件的数组。由于切片引用原始数组，只要切片被保留在垃圾收集器周围，就不能释放数组;文件的几个有用字节将整个内容保存在内存中。 为了解决这个问题，你可以在返回之前将感兴趣的数据复制到一个新的切片: func CopyDigits(filename string) []byte { b, _ := ioutil.ReadFile(filename) b = digitRegexp.Find(b) c := make([]byte, len(b)) copy(c, b) return c } 这个函数的一个更简洁的版本可以通过使用 append 构造。 ","date":"2022-09-16","objectID":"/go-slice/:6:0","tags":["Go"],"title":"Go Slice","uri":"/go-slice/"},{"categories":["Life"],"content":"OKR (Objectives and Key Results) 由 Andy Grove 发明，Intel、Google 践行的目标管理方法。 ","date":"2022-08-29","objectID":"/use-okr/:0:0","tags":["method"],"title":"使用OKR","uri":"/use-okr/"},{"categories":["Life"],"content":"OKR 能带给你什么 Something Bigger Than Yourself. 通过设立「富有挑战性的目标」，会让你取得之前都不敢想象的成就。 ","date":"2022-08-29","objectID":"/use-okr/:1:0","tags":["method"],"title":"使用OKR","uri":"/use-okr/"},{"categories":["Life"],"content":"我的「普通」新年计划 学习理财： 学习香帅金融课 学习薛兆丰经济学课 了解指数基金 ","date":"2022-08-29","objectID":"/use-okr/:2:0","tags":["method"],"title":"使用OKR","uri":"/use-okr/"},{"categories":["Life"],"content":"「普通」新年计划有什么问题？ 由待办事项组成，缺失目标导向 无法量化完成情况 动力不足 ","date":"2022-08-29","objectID":"/use-okr/:3:0","tags":["method"],"title":"使用OKR","uri":"/use-okr/"},{"categories":["Life"],"content":"使用 OKR 制定新年计划 目标 Objects: 通过股票投资赚钱 关键结果: Key Results 输出股票资产的知识框架(含微观与宏观的分析方法) 设计定投规则，使用月收入30%进行定投，执行效率不得低于90% 建仓时间至少为一年，投资回报率大于等于6% ","date":"2022-08-29","objectID":"/use-okr/:4:0","tags":["method"],"title":"使用OKR","uri":"/use-okr/"},{"categories":["Life"],"content":"OKR的「精髓」是什么 一个目标，可以对应多个关键结果， 目标达成的情况是由这几个关键结果的完成情况来衡量的。 I will achieve object as measured by key values. ","date":"2022-08-29","objectID":"/use-okr/:5:0","tags":["method"],"title":"使用OKR","uri":"/use-okr/"},{"categories":["Life"],"content":"制定目标与关键结果的建议 富有挑战性的目标，迫使你逆向思考 聚焦在目标上，而非行动项 KR 三要素：必须明确|有时间节点｜符合实际 ","date":"2022-08-29","objectID":"/use-okr/:6:0","tags":["method"],"title":"使用OKR","uri":"/use-okr/"},{"categories":["Life"],"content":"OKR 的注意点 制定 KR 需要前期调查 70% 完成算优秀 KR 随进展可变 ","date":"2022-08-29","objectID":"/use-okr/:7:0","tags":["method"],"title":"使用OKR","uri":"/use-okr/"},{"categories":["Life"],"content":"与「周计划」结合使用 参考资料 使用谷歌的目标管理方法-OKR：制定新年计划 ","date":"2022-08-29","objectID":"/use-okr/:8:0","tags":["method"],"title":"使用OKR","uri":"/use-okr/"},{"categories":["Life"],"content":"为什么要提高「专注力」 专注力 = 时间管理 = 注意力 本质：提高时间利用率 做成事的两个关键：效能(Effectiveness)与效率(Efficiency) 本质：专注力影响效率 取得成就 效能：做正确的事 - Do The Right Thing. 效率: 如何正确地做事 - Do The Thing Right. ","date":"2022-08-29","objectID":"/impove-focus/:1:0","tags":["Method"],"title":"提高专注力","uri":"/impove-focus/"},{"categories":["Life"],"content":"DI (Deep Involved) 一段时间内的一种专注状态 一个 DI 的时长为45分钟 DI 期间内只聚焦一件事 一天安排若干个 DI DI 不完整时，重新计时 两个 DI 之间有休息间隔，一般为10～15分钟 ","date":"2022-08-29","objectID":"/impove-focus/:2:0","tags":["Method"],"title":"提高专注力","uri":"/impove-focus/"},{"categories":["Life"],"content":"消除3大阻碍 ","date":"2022-08-29","objectID":"/impove-focus/:3:0","tags":["Method"],"title":"提高专注力","uri":"/impove-focus/"},{"categories":["Life"],"content":"被打断 创造条件，远离干扰源 请求对方的理解 - 重新约个时间 停止 DI，重新开始 ","date":"2022-08-29","objectID":"/impove-focus/:3:1","tags":["Method"],"title":"提高专注力","uri":"/impove-focus/"},{"categories":["Life"],"content":"胡思乱想 写在纸上 闭眼后深呼吸，注意力放在呼吸上 多做几次深呼吸 ","date":"2022-08-29","objectID":"/impove-focus/:3:2","tags":["Method"],"title":"提高专注力","uri":"/impove-focus/"},{"categories":["Life"],"content":"状态不在线 从简单的事情开始 坚持半个 DI 后再看看 回顾下目标，检查下Deadline 放弃，做当前想干的事情 ^.^ ","date":"2022-08-29","objectID":"/impove-focus/:3:3","tags":["Method"],"title":"提高专注力","uri":"/impove-focus/"},{"categories":["Life"],"content":"工具 Windows 下的倒计时工具：wnr Mac 下的倒计时工具：Horo (Mac App Store支持) iOS下的倒计时工具：使用siri进行45分钟倒计时 参考资料 高效率的秘密，我是如何提高专注力 | 生产力提升 ","date":"2022-08-29","objectID":"/impove-focus/:4:0","tags":["Method"],"title":"提高专注力","uri":"/impove-focus/"},{"categories":["Life"],"content":"动机 年初制定的计划完成度很低 时常感觉不知道该做什么 总是疲于应付突发情况 被事情带着走，控制感弱 焦虑，缺乏安全感 没有成长 ","date":"2022-08-29","objectID":"/make-weekly-plan/:1:0","tags":["Plan","Method"],"title":"制定周计划","uri":"/make-weekly-plan/"},{"categories":["Life"],"content":"工具 笔记软件, 多端同步 eg: 印象笔记 ","date":"2022-08-29","objectID":"/make-weekly-plan/:2:0","tags":["Plan","Method"],"title":"制定周计划","uri":"/make-weekly-plan/"},{"categories":["Life"],"content":"整体流程内容 本周计划列表 记录一周中需要做的事 新发现，新思考 记录那些闪现的 idea 周记录 中长期计划表 年度计划、季度计划、月计划 复盘回顾 价值评估 任务分解 主题日 ","date":"2022-08-29","objectID":"/make-weekly-plan/:3:0","tags":["Plan","Method"],"title":"制定周计划","uri":"/make-weekly-plan/"},{"categories":["Life"],"content":"整体流程步骤 记录 记录下这周发生的所有事情 回顾 复盘这周所有任务的执行情况，并结合中长期计划表， 再经过价值评估与任务分解得出下周的待办事项。 编排 将待办事件安排到每一个工作日 ","date":"2022-08-29","objectID":"/make-weekly-plan/:4:0","tags":["Plan","Method"],"title":"制定周计划","uri":"/make-weekly-plan/"},{"categories":["Life"],"content":"主题 ","date":"2022-08-29","objectID":"/make-weekly-plan/:5:0","tags":["Plan","Method"],"title":"制定周计划","uri":"/make-weekly-plan/"},{"categories":["Life"],"content":"记录 周计划执行时的记录 将遇到过的、想到过的、都记录下来 原则：先记下来，不着急做，除非是重要且紧急的事 确认 需要确认的内容 补充 需要补充的内容 关注 需要留意关注的内容 思考 平时思考的结果 闪现 idea 的记录 将大脑从记忆琐事中解脱出来，用来思考问题 原则：使用工具、将闪现的想法记录下来，重要的是这个操作要迅速，成本低，不能影响到思考本身 场景: 走路 洗澡 浏览网页 与人交谈 ","date":"2022-08-29","objectID":"/make-weekly-plan/:5:1","tags":["Plan","Method"],"title":"制定周计划","uri":"/make-weekly-plan/"},{"categories":["Life"],"content":"回顾 每周回顾 对本周的复盘 创建新的笔记，将这周发生的所有信息都汇总起来，包括中长期计划表中的内容 逐条评估！ 完成的情况怎么样？做得好的地方，没做好的地方 完成不理想的原因是什么？怎么改善？ 下周是否还要继续？是否可以委托别人做？ 是否有价值仔细阅读与学习？ 是否是对已有知识体系的补充？ 是否有利于长期成长？要持续关注？ 甄别最有价值的事情 原则：Pareto principle - 关键少数法则 是否有助于达成目标的关键事项 重要但不紧急的任务，可以降低发生重要又紧急的事情的概率， 会增加你的掌控感 每件事，都要有目标 原则：SMART criteria - SMART法则 目标，最好是可量化的 每一天只需要聚焦在具体事项的目标达成上 ","date":"2022-08-29","objectID":"/make-weekly-plan/:5:2","tags":["Plan","Method"],"title":"制定周计划","uri":"/make-weekly-plan/"},{"categories":["Life"],"content":"编排 事项划分 结合复盘内容与中长期计划表，将任务安排到下周的每一天 可以分为以下几大类： 工作 学习 创作 生活 享受 使命 重要的是平衡，否则无法长期维持 努力的目标那就是，享受生活 安排到每一天 同一天内尽量安排类似的事情 - 主题日 把一定要完成的事情安排在效率最高，不被打扰的时段 - 要事第一 ","date":"2022-08-29","objectID":"/make-weekly-plan/:5:3","tags":["Plan","Method"],"title":"制定周计划","uri":"/make-weekly-plan/"},{"categories":["Life"],"content":"注意事项 根据实际情况制定任务量，并不是越多越好，追求的应该是高质量完成， 不要给自己太多压力。 太多未完成任务会带来沮丧感。 如果经常被打断(集中思考)，需要与外部做协调，或给自己创造环境。 提醒自己关注下计划表 参考资料 我是怎么做周计划 | 生产力提升 | 我的方法 ","date":"2022-08-29","objectID":"/make-weekly-plan/:6:0","tags":["Plan","Method"],"title":"制定周计划","uri":"/make-weekly-plan/"},{"categories":["DevOps"],"content":"创建 GitHub repo,并生成token ","date":"2020-12-14","objectID":"/github-picgo/:1:0","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"创建仓库 ","date":"2020-12-14","objectID":"/github-picgo/:1:1","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"生成token 进入个人中心设置 进入Developer settings中，选择Personal aceess token,然后Generate new token. 生成 token(Note 部分随便写即可，下边的权限把 repo 相关的打上勾即可) 接下来便会生成一个 token，将它复制下来，因为一旦刷新网页，你将见不到这个 token 了。 ","date":"2020-12-14","objectID":"/github-picgo/:1:2","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"下载安装 PicGo ","date":"2020-12-14","objectID":"/github-picgo/:2:0","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"安装PicGo PicGo仓库。 ","date":"2020-12-14","objectID":"/github-picgo/:2:1","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"GitHub图床配置 ","date":"2020-12-14","objectID":"/github-picgo/:3:0","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"图床配置 PicGo的Github图床配置有固定的格式: 仓库名格式为:用户名/仓库名 分支名设置为:master Token设置为:上面获取到的token 存储路径可以随意,如设置为:img/ 自定义域名格式为: https://raw.githubusercontent.com/用户名/仓库名/分支名 ","date":"2020-12-14","objectID":"/github-picgo/:3:1","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"GitHubPlus插件配置 GitHubPlus可以同步相册,当在PigGo的相册中删除图片后,会同步删除GitHub上的图片. ","date":"2020-12-14","objectID":"/github-picgo/:4:0","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"同步更新删除GitHub 在插件设置的搜索框中,输入插件的名称:picgo-plugin-github-plus,然后安装这个插件. ","date":"2020-12-14","objectID":"/github-picgo/:4:1","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"配置GitHubPlus 这个插件的配置和上面的Github图片配置差不多一样. ","date":"2020-12-14","objectID":"/github-picgo/:4:2","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"PicGo其他配置 ","date":"2020-12-14","objectID":"/github-picgo/:5:0","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"上传快捷方式 进入PicGo设置，修改上传快捷键(像这样，配合SniPaste食用更佳，每次F1截完图，选择复制，然后按绑定的组合键即可实现快速上传（快捷键上传的是剪切板中的东西）) ","date":"2020-12-14","objectID":"/github-picgo/:5:1","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"参考文档 PicGo GitHub图床 配置PicGo成为多平台图床工具 ","date":"2020-12-14","objectID":"/github-picgo/:6:0","tags":["Github","PicGo"],"title":"使用 GitHub repo + PicGo 搭建博客图床","uri":"/github-picgo/"},{"categories":["DevOps"],"content":"Windows 常用快捷键 Ctrl + C : 复制 Ctrl + V : 粘贴 Ctrl + X : 剪切 Ctrl + Z : 撤销 Alt + F4 : 关闭窗口 Ctrl + Shift + Esc : 任务管理器 Win + R : 运行 Win + E : 资源管理器 ","date":"2020-07-05","objectID":"/windows-shortcut/:1:0","tags":["Windows","Linux"],"title":"Windows 与 Linux 基础使用","uri":"/windows-shortcut/"},{"categories":["DevOps"],"content":"常用 Dos 命令 #正斜杠与反斜杠 / 和 \\ 前正后反 #切换盘符 dir #查看当前目录下的所有文件 cd #切换目录 cd change directory cd .. #返回上一级目录 cd /d E:\\a #/d参数用于跨盘符切换 cls #清理屏幕(clear screen) exit #关闭终端 ipconfig #查看ip信息 查看ip的详细信息 ipconfig \\all #打开应用 mspaint #打开画图工具 calc #打开计算器 notepad #打开记事本 #ping 命令 ping www.baidu.com #文件操作 md 目录名 #创建文件夹 cd\u003e 文件名 #创建文件 del 文件名 #删除文件 rd 目录名 #移除文件夹 ","date":"2020-07-05","objectID":"/windows-shortcut/:2:0","tags":["Windows","Linux"],"title":"Windows 与 Linux 基础使用","uri":"/windows-shortcut/"},{"categories":["DevOps"],"content":"常用 Linux 命令 ##### 显示和修改文本文件 ##### $ echo \"Hello World!\" \u003e hello-world.txt $ cat hello-world.txt Hello World! $ echo \"Go is the best!\" \u003e\u003e hello-world.txt $ cat hello-world.txt Hello World! Go is the best! ##### 搜索文件和在文件内搜索 ##### $ find /etc -name hosts /etc/hosts /etc/avahi/hosts $ find /etc -name \"hosts*\" /etc/hosts /etc/hosts.allow /etc/hosts.deny /etc/avahi/hosts ##### 管理进程 ##### $ ps aux # ps aux命令列出当前正在运行的进程，并显示他们的PID。 # 比如查询 docker $ ps aux | grep docker root 115 0.0 0.0 4997956 168 ?? Ss 13Sep22 0:00.57 /Library/PrivilegedHelperTools/com.docker.vmnetd # 向进程发送信号 kill -signal pid # 通过kill -l可以看到所有的信号变量 $ kill -l # 常用的几个信号, TERM(请求优雅的终止)、KILL(强制杀死)。 $ kill -TERM pid # 命令后台运行 命令后面跟 \u0026 $ ping www.google.com \u0026 # 显示在后台运行的进程列表 $ jobs # 将恢复一个工作到前台 $ fg %job-number # job-number(对于前台) # 将一个正在前台执行的命令放到后台,暂停流程并恢复对命令行的控制 Control+Z组合键 # 重新启动该流程在后台 $ bg %job-number # (用于后台)。 ##### 管理权限 ###### # 三类 users: u:user g:group o:other 三种权限：r:read w:write x:execute 八进制表示:421 # changes the owner of the file $ chown user file # alters the owner group $ chgrp group file # changes the permissions for the file $ chmod rights file # example $ chmod 755 hello-world.txt ##### 获取系统信息和日志 ##### # 显示内存信息 $ free # 报告可用的文件系统中挂载的每个磁盘上的磁盘空间 $ df -h # df means disk free -h human readable # 显示运行会话的用户的身份，以及他们所属的组的列表 # 由于对某些文件或设备的访问可能只限于小组成员，检查可用的小组成员资格可能很有用 $ id # 返回一行记录内核名称(Linux)、主机名、内核版本、内核版本、机器类型(一个体系结构字符串，如x86_64)、以及操作系统的名称(GNU/Linux) # 此命令的输出通常应该包含在错误报告中，因为它清楚地定义了使用的内核和硬件平台上运行。 $ uname -a # 检索内核日志 # 通常你需要查阅日志来了解你的计算机上发生了什么。比如一个新的USB设备被插入。一个失败的硬盘操作，或者启动时的初始硬件检测 $ dmesg ##### 发现硬件 ##### # lists PCI devices $ lspci # lists USB devices $ lsusb # lists PCMCIA cards. $ lspcmcia ","date":"2020-07-05","objectID":"/windows-shortcut/:3:0","tags":["Windows","Linux"],"title":"Windows 与 Linux 基础使用","uri":"/windows-shortcut/"},{"categories":["Markdown"],"content":"这篇文章展示了基本的 Markdown 语法和格式.","date":"2019-12-01","objectID":"/basic-markdown-syntax/","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"这篇文章提供了可以在 Hugo 的文章中使用的基本 Markdown 语法示例. 注意 这篇文章借鉴了一篇很棒的来自 Grav 的文章. 如果你想了解 Loveit 主题的扩展 Markdown 语法, 请阅读扩展 Markdown 语法页面. 事实上, 编写 Web 内容很麻烦. WYSIWYG所见即所得 编辑器帮助减轻了这一任务. 但通常会导致代码太糟, 或更糟糕的是, 网页也会很丑. 没有通常伴随的所有复杂和丑陋的问题, Markdown 是一种更好的生成 HTML 内容的方式. 一些主要好处是: Markdown 简单易学, 几乎没有多余的字符, 因此编写内容也更快. 用 Markdown 书写时出错的机会更少. 可以产生有效的 XHTML 输出. 将内容和视觉显示保持分开, 这样就不会打乱网站的外观. 可以在你喜欢的任何文本编辑器或 Markdown 应用程序中编写内容. Markdown 使用起来很有趣! John Gruber, Markdown 的作者如是说: Markdown 格式的首要设计目标是更具可读性. 最初的想法是 Markdown 格式的文档应当以纯文本形式发布, 而不会看起来像被标签或格式说明所标记. 虽然 Markdown 的语法受到几种现有的文本到 HTML 转换工具的影响, 但 Markdown 语法的最大灵感来源是纯文本电子邮件的格式. – John Gruber 话不多说, 我们来回顾一下 Markdown 的主要语法以及生成的 HTML 样式! 技巧  将此页保存为书签，以备将来参考! ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:0:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"1 标题 从 h2 到 h6 的标题在每个级别上都加上一个 ＃: ## h2 标题 ### h3 标题 #### h4 标题 ##### h5 标题 ###### h6 标题 输出的 HTML 看起来像这样: \u003ch2\u003eh2 标题\u003c/h2\u003e \u003ch3\u003eh3 标题\u003c/h3\u003e \u003ch4\u003eh4 标题\u003c/h4\u003e \u003ch5\u003eh5 标题\u003c/h5\u003e \u003ch6\u003eh6 标题\u003c/h6\u003e 标题 ID 要添加自定义标题 ID, 请在与标题相同的行中将自定义 ID 放在花括号中: ### 一个很棒的标题 {#custom-id} 输出的 HTML 看起来像这样: \u003ch3 id=\"custom-id\"\u003e一个很棒的标题\u003c/h3\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:1:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"2 注释 注释是和 HTML 兼容的： \u003c!-- 这是一段注释 --\u003e 不能看到以下的注释: ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:2:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"3 水平线 HTML 中的 \u003chr\u003e 标签是用来在段落元素之间创建一个 “专题间隔” 的. 使用 Markdown, 你可以用以下方式创建一个 \u003chr\u003e 标签: ___: 三个连续的下划线 ---: 三个连续的破折号 ***: 三个连续的星号 呈现的输出效果如下: ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:3:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"4 段落 按照纯文本的方式书写段落, 纯文本在呈现的 HTML 中将用 \u003cp\u003e/\u003c/p\u003e 标签包裹. 如下段落: Lorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad. 输出的 HTML 看起来像这样: \u003cp\u003eLorem ipsum dolor sit amet, graecis denique ei vel, at duo primis mandamus. Et legere ocurreret pri, animal tacimates complectitur ad cum. Cu eum inermis inimicus efficiendi. Labore officiis his ex, soluta officiis concludaturque ei qui, vide sensibus vim ad.\u003c/p\u003e 可以使用一个空白行进行换行. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:4:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"5 内联 HTML 元素 如果你需要某个 HTML 标签 (带有一个类), 则可以简单地像这样使用: Markdown 格式的段落. \u003cdiv class=\"class\"\u003e 这是 \u003cb\u003eHTML\u003c/b\u003e \u003c/div\u003e Markdown 格式的段落. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:5:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"6 强调 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"加粗 用于强调带有较粗字体的文本片段. 以下文本片段会被 渲染为粗体. **渲染为粗体** __渲染为粗体__ 输出的 HTML 看起来像这样: \u003cstrong\u003e渲染为粗体\u003c/strong\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"斜体 用于强调带有斜体的文本片段. 以下文本片段被 渲染为斜体. *渲染为斜体* _渲染为斜体_ 输出的 HTML 看起来像这样: \u003cem\u003e渲染为斜体\u003c/em\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"删除线 按照 GFMGitHub flavored Markdown 你可以使用删除线. ~~这段文本带有删除线.~~ 呈现的输出效果如下: 这段文本带有删除线. 输出的 HTML 看起来像这样: \u003cdel\u003e这段文本带有删除线.\u003c/del\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"组合 加粗, 斜体, 和删除线可以 组合使用. ***加粗和斜体*** ~~**删除线和加粗**~~ ~~*删除线和斜体*~~ ~~***加粗, 斜体和删除线***~~ 呈现的输出效果如下: 加粗和斜体 删除线和加粗 删除线和斜体 加粗, 斜体和删除线 输出的 HTML 看起来像这样: \u003cem\u003e\u003cstrong\u003e加粗和斜体\u003c/strong\u003e\u003c/em\u003e \u003cdel\u003e\u003cstrong\u003e删除线和加粗\u003c/strong\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003e删除线和斜体\u003c/em\u003e\u003c/del\u003e \u003cdel\u003e\u003cem\u003e\u003cstrong\u003e加粗, 斜体和删除线\u003c/strong\u003e\u003c/em\u003e\u003c/del\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:6:4","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"7 引用 用于在文档中引用其他来源的内容块. 在要引用的任何文本之前添加 \u003e: \u003e **Fusion Drive** combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. 呈现的输出效果如下: Fusion Drive combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. 输出的 HTML 看起来像这样: \u003cblockquote\u003e \u003cp\u003e \u003cstrong\u003eFusion Drive\u003c/strong\u003e combines a hard drive with a flash storage (solid-state drive) and presents it as a single logical volume with the space of both drives combined. \u003c/p\u003e \u003c/blockquote\u003e 引用也可以嵌套: \u003e Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. \u003e\u003e Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. 呈现的输出效果如下: Donec massa lacus, ultricies a ullamcorper in, fermentum sed augue. Nunc augue augue, aliquam non hendrerit ac, commodo vel nisi. Sed adipiscing elit vitae augue consectetur a gravida nunc vehicula. Donec auctor odio non est accumsan facilisis. Aliquam id turpis in dolor tincidunt mollis ac eu diam. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:7:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"8 列表 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"无序列表 一系列项的列表, 其中项的顺序没有明显关系. 你可以使用以下任何符号来表示无序列表中的项: * 一项内容 - 一项内容 + 一项内容 例如: * Lorem ipsum dolor sit amet * Consectetur adipiscing elit * Integer molestie lorem at massa * Facilisis in pretium nisl aliquet * Nulla volutpat aliquam velit * Phasellus iaculis neque * Purus sodales ultricies * Vestibulum laoreet porttitor sem * Ac tristique libero volutpat at * Faucibus porta lacus fringilla vel * Aenean sit amet erat nunc * Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Phasellus iaculis neque Purus sodales ultricies Vestibulum laoreet porttitor sem Ac tristique libero volutpat at Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem 输出的 HTML 看起来像这样: \u003cul\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit \u003cul\u003e \u003cli\u003ePhasellus iaculis neque\u003c/li\u003e \u003cli\u003ePurus sodales ultricies\u003c/li\u003e \u003cli\u003eVestibulum laoreet porttitor sem\u003c/li\u003e \u003cli\u003eAc tristique libero volutpat at\u003c/li\u003e \u003c/ul\u003e \u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ul\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"有序列表 一系列项的列表, 其中项的顺序确实很重要. 1. Lorem ipsum dolor sit amet 2. Consectetur adipiscing elit 3. Integer molestie lorem at massa 4. Facilisis in pretium nisl aliquet 5. Nulla volutpat aliquam velit 6. Faucibus porta lacus fringilla vel 7. Aenean sit amet erat nunc 8. Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem 输出的 HTML 看起来像这样: \u003col\u003e \u003cli\u003eLorem ipsum dolor sit amet\u003c/li\u003e \u003cli\u003eConsectetur adipiscing elit\u003c/li\u003e \u003cli\u003eInteger molestie lorem at massa\u003c/li\u003e \u003cli\u003eFacilisis in pretium nisl aliquet\u003c/li\u003e \u003cli\u003eNulla volutpat aliquam velit\u003c/li\u003e \u003cli\u003eFaucibus porta lacus fringilla vel\u003c/li\u003e \u003cli\u003eAenean sit amet erat nunc\u003c/li\u003e \u003cli\u003eEget porttitor lorem\u003c/li\u003e \u003c/ol\u003e 技巧 如果你对每一项使用 1., Markdown 将自动为每一项编号. 例如: 1. Lorem ipsum dolor sit amet 1. Consectetur adipiscing elit 1. Integer molestie lorem at massa 1. Facilisis in pretium nisl aliquet 1. Nulla volutpat aliquam velit 1. Faucibus porta lacus fringilla vel 1. Aenean sit amet erat nunc 1. Eget porttitor lorem 呈现的输出效果如下: Lorem ipsum dolor sit amet Consectetur adipiscing elit Integer molestie lorem at massa Facilisis in pretium nisl aliquet Nulla volutpat aliquam velit Faucibus porta lacus fringilla vel Aenean sit amet erat nunc Eget porttitor lorem ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"任务列表 任务列表使你可以创建带有复选框的列表. 要创建任务列表, 请在任务列表项之前添加破折号 (-) 和带有空格的方括号 ([ ]). 要选择一个复选框，请在方括号之间添加 x ([x]). - [x] Write the press release - [ ] Update the website - [ ] Contact the media 呈现的输出效果如下: Write the press release Update the website Contact the media ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:8:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"9 代码 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"行内代码 用 ` 包装行内代码段. 在这个例子中, `\u003csection\u003e\u003c/section\u003e` 会被包裹成 **代码**. 呈现的输出效果如下: 在这个例子中, \u003csection\u003e\u003c/section\u003e 会被包裹成 代码. 输出的 HTML 看起来像这样: \u003cp\u003e 在这个例子中, \u003ccode\u003e\u0026lt;section\u0026gt;\u0026lt;/section\u0026gt;\u003c/code\u003e 会被包裹成 \u003cstrong\u003e代码\u003c/strong\u003e. \u003c/p\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"缩进代码 将几行代码缩进至少四个空格，例如: // Some comments line 1 of code line 2 of code line 3 of code 呈现的输出效果如下: // Some comments line 1 of code line 2 of code line 3 of code 输出的 HTML 看起来像这样: \u003cpre\u003e \u003ccode\u003e // Some comments line 1 of code line 2 of code line 3 of code \u003c/code\u003e \u003c/pre\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"围栏代码块 使用 “围栏” ``` 来生成一段带有语言属性的代码块. ```markdown Sample text here... ``` 输出的 HTML 看起来像这样: \u003cpre language-html\u003e \u003ccode\u003eSample text here...\u003c/code\u003e \u003c/pre\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"语法高亮 GFMGitHub Flavored Markdown 也支持语法高亮. 要激活它，只需在第一个代码 “围栏” 之后直接添加你要使用的语言的文件扩展名, ```js, 语法高亮显示将自动应用于渲染的 HTML 中. 例如, 在以下 JavaScript 代码中应用语法高亮: ```js grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; ``` 呈现的输出效果如下: grunt.initConfig({ assemble: { options: { assets: 'docs/assets', data: 'src/data/*.{json,yml}', helpers: 'src/custom-helpers.js', partials: ['src/partials/**/*.{hbs,md}'] }, pages: { options: { layout: 'default.hbs' }, files: { './': ['src/templates/pages/index.hbs'] } } } }; 注意 Hugo 文档中的 语法高亮页面 介绍了有关语法高亮的更多信息, 包括语法高亮的 shortcode. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:9:4","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"10 表格 通过在每个单元格之间添加竖线作为分隔线, 并在标题下添加一行破折号 (也由竖线分隔) 来创建表格. 注意, 竖线不需要垂直对齐. | Option | Description | | ------ | ----------- | | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | 呈现的输出效果如下: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. 输出的 HTML 看起来像这样: \u003ctable\u003e \u003cthead\u003e \u003ctr\u003e \u003cth\u003eOption\u003c/th\u003e \u003cth\u003eDescription\u003c/th\u003e \u003c/tr\u003e \u003c/thead\u003e \u003ctbody\u003e \u003ctr\u003e \u003ctd\u003edata\u003c/td\u003e \u003ctd\u003epath to data files to supply the data that will be passed into templates.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eengine\u003c/td\u003e \u003ctd\u003eengine to be used for processing templates. Handlebars is the default.\u003c/td\u003e \u003c/tr\u003e \u003ctr\u003e \u003ctd\u003eext\u003c/td\u003e \u003ctd\u003eextension to be used for dest files.\u003c/td\u003e \u003c/tr\u003e \u003c/tbody\u003e \u003c/table\u003e 文本右对齐或居中对齐 在任何标题下方的破折号右侧添加冒号将使该列的文本右对齐. 在任何标题下方的破折号两边添加冒号将使该列的对齐文本居中. | Option | Description | |:------:| -----------:| | data | path to data files to supply the data that will be passed into templates. | | engine | engine to be used for processing templates. Handlebars is the default. | | ext | extension to be used for dest files. | 呈现的输出效果如下: Option Description data path to data files to supply the data that will be passed into templates. engine engine to be used for processing templates. Handlebars is the default. ext extension to be used for dest files. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:10:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"11 链接 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"基本链接 \u003chttps://assemble.io\u003e \u003ccontact@revolunet.com\u003e [Assemble](https://assemble.io) 呈现的输出效果如下 (将鼠标悬停在链接上，没有提示): https://assemble.io contact@revolunet.com Assemble 输出的 HTML 看起来像这样: \u003ca href=\"https://assemble.io\"\u003ehttps://assemble.io\u003c/a\u003e \u003ca href=\"mailto:contact@revolunet.com\"\u003econtact@revolunet.com\u003c/a\u003e \u003ca href=\"https://assemble.io\"\u003eAssemble\u003c/a\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:1","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"添加一个标题 [Upstage](https://github.com/upstage/ \"Visit Upstage!\") 呈现的输出效果如下 (将鼠标悬停在链接上，会有一行提示): Upstage 输出的 HTML 看起来像这样: \u003ca href=\"https://github.com/upstage/\" title=\"Visit Upstage!\"\u003eUpstage\u003c/a\u003e ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:2","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"定位标记 定位标记使你可以跳至同一页面上的指定锚点. 例如, 每个章节: ## Table of Contents * [Chapter 1](#chapter-1) * [Chapter 2](#chapter-2) * [Chapter 3](#chapter-3) 将跳转到这些部分: ## Chapter 1 \u003ca id=\"chapter-1\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 2 \u003ca id=\"chapter-2\"\u003e\u003c/a\u003e Content for chapter one. ## Chapter 3 \u003ca id=\"chapter-3\"\u003e\u003c/a\u003e Content for chapter one. 注意 定位标记的位置几乎是任意的. 因为它们并不引人注目, 所以它们通常被放在同一行了. ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:11:3","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"12 脚注 脚注使你可以添加注释和参考, 而不会使文档正文混乱. 当你创建脚注时, 会在添加脚注引用的位置出现带有链接的上标编号. 读者可以单击链接以跳至页面底部的脚注内容. 要创建脚注引用, 请在方括号中添加插入符号和标识符 ([^1]). 标识符可以是数字或单词, 但不能包含空格或制表符. 标识符仅将脚注引用与脚注本身相关联 - 在脚注输出中, 脚注按顺序编号. 在中括号内使用插入符号和数字以及用冒号和文本来添加脚注内容 ([^1]：这是一段脚注). 你不一定要在文档末尾添加脚注. 可以将它们放在除列表, 引用和表格等元素之外的任何位置. 这是一个数字脚注[^1]. 这是一个带标签的脚注[^label] [^1]: 这是一个数字脚注 [^label]: 这是一个带标签的脚注 这是一个数字脚注1. 这是一个带标签的脚注2 ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:12:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"13 图片 图片的语法与链接相似, 但包含一个在前面的感叹号. ![Minion](https://octodex.github.com/images/minion.png) 或者: ![Alt text](https://octodex.github.com/images/stormtroopocat.jpg \"The Stormtroopocat\") The Stormtroopocat 像链接一样, 图片也具有脚注样式的语法: ![Alt text][id] The Dojocat 稍后在文档中提供参考内容, 用来定义 URL 的位置: [id]: https://octodex.github.com/images/dojocat.jpg \"The Dojocat\" 技巧 LoveIt 主题提供了一个包含更多功能的 图片的 shortcode. 这是一个数字脚注 ↩︎ 这是一个带标签的脚注 ↩︎ ","date":"2019-12-01","objectID":"/basic-markdown-syntax/:13:0","tags":["Markdown","HTML"],"title":"Markdown 基本语法","uri":"/basic-markdown-syntax/"},{"categories":["Markdown"],"content":"Hugo 和 LoveIt 中的 Emoji 的用法指南.","date":"2019-10-01","objectID":"/emoji-support/","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"Emoji 可以通过多种方式在 Hugo 项目中启用. emojify 方法可以直接在模板中调用, 或者使用行内 Shortcodes. 要全局使用 emoji, 需要在你的网站配置中设置 enableEmoji 为 true, 然后你就可以直接在文章中输入 emoji 的代码. 它们以冒号开头和结尾，并且包含 emoji 的 代码: 去露营啦! :tent: 很快就回来. 真开心! :joy: 呈现的输出效果如下: 去露营啦! ⛺ 很快就回来. 真开心! 😂 以下符号清单是 emoji 代码的非常有用的参考. ","date":"2019-10-01","objectID":"/emoji-support/:0:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"表情与情感 ","date":"2019-10-01","objectID":"/emoji-support/:1:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"笑脸表情 图标 代码 图标 代码 😀 grinning 😃 smiley 😄 smile 😁 grin 😆 laughing satisfied 😅 sweat_smile 🤣 rofl 😂 joy 🙂 slightly_smiling_face 🙃 upside_down_face 😉 wink 😊 blush 😇 innocent ","date":"2019-10-01","objectID":"/emoji-support/:1:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"爱意表情 图标 代码 图标 代码 😍 heart_eyes 😘 kissing_heart 😗 kissing ☺️ relaxed 😚 kissing_closed_eyes 😙 kissing_smiling_eyes ","date":"2019-10-01","objectID":"/emoji-support/:1:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"吐舌头表情 图标 代码 图标 代码 😋 yum 😛 stuck_out_tongue 😜 stuck_out_tongue_winking_eye 😝 stuck_out_tongue_closed_eyes 🤑 money_mouth_face ","date":"2019-10-01","objectID":"/emoji-support/:1:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"带手的表情 图标 代码 图标 代码 🤗 hugs 🤔 thinking ","date":"2019-10-01","objectID":"/emoji-support/:1:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"中性表情 图标 代码 图标 代码 🤐 zipper_mouth_face 😐 neutral_face 😑 expressionless 😶 no_mouth 😏 smirk 😒 unamused 🙄 roll_eyes 😬 grimacing 🤥 lying_face ","date":"2019-10-01","objectID":"/emoji-support/:1:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"困倦的表情 图标 代码 图标 代码 😌 relieved 😔 pensive 😪 sleepy 🤤 drooling_face 😴 sleeping ","date":"2019-10-01","objectID":"/emoji-support/:1:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"不适的表情 图标 代码 图标 代码 😷 mask 🤒 face_with_thermometer 🤕 face_with_head_bandage 🤢 nauseated_face 🤧 sneezing_face 😵 dizzy_face ","date":"2019-10-01","objectID":"/emoji-support/:1:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"戴帽子的表情 图标 代码 图标 代码 🤠 cowboy_hat_face ","date":"2019-10-01","objectID":"/emoji-support/:1:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"戴眼镜的表情 图标 代码 图标 代码 😎 sunglasses 🤓 nerd_face ","date":"2019-10-01","objectID":"/emoji-support/:1:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"担心的表情 图标 代码 图标 代码 😕 confused 😟 worried 🙁 slightly_frowning_face ☹ frowning_face 😮 open_mouth 😯 hushed 😲 astonished 😳 flushed 😦 frowning 😧 anguished 😨 fearful 😰 cold_sweat 😥 disappointed_relieved 😢 cry 😭 sob 😱 scream 😖 confounded 😣 persevere 😞 disappointed 😓 sweat 😩 weary 😫 tired_face ","date":"2019-10-01","objectID":"/emoji-support/:1:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"否定的表情 图标 代码 图标 代码 😤 triumph 😡 pout rage 😠 angry 😈 smiling_imp 👿 imp 💀 skull ☠️ skull_and_crossbones ","date":"2019-10-01","objectID":"/emoji-support/:1:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"特殊打扮的表情 图标 代码 图标 代码 💩 hankey poop shit 🤡 clown_face 👹 japanese_ogre 👺 japanese_goblin 👻 ghost 👽 alien 👾 space_invader 🤖 robot ","date":"2019-10-01","objectID":"/emoji-support/:1:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"猫脸表情 图标 代码 图标 代码 😺 smiley_cat 😸 smile_cat 😹 joy_cat 😻 heart_eyes_cat 😼 smirk_cat 😽 kissing_cat 🙀 scream_cat 😿 crying_cat_face 😾 pouting_cat ","date":"2019-10-01","objectID":"/emoji-support/:1:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"猴脸表情 图标 代码 图标 代码 🙈 see_no_evil 🙉 hear_no_evil 🙊 speak_no_evil ","date":"2019-10-01","objectID":"/emoji-support/:1:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"情感 图标 代码 图标 代码 💋 kiss 💌 love_letter 💘 cupid 💝 gift_heart 💖 sparkling_heart 💗 heartpulse 💓 heartbeat 💞 revolving_hearts 💕 two_hearts 💟 heart_decoration ❣️ heavy_heart_exclamation 💔 broken_heart ❤️ heart 💛 yellow_heart 💚 green_heart 💙 blue_heart 💜 purple_heart 🖤 black_heart 💯 100 💢 anger 💥 boom collision 💫 dizzy 💦 sweat_drops 💨 dash 🕳️ hole 💣 bomb 💬 speech_balloon 👁️‍🗨️ eye_speech_bubble 🗯️ right_anger_bubble 💭 thought_balloon 💤 zzz ","date":"2019-10-01","objectID":"/emoji-support/:1:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人与身体 ","date":"2019-10-01","objectID":"/emoji-support/:2:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"张开手掌的手势 图标 代码 图标 代码 👋 wave 🤚 raised_back_of_hand 🖐️ raised_hand_with_fingers_splayed ✋ hand raised_hand 🖖 vulcan_salute ","date":"2019-10-01","objectID":"/emoji-support/:2:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"部分手指的手势 图标 代码 图标 代码 👌 ok_hand ✌️ v 🤞 crossed_fingers 🤘 metal 🤙 call_me_hand ","date":"2019-10-01","objectID":"/emoji-support/:2:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"一根手指的手势 图标 代码 图标 代码 👈 point_left 👉 point_right 👆 point_up_2 🖕 fu middle_finger 👇 point_down ☝️ point_up ","date":"2019-10-01","objectID":"/emoji-support/:2:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"握紧的手势 图标 代码 图标 代码 👍 +1 thumbsup 👎 -1 thumbsdown ✊ fist fist_raised 👊 facepunch fist_oncoming punch 🤛 fist_left 🤜 fist_right ","date":"2019-10-01","objectID":"/emoji-support/:2:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"两只手 图标 代码 图标 代码 👏 clap 🙌 raised_hands 👐 open_hands 🤝 handshake 🙏 pray ","date":"2019-10-01","objectID":"/emoji-support/:2:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"握住东西的手势 图标 代码 图标 代码 ✍️ writing_hand 💅 nail_care 🤳 selfie ","date":"2019-10-01","objectID":"/emoji-support/:2:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"身体部位 图标 代码 图标 代码 💪 muscle 👂 ear 👃 nose 👀 eyes 👁️ eye 👅 tongue 👄 lips ","date":"2019-10-01","objectID":"/emoji-support/:2:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人 图标 代码 图标 代码 👶 baby 👦 boy 👧 girl :blonde_man: blonde_man person_with_blond_hair 👨 man 👩 woman 👱‍♀️ blonde_woman 👴 older_man 👵 older_woman ","date":"2019-10-01","objectID":"/emoji-support/:2:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"身体动作 图标 代码 图标 代码 🙍‍♀️ frowning_woman person_frowning 🙍‍♂️ frowning_man 🙎‍♀️ person_with_pouting_face pouting_woman 🙎‍♂️ pouting_man 🙅‍♀️ ng_woman no_good no_good_woman 🙅‍♂️ ng_man no_good_man 🙆‍♀️ ok_woman 🙆‍♂️ ok_man 💁‍♀️ information_desk_person sassy_woman tipping_hand_woman 💁‍♂️ sassy_man tipping_hand_man 🙋‍♀️ raising_hand raising_hand_woman 🙋‍♂️ raising_hand_man 🙇 bow bowing_man 🙇‍♀️ bowing_woman 🤦‍♂️ man_facepalming 🤦‍♀️ woman_facepalming 🤷‍♂️ man_shrugging 🤷‍♀️ woman_shrugging ","date":"2019-10-01","objectID":"/emoji-support/:2:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物角色 图标 代码 图标 代码 👨‍⚕️ man_health_worker 👩‍⚕️ woman_health_worker 👨‍🎓 man_student 👩‍🎓 woman_student 👨‍🏫 man_teacher 👩‍🏫 woman_teacher 👨‍⚖️ man_judge 👩‍⚖️ woman_judge 👨‍🌾 man_farmer 👩‍🌾 woman_farmer 👨‍🍳 man_cook 👩‍🍳 woman_cook 👨‍🔧 man_mechanic 👩‍🔧 woman_mechanic 👨‍🏭 man_factory_worker 👩‍🏭 woman_factory_worker 👨‍💼 man_office_worker 👩‍💼 woman_office_worker 👨‍🔬 man_scientist 👩‍🔬 woman_scientist 👨‍💻 man_technologist 👩‍💻 woman_technologist 👨‍🎤 man_singer 👩‍🎤 woman_singer 👨‍🎨 man_artist 👩‍🎨 woman_artist 👨‍✈️ man_pilot 👩‍✈️ woman_pilot 👨‍🚀 man_astronaut 👩‍🚀 woman_astronaut 👨‍🚒 man_firefighter 👩‍🚒 woman_firefighter 👮‍♂️ cop policeman 👮‍♀️ policewoman 🕵 detective male_detective 🕵️‍♀️ female_detective 💂‍♂️ guardsman 💂‍♀️ guardswoman 👷‍♂️ construction_worker construction_worker_man 👷‍♀️ construction_worker_woman 🤴 prince 👸 princess 👳‍♂️ man_with_turban 👳‍♀️ woman_with_turban 👲 man_with_gua_pi_mao 🤵‍♂️ man_in_tuxedo 👰 bride_with_veil 🤰 pregnant_woman ","date":"2019-10-01","objectID":"/emoji-support/:2:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"幻想的人物 图标 代码 图标 代码 👼 angel 🎅 santa 🤶 mrs_claus ","date":"2019-10-01","objectID":"/emoji-support/:2:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物活动 图标 代码 图标 代码 💆‍♀️ massage massage_woman 💆‍♂️ massage_man 💇‍♀️ haircut haircut_woman 💇‍♂️ haircut_man 🚶‍♂️ walking walking_man 🚶‍♀️ walking_woman 🏃‍♂️ runner running running_man 🏃‍♀️ running_woman 💃 dancer 🕺 man_dancing 🕴️ business_suit_levitating 👯‍♀️ dancers dancing_women 👯‍♂️ dancing_men ","date":"2019-10-01","objectID":"/emoji-support/:2:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"体育 图标 代码 图标 代码 🤺 person_fencing 🏇 horse_racing ⛷️ skier 🏂 snowboarder 🏌️‍♂️ golfing_man 🏌️‍♀️ golfing_woman 🏄‍♂️ surfer surfing_man 🏄‍♀️ surfing_woman 🚣‍♂️ rowboat rowing_man 🚣‍♀️ rowing_woman 🏊‍♂️ swimmer swimming_man 🏊‍♀️ swimming_woman ⛹️‍♂️ basketball_man ⛹️‍♀️ basketball_woman 🏋️‍♂️ weight_lifting_man 🏋️‍♀️ weight_lifting_woman 🚴‍♂️ bicyclist biking_man 🚴‍♀️ biking_woman 🚵‍♂️ mountain_bicyclist mountain_biking_man 🚵‍♀️ mountain_biking_woman 🤸‍♂️ man_cartwheeling 🤸‍♀️ woman_cartwheeling 🤼‍♂️ men_wrestling 🤼‍♀️ women_wrestling 🤽‍♂️ man_playing_water_polo 🤽‍♀️ woman_playing_water_polo 🤾‍♂️ man_playing_handball 🤾‍♀️ woman_playing_handball 🤹‍♂️ man_juggling 🤹‍♀️ woman_juggling ","date":"2019-10-01","objectID":"/emoji-support/:2:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"休息 图标 代码 图标 代码 🛀 bath 🛌 sleeping_bed ","date":"2019-10-01","objectID":"/emoji-support/:2:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"家庭 图标 代码 图标 代码 👭 two_women_holding_hands 👫 couple 👬 two_men_holding_hands 👩‍❤️‍💋‍👨 couplekiss_man_woman 👨‍❤️‍💋‍👨 couplekiss_man_man 👩‍❤️‍💋‍👩 couplekiss_woman_woman 💑 couple_with_heart couple_with_heart_woman_man 👨‍❤️‍👨 couple_with_heart_man_man 👩‍❤️‍👩 couple_with_heart_woman_woman 👨‍👩‍👦 family family_man_woman_boy 👨‍👩‍👧 family_man_woman_girl 👨‍👩‍👧‍👦 family_man_woman_girl_boy 👨‍👩‍👦‍👦 family_man_woman_boy_boy 👨‍👩‍👧‍👧 family_man_woman_girl_girl 👨‍👨‍👦 family_man_man_boy 👨‍👨‍👧 family_man_man_girl 👨‍👨‍👧‍👦 family_man_man_girl_boy 👨‍👨‍👦‍👦 family_man_man_boy_boy 👨‍👨‍👧‍👧 family_man_man_girl_girl 👩‍👩‍👦 family_woman_woman_boy 👩‍👩‍👧 family_woman_woman_girl 👩‍👩‍👧‍👦 family_woman_woman_girl_boy 👩‍👩‍👦‍👦 family_woman_woman_boy_boy 👩‍👩‍👧‍👧 family_woman_woman_girl_girl 👨‍👦 family_man_boy 👨‍👦‍👦 family_man_boy_boy 👨‍👧 family_man_girl 👨‍👧‍👦 family_man_girl_boy 👨‍👧‍👧 family_man_girl_girl 👩‍👦 family_woman_boy 👩‍👦‍👦 family_woman_boy_boy 👩‍👧 family_woman_girl 👩‍👧‍👦 family_woman_girl_boy 👩‍👧‍👧 family_woman_girl_girl ","date":"2019-10-01","objectID":"/emoji-support/:2:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"人物符号 图标 代码 图标 代码 🗣 speaking_head 👤 bust_in_silhouette 👥 busts_in_silhouette 👣 footprints ","date":"2019-10-01","objectID":"/emoji-support/:2:16","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"动物与自然 ","date":"2019-10-01","objectID":"/emoji-support/:3:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"哺乳动物 图标 代码 图标 代码 🐵 monkey_face 🐒 monkey 🦍 gorilla 🐶 dog 🐕 dog2 🐩 poodle 🐺 wolf 🦊 fox_face 🐱 cat 🐈 cat2 🦁 lion 🐯 tiger 🐅 tiger2 🐆 leopard 🐴 horse 🐎 racehorse 🦄 unicorn 🦌 deer 🐮 cow 🐂 ox 🐃 water_buffalo 🐄 cow2 🐷 pig 🐖 pig2 🐗 boar 🐽 pig_nose 🐏 ram 🐑 sheep 🐐 goat 🐪 dromedary_camel 🐫 camel 🐘 elephant 🦏 rhinoceros 🐭 mouse 🐁 mouse2 🐀 rat 🐹 hamster 🐰 rabbit 🐇 rabbit2 🐿️ chipmunk 🦇 bat 🐻 bear 🐨 koala 🐼 panda_face 🐾 feet paw_prints ","date":"2019-10-01","objectID":"/emoji-support/:3:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"鸟类 图标 代码 图标 代码 🦃 turkey 🐔 chicken 🐓 rooster 🐣 hatching_chick 🐤 baby_chick 🐥 hatched_chick 🐦 bird 🐧 penguin 🕊 dove 🦅 eagle 🦆 duck 🦉 owl ","date":"2019-10-01","objectID":"/emoji-support/:3:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"两栖动物 icon code icon code 🐸 frog ","date":"2019-10-01","objectID":"/emoji-support/:3:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"爬虫类 图标 代码 图标 代码 🐊 crocodile 🐢 turtle 🦎 lizard 🐍 snake 🐲 dragon_face 🐉 dragon ","date":"2019-10-01","objectID":"/emoji-support/:3:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"海洋动物 图标 代码 图标 代码 🐳 whale 🐋 whale2 🐬 dolphin flipper 🐟 fish 🐠 tropical_fish 🐡 blowfish 🦈 shark 🐙 octopus 🐚 shell ","date":"2019-10-01","objectID":"/emoji-support/:3:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"虫类 图标 代码 图标 代码 🐌 snail 🦋 butterfly 🐛 bug 🐜 ant 🐝 bee honeybee 🪲 beetle 🕷️ spider 🕸️ spider_web 🦂 scorpion ","date":"2019-10-01","objectID":"/emoji-support/:3:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"花类植物 图标 代码 图标 代码 💐 bouquet 🌸 cherry_blossom 💮 white_flower 🏵️ rosette 🌹 rose 🥀 wilted_flower 🌺 hibiscus 🌻 sunflower 🌼 blossom 🌷 tulip ","date":"2019-10-01","objectID":"/emoji-support/:3:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它植物 图标 代码 图标 代码 🌱 seedling 🌲 evergreen_tree 🌳 deciduous_tree 🌴 palm_tree 🌵 cactus 🌾 ear_of_rice 🌿 herb ☘️ shamrock 🍀 four_leaf_clover 🍁 maple_leaf 🍂 fallen_leaf 🍃 leaves ","date":"2019-10-01","objectID":"/emoji-support/:3:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"食物与饮料 ","date":"2019-10-01","objectID":"/emoji-support/:4:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"水果 图标 代码 图标 代码 🍇 grapes 🍈 melon 🍉 watermelon 🍊 mandarin orange tangerine 🍋 lemon 🍌 banana 🍍 pineapple 🍎 apple 🍏 green_apple 🍐 pear 🍑 peach 🍒 cherries 🍓 strawberry 🥝 kiwi_fruit 🍅 tomato ","date":"2019-10-01","objectID":"/emoji-support/:4:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"蔬菜 图标 代码 图标 代码 🥑 avocado 🍆 eggplant 🥔 potato 🥕 carrot 🌽 corn 🌶️ hot_pepper 🥒 cucumber 🍄 mushroom 🥜 peanuts 🌰 chestnut ","date":"2019-10-01","objectID":"/emoji-support/:4:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"快餐 图标 代码 图标 代码 🍞 bread 🥐 croissant 🥖 baguette_bread 🥞 pancakes 🧀 cheese 🍖 meat_on_bone 🍗 poultry_leg 🥓 bacon 🍔 hamburger 🍟 fries 🍕 pizza 🌭 hotdog 🌮 taco 🌯 burrito 🥙 stuffed_flatbread 🥚 egg 🍳 fried_egg 🥘 shallow_pan_of_food 🍲 stew 🥗 green_salad 🍿 popcorn ","date":"2019-10-01","objectID":"/emoji-support/:4:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"亚洲食物 图标 代码 图标 代码 🍱 bento 🍘 rice_cracker 🍙 rice_ball 🍚 rice 🍛 curry 🍜 ramen 🍝 spaghetti 🍠 sweet_potato 🍢 oden 🍣 sushi 🍤 fried_shrimp 🍥 fish_cake 🍡 dango ","date":"2019-10-01","objectID":"/emoji-support/:4:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"海鲜 图标 代码 图标 代码 🦀 crab 🦐 shrimp 🦑 squid ","date":"2019-10-01","objectID":"/emoji-support/:4:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"甜点 图标 代码 图标 代码 🍦 icecream 🍧 shaved_ice 🍨 ice_cream 🍩 doughnut 🍪 cookie 🎂 birthday 🍰 cake 🍫 chocolate_bar 🍬 candy 🍭 lollipop 🍮 custard 🍯 honey_pot ","date":"2019-10-01","objectID":"/emoji-support/:4:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"饮料 图标 代码 图标 代码 🍼 baby_bottle 🥛 milk_glass ☕ coffee 🍵 tea 🍶 sake 🍾 champagne 🍷 wine_glass 🍸 cocktail 🍹 tropical_drink 🍺 beer 🍻 beers 🥂 clinking_glasses 🥃 tumbler_glass ","date":"2019-10-01","objectID":"/emoji-support/:4:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"餐具 图标 代码 图标 代码 🍽️ plate_with_cutlery 🍴 fork_and_knife 🥄 spoon 🔪 hocho knife 🏺 amphora ","date":"2019-10-01","objectID":"/emoji-support/:4:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旅游与地理 ","date":"2019-10-01","objectID":"/emoji-support/:5:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"地图 图标 代码 图标 代码 🌍 earth_africa 🌎 earth_americas 🌏 earth_asia 🌐 globe_with_meridians 🗺️ world_map 🗾 japan ","date":"2019-10-01","objectID":"/emoji-support/:5:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"地理现象 图标 代码 图标 代码 🏔 mountain_snow ⛰️ mountain 🌋 volcano 🗻 mount_fuji 🏕️ camping ⛱ beach_umbrella 🏜️ desert 🏝️ desert_island 🏞️ national_park ","date":"2019-10-01","objectID":"/emoji-support/:5:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"建筑物 图标 代码 图标 代码 🏟️ stadium 🏛️ classical_building 🏗️ building_construction 🏘 houses 🏚 derelict_house 🏠 house 🏡 house_with_garden 🏢 office 🏣 post_office 🏤 european_post_office 🏥 hospital 🏦 bank 🏨 hotel 🏩 love_hotel 🏪 convenience_store 🏫 school 🏬 department_store 🏭 factory 🏯 japanese_castle 🏰 european_castle 💒 wedding 🗼 tokyo_tower 🗽 statue_of_liberty ","date":"2019-10-01","objectID":"/emoji-support/:5:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"宗教建筑 图标 代码 图标 代码 ⛪ church 🕌 mosque 🕍 synagogue ⛩️ shinto_shrine 🕋 kaaba ","date":"2019-10-01","objectID":"/emoji-support/:5:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它地点 图标 代码 图标 代码 ⛲ fountain ⛺ tent 🌁 foggy 🌃 night_with_stars 🏙️ cityscape 🌄 sunrise_over_mountains 🌅 sunrise 🌆 city_sunset 🌇 city_sunrise 🌉 bridge_at_night ♨️ hotsprings 🎠 carousel_horse 🎡 ferris_wheel 🎢 roller_coaster 💈 barber 🎪 circus_tent ","date":"2019-10-01","objectID":"/emoji-support/:5:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"陆路运输 图标 代码 图标 代码 🚂 steam_locomotive 🚃 railway_car 🚄 bullettrain_side 🚅 bullettrain_front 🚆 train2 🚇 metro 🚈 light_rail 🚉 station 🚊 tram 🚝 monorail 🚞 mountain_railway 🚋 train 🚌 bus 🚍 oncoming_bus 🚎 trolleybus 🚐 minibus 🚑 ambulance 🚒 fire_engine 🚓 police_car 🚔 oncoming_police_car 🚕 taxi 🚖 oncoming_taxi 🚗 car red_car 🚘 oncoming_automobile 🚙 blue_car 🚚 truck 🚛 articulated_lorry 🚜 tractor 🏎️ racing_car 🏍 motorcycle 🛵 motor_scooter 🚲 bike 🛴 kick_scooter 🚏 busstop 🛣️ motorway 🛤️ railway_track 🛢️ oil_drum ⛽ fuelpump 🚨 rotating_light 🚥 traffic_light 🚦 vertical_traffic_light 🛑 stop_sign 🚧 construction ","date":"2019-10-01","objectID":"/emoji-support/:5:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"水路运输 图标 代码 图标 代码 ⚓ anchor ⛵ boat sailboat 🛶 canoe 🚤 speedboat 🛳️ passenger_ship ⛴️ ferry 🛥️ motor_boat 🚢 ship ","date":"2019-10-01","objectID":"/emoji-support/:5:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"空中运输 图标 代码 图标 代码 ✈️ airplane 🛩️ small_airplane 🛫 flight_departure 🛬 flight_arrival 💺 seat 🚁 helicopter 🚟 suspension_railway 🚠 mountain_cableway 🚡 aerial_tramway 🛰️ artificial_satellite 🚀 rocket ","date":"2019-10-01","objectID":"/emoji-support/:5:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旅馆 icon code icon code 🛎️ bellhop_bell ","date":"2019-10-01","objectID":"/emoji-support/:5:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"时间 图标 代码 图标 代码 ⌛ hourglass ⏳ hourglass_flowing_sand ⌚ watch ⏰ alarm_clock ⏱️ stopwatch ⏲️ timer_clock 🕰️ mantelpiece_clock 🕛 clock12 🕧 clock1230 🕐 clock1 🕜 clock130 🕑 clock2 🕝 clock230 🕒 clock3 🕞 clock330 🕓 clock4 🕟 clock430 🕔 clock5 🕠 clock530 🕕 clock6 🕡 clock630 🕖 clock7 🕢 clock730 🕗 clock8 🕣 clock830 🕘 clock9 🕤 clock930 🕙 clock10 🕥 clock1030 🕚 clock11 🕦 clock1130 ","date":"2019-10-01","objectID":"/emoji-support/:5:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"天空与天气 图标 代码 图标 代码 🌑 new_moon 🌒 waxing_crescent_moon 🌓 first_quarter_moon 🌔 moon waxing_gibbous_moon 🌕 full_moon 🌖 waning_gibbous_moon 🌗 last_quarter_moon 🌘 waning_crescent_moon 🌙 crescent_moon 🌚 new_moon_with_face 🌛 first_quarter_moon_with_face 🌜 last_quarter_moon_with_face 🌡️ thermometer ☀️ sunny 🌝 full_moon_with_face 🌞 sun_with_face ⭐ star 🌟 star2 🌠 stars 🌌 milky_way ☁️ cloud ⛅ partly_sunny ⛈ cloud_with_lightning_and_rain 🌤 sun_behind_small_cloud 🌥 sun_behind_large_cloud 🌦 sun_behind_rain_cloud 🌧 cloud_with_rain 🌨 cloud_with_snow 🌩 cloud_with_lightning 🌪️ tornado 🌫️ fog 🌬 wind_face 🌀 cyclone 🌈 rainbow 🌂 closed_umbrella ☂️ open_umbrella ☂️ umbrella ⛱️ parasol_on_ground ⚡ zap ❄️ snowflake ☃️ snowman_with_snow ☃️ snowman ☄️ comet 🔥 fire 💧 droplet 🌊 ocean ","date":"2019-10-01","objectID":"/emoji-support/:5:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"活动 ","date":"2019-10-01","objectID":"/emoji-support/:6:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"事件 图标 代码 图标 代码 🎃 jack_o_lantern 🎄 christmas_tree 🎆 fireworks 🎇 sparkler ✨ sparkles 🎈 balloon 🎉 tada 🎊 confetti_ball 🎋 tanabata_tree 🎍 bamboo 🎎 dolls 🎏 flags 🎐 wind_chime 🎑 rice_scene 🎀 ribbon 🎁 gift 🎗️ reminder_ribbon 🎟 tickets 🎫 ticket ","date":"2019-10-01","objectID":"/emoji-support/:6:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"奖杯与奖牌 图标 代码 图标 代码 🎖️ medal_military 🏆 trophy 🏅 medal_sports 🥇 1st_place_medal 🥈 2nd_place_medal 🥉 3rd_place_medal ","date":"2019-10-01","objectID":"/emoji-support/:6:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"体育运动 图标 代码 图标 代码 ⚽ soccer ⚾ baseball 🏀 basketball 🏐 volleyball 🏈 football 🏉 rugby_football 🎾 tennis 🎳 bowling 🦗 cricket 🏑 field_hockey 🏒 ice_hockey 🏓 ping_pong 🏸 badminton 🥊 boxing_glove 🥋 martial_arts_uniform 🥅 goal_net ⛳ golf ⛸️ ice_skate 🎣 fishing_pole_and_fish 🎽 running_shirt_with_sash 🎿 ski ","date":"2019-10-01","objectID":"/emoji-support/:6:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"游戏 图标 代码 图标 代码 🎯 dart 🎱 8ball 🔮 crystal_ball 🎮 video_game 🕹️ joystick 🎰 slot_machine 🎲 game_die ♠️ spades ♥️ hearts ♦️ diamonds ♣️ clubs 🃏 black_joker 🀄 mahjong 🎴 flower_playing_cards ","date":"2019-10-01","objectID":"/emoji-support/:6:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"艺术与工艺 图标 代码 图标 代码 🎭 performing_arts 🖼 framed_picture 🎨 art ","date":"2019-10-01","objectID":"/emoji-support/:6:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"物品 ","date":"2019-10-01","objectID":"/emoji-support/:7:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"服装 图标 代码 图标 代码 👓 eyeglasses 🕶️ dark_sunglasses 👔 necktie 👕 shirt tshirt 👖 jeans 👗 dress 👘 kimono 👙 bikini 👚 womans_clothes 👛 purse 👜 handbag 👝 pouch 🛍️ shopping 🎒 school_satchel 👞 mans_shoe shoe 👟 athletic_shoe 👠 high_heel 👡 sandal 👢 boot 👑 crown 👒 womans_hat 🎩 tophat 🎓 mortar_board ⛑️ rescue_worker_helmet 📿 prayer_beads 💄 lipstick 💍 ring 💎 gem ","date":"2019-10-01","objectID":"/emoji-support/:7:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"声音 图标 代码 图标 代码 🔇 mute 🔈 speaker 🔉 sound 🔊 loud_sound 📢 loudspeaker 📣 mega 📯 postal_horn 🔔 bell 🔕 no_bell ","date":"2019-10-01","objectID":"/emoji-support/:7:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"音乐 图标 代码 图标 代码 🎼 musical_score 🎵 musical_note 🎶 notes 🎙️ studio_microphone 🎚️ level_slider 🎛️ control_knobs 🎤 microphone 🎧 headphones 📻 radio ","date":"2019-10-01","objectID":"/emoji-support/:7:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"乐器 图标 代码 图标 代码 🎷 saxophone 🎸 guitar 🎹 musical_keyboard 🎺 trumpet 🎻 violin 🥁 drum ","date":"2019-10-01","objectID":"/emoji-support/:7:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"电话 图标 代码 图标 代码 📱 iphone 📲 calling ☎️ phone telephone 📞 telephone_receiver 📟 pager 📠 fax ","date":"2019-10-01","objectID":"/emoji-support/:7:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"电脑 图标 代码 图标 代码 🔋 battery 🔌 electric_plug 💻 computer 🖥️ desktop_computer 🖨️ printer ⌨️ keyboard 🖱 computer_mouse 🖲️ trackball 💽 minidisc 💾 floppy_disk 💿 cd 📀 dvd ","date":"2019-10-01","objectID":"/emoji-support/:7:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"灯光与影像 图标 代码 图标 代码 🎥 movie_camera 🎞️ film_strip 📽️ film_projector 🎬 clapper 📺 tv 📷 camera 📸 camera_flash 📹 video_camera 📼 vhs 🔍 mag 🔎 mag_right 🕯️ candle 💡 bulb 🔦 flashlight 🏮 izakaya_lantern lantern ","date":"2019-10-01","objectID":"/emoji-support/:7:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"书与纸张 图标 代码 图标 代码 📔 notebook_with_decorative_cover 📕 closed_book 📖 book open_book 📗 green_book 📘 blue_book 📙 orange_book 📚 books 📓 notebook 📒 ledger 📃 page_with_curl 📜 scroll 📄 page_facing_up 📰 newspaper 🗞️ newspaper_roll 📑 bookmark_tabs 🔖 bookmark 🏷️ label ","date":"2019-10-01","objectID":"/emoji-support/:7:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"钱 图标 代码 图标 代码 💰 moneybag 💴 yen 💵 dollar 💶 euro 💷 pound 💸 money_with_wings 💳 credit_card 💹 chart ","date":"2019-10-01","objectID":"/emoji-support/:7:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"邮件 图标 代码 图标 代码 ✉️ email envelope 📧 📧 📨 incoming_envelope 📩 envelope_with_arrow 📤 outbox_tray 📥 inbox_tray 📦 package 📫 mailbox 📪 mailbox_closed 📬 mailbox_with_mail 📭 mailbox_with_no_mail 📮 postbox 🗳 ballot_box ","date":"2019-10-01","objectID":"/emoji-support/:7:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"书写 图标 代码 图标 代码 ✏️ pencil2 ✒️ black_nib 🖋 fountain_pen 🖊 pen 🖌 paintbrush 🖍 crayon 📝 memo pencil ","date":"2019-10-01","objectID":"/emoji-support/:7:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"办公 图标 代码 图标 代码 💼 briefcase 📁 file_folder 📂 open_file_folder 🗂️ card_index_dividers 📅 date 📆 calendar 🗒 spiral_notepad 🗓 spiral_calendar 📇 card_index 📈 chart_with_upwards_trend 📉 chart_with_downwards_trend 📊 bar_chart 📋 clipboard 📌 pushpin 📍 round_pushpin 📎 paperclip 🖇 paperclips 📏 straight_ruler 📐 triangular_ruler ✂️ scissors 🗃️ card_file_box 🗄️ file_cabinet 🗑️ wastebasket ","date":"2019-10-01","objectID":"/emoji-support/:7:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"锁 图标 代码 图标 代码 🔒 lock 🔓 unlock 🔏 lock_with_ink_pen 🔐 closed_lock_with_key 🔑 key 🗝️ old_key ","date":"2019-10-01","objectID":"/emoji-support/:7:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"工具 图标 代码 图标 代码 🔨 hammer ⛏️ pick ⚒️ hammer_and_pick 🛠️ hammer_and_wrench 🗡 dagger ⚔️ crossed_swords 🔫 gun 🏹 bow_and_arrow 🛡️ shield 🔧 wrench 🔩 nut_and_bolt ⚙️ gear 🗜 clamp ⚖ balance_scale 🔗 link ⛓️ chains ","date":"2019-10-01","objectID":"/emoji-support/:7:14","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"科学 图标 代码 图标 代码 ⚗️ alembic 🔬 microscope 🔭 telescope 🛰️ satellite ","date":"2019-10-01","objectID":"/emoji-support/:7:15","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"医疗 图标 代码 图标 代码 💉 syringe 💊 pill ","date":"2019-10-01","objectID":"/emoji-support/:7:16","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"生活用品 图标 代码 图标 代码 🚪 door 🛏️ bed 🛋️ couch_and_lamp 🚽 toilet 🚿 shower 🛁 bathtub 🛒 shopping_cart ","date":"2019-10-01","objectID":"/emoji-support/:7:17","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它物品 图标 代码 图标 代码 🚬 smoking ⚰️ coffin ⚱️ funeral_urn 🗿 moyai ","date":"2019-10-01","objectID":"/emoji-support/:7:18","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"符号 ","date":"2019-10-01","objectID":"/emoji-support/:8:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"交通标识 图标 代码 图标 代码 🏧 atm 🚮 put_litter_in_its_place 🚰 potable_water ♿ wheelchair 🚹 mens 🚺 womens 🚻 restroom 🚼 baby_symbol 🚾 wc 🛂 passport_control 🛃 customs 🛄 baggage_claim 🛅 left_luggage ","date":"2019-10-01","objectID":"/emoji-support/:8:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"警告 图标 代码 图标 代码 ⚠️ warning 🚸 children_crossing ⛔ no_entry 🚫 no_entry_sign 🚳 no_bicycles 🚭 no_smoking 🚯 do_not_litter 🚱 🚱 🚷 no_pedestrians 📵 no_mobile_phones 🔞 underage ☢ radioactive ☣ biohazard ","date":"2019-10-01","objectID":"/emoji-support/:8:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"箭头 图标 代码 图标 代码 ⬆️ arrow_up ↗️ arrow_upper_right ➡️ arrow_right ↘️ arrow_lower_right ⬇️ arrow_down ↙️ arrow_lower_left ⬅️ arrow_left ↖️ arrow_upper_left ↕️ arrow_up_down ↔️ left_right_arrow ↩️ leftwards_arrow_with_hook ↪️ arrow_right_hook ⤴️ arrow_heading_up ⤵️ arrow_heading_down 🔃 arrows_clockwise 🔄 arrows_counterclockwise 🔙 back 🔚 end 🔛 on 🔜 soon 🔝 top ","date":"2019-10-01","objectID":"/emoji-support/:8:3","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"宗教 图标 代码 图标 代码 🛐 place_of_worship ⚛️ atom_symbol 🕉 om ✡️ star_of_david ☸️ wheel_of_dharma ☯️ yin_yang ✝️ latin_cross ☦️ orthodox_cross ☪️ star_and_crescent ☮️ peace_symbol 🕎 menorah 🔯 six_pointed_star ","date":"2019-10-01","objectID":"/emoji-support/:8:4","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"生肖 图标 代码 图标 代码 ♈ aries ♉ taurus ♊ gemini ♋ cancer ♌ leo ♍ virgo ♎ libra ♏ scorpius ♐ sagittarius ♑ capricorn ♒ aquarius ♓ pisces ⛎ ophiuchus ","date":"2019-10-01","objectID":"/emoji-support/:8:5","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"影像符号 图标 代码 图标 代码 🔀 twisted_rightwards_arrows 🔁 repeat 🔂 repeat_one ▶️ arrow_forward ⏩ fast_forward ⏭ next_track_button ⏯ play_or_pause_button ◀️ arrow_backward ⏪ rewind ⏮️ previous_track_button 🔼 arrow_up_small ⏫ arrow_double_up 🔽 arrow_down_small ⏬ arrow_double_down ⏸ pause_button ⏹ stop_button ⏺ record_button 🎦 cinema 🔅 low_brightness 🔆 high_brightness 📶 signal_strength 📳 vibration_mode 📴 mobile_phone_off ","date":"2019-10-01","objectID":"/emoji-support/:8:6","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"数学 图标 代码 图标 代码 ✖️ heavy_multiplication_x ➕ heavy_plus_sign ➖ heavy_minus_sign ➗ heavy_division_sign ","date":"2019-10-01","objectID":"/emoji-support/:8:7","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"标点符号 图标 代码 图标 代码 ‼️ bangbang ⁉️ interrobang ❓ question ❔ grey_question ❕ grey_exclamation ❗ exclamation heavy_exclamation_mark 〰️ wavy_dash ","date":"2019-10-01","objectID":"/emoji-support/:8:8","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"货币 图标 代码 图标 代码 💱 currency_exchange 💲 heavy_dollar_sign ","date":"2019-10-01","objectID":"/emoji-support/:8:9","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"按键符号 图标 代码 图标 代码 #️⃣ hash *️⃣ asterisk 0️⃣ zero 1️⃣ one 2️⃣ two 3️⃣ three 4️⃣ four 5️⃣ five 6️⃣ six 7️⃣ seven 8️⃣ eight 9️⃣ nine 🔟 keycap_ten ","date":"2019-10-01","objectID":"/emoji-support/:8:10","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"字母符号 图标 代码 图标 代码 🔠 capital_abcd 🔡 abcd 🔢 1234 🔣 symbols 🔤 abc 🅰️ a 🆎 ab 🅱️ b 🆑 cl 🆒 cool 🆓 free ℹ️ information_source 🆔 id ⓜ️ m 🆕 new 🆖 ng 🅾️ o2 🆗 ok 🅿️ parking 🆘 sos 🆙 up 🆚 vs 🈁 koko 🈂️ sa 🈷️ u6708 🈶 u6709 🈯 u6307 🉐 ideograph_advantage 🈹 u5272 🈚 u7121 🈲 u7981 🉑 accept 🈸 u7533 🈴 u5408 🈳 u7a7a ㊗️ congratulations ㊙️ secret 🈺 u55b6 🈵 u6e80 ","date":"2019-10-01","objectID":"/emoji-support/:8:11","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"几何符号 图标 代码 图标 代码 🔴 red_circle 🔵 large_blue_circle ⚫ black_circle ⚪ white_circle ⬛ black_large_square ⬜ white_large_square ◼️ black_medium_square ◻️ white_medium_square ◾ black_medium_small_square ◽ white_medium_small_square ▪️ black_small_square ▫️ white_small_square 🔶 large_orange_diamond 🔷 large_blue_diamond 🔸 small_orange_diamond 🔹 small_blue_diamond 🔺 small_red_triangle 🔻 small_red_triangle_down 💠 diamond_shape_with_a_dot_inside 🔘 radio_button 🔳 white_square_button 🔲 black_square_button ","date":"2019-10-01","objectID":"/emoji-support/:8:12","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"其它符合 图标 代码 图标 代码 ♻️ recycle ⚜️ fleur_de_lis 🔱 trident 📛 name_badge 🔰 beginner ⭕ o ✅ white_check_mark ☑️ ballot_box_with_check ✔️ heavy_check_mark ❌ x ❎ negative_squared_cross_mark ➰ curly_loop ➿ loop 〽️ part_alternation_mark ✳️ eight_spoked_asterisk ✴️ eight_pointed_black_star ❇️ sparkle ©️ copyright ®️ registered ™️ tm ","date":"2019-10-01","objectID":"/emoji-support/:8:13","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"旗帜 ","date":"2019-10-01","objectID":"/emoji-support/:9:0","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"常用旗帜 图标 代码 图标 代码 🏁 checkered_flag 🚩 triangular_flag_on_post 🎌 crossed_flags 🏴 black_flag 🏳 white_flag 🏳️‍🌈 rainbow_flag ","date":"2019-10-01","objectID":"/emoji-support/:9:1","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":["Markdown"],"content":"国家和地区旗帜 图标 代码 图标 代码 🇦🇩 andorra 🇦🇪 united_arab_emirates 🇦🇫 afghanistan 🇦🇬 antigua_barbuda 🇦🇮 anguilla 🇦🇱 albania 🇦🇲 armenia 🇦🇴 angola 🇦🇶 antarctica 🇦🇷 argentina 🇦🇸 american_samoa 🇦🇹 austria 🇦🇺 australia 🇦🇼 aruba 🇦🇽 aland_islands 🇦🇿 azerbaijan 🇧🇦 bosnia_herzegovina 🇧🇧 barbados 🇧🇩 bangladesh 🇧🇪 belgium 🇧🇫 burkina_faso 🇧🇬 bulgaria 🇧🇭 bahrain 🇧🇮 burundi 🇧🇯 benin 🇧🇱 st_barthelemy 🇧🇲 bermuda 🇧🇳 brunei 🇧🇴 bolivia 🇧🇶 caribbean_netherlands 🇧🇷 brazil 🇧🇸 bahamas 🇧🇹 bhutan 🇧🇼 botswana 🇧🇾 belarus 🇧🇿 belize 🇨🇦 canada 🇨🇨 cocos_islands 🇨🇩 congo_kinshasa 🇨🇫 central_african_republic 🇨🇬 congo_brazzaville 🇨🇭 switzerland 🇨🇮 cote_divoire 🇨🇰 cook_islands 🇨🇱 chile 🇨🇲 cameroon 🇨🇳 cn 🇨🇴 colombia 🇨🇷 costa_rica 🇨🇺 cuba 🇨🇻 cape_verde 🇨🇼 curacao 🇨🇽 christmas_island 🇨🇾 cyprus 🇨🇿 czech_republic 🇩🇪 de 🇩🇯 djibouti 🇩🇰 denmark 🇩🇲 dominica 🇩🇴 dominican_republic 🇩🇿 algeria 🇪🇨 ecuador 🇪🇪 estonia 🇪🇬 egypt 🇪🇭 western_sahara 🇪🇷 eritrea 🇪🇸 es 🇪🇹 ethiopia 🇪🇺 eu european_union 🇫🇮 finland 🇫🇯 fiji 🇫🇰 falkland_islands 🇫🇲 micronesia 🇫🇴 faroe_islands 🇫🇷 fr 🇬🇦 gabon 🇬🇧 gb uk 🇬🇩 grenada 🇬🇪 georgia 🇬🇫 french_guiana 🇬🇬 guernsey 🇬🇭 ghana 🇬🇮 gibraltar 🇬🇱 greenland 🇬🇲 gambia 🇬🇳 guinea 🇬🇵 guadeloupe 🇬🇶 equatorial_guinea 🇬🇷 greece 🇬🇸 south_georgia_south_sandwich_islands 🇬🇹 guatemala 🇬🇺 guam 🇬🇼 guinea_bissau 🇬🇾 guyana 🇭🇰 hong_kong 🇭🇳 honduras 🇭🇷 croatia 🇭🇹 haiti 🇭🇺 hungary 🇮🇨 canary_islands 🇮🇩 indonesia 🇮🇪 ireland 🇮🇱 israel 🇮🇲 isle_of_man 🇮🇳 india 🇮🇴 british_indian_ocean_territory 🇮🇶 iraq 🇮🇷 iran 🇮🇸 iceland 🇮🇹 it 🇯🇪 jersey 🇯🇲 jamaica 🇯🇴 jordan 🇯🇵 jp 🇰🇪 kenya 🇰🇬 kyrgyzstan 🇰🇭 cambodia 🇰🇮 kiribati 🇰🇲 comoros 🇰🇳 st_kitts_nevis 🇰🇵 north_korea 🇰🇷 kr 🇰🇼 kuwait 🇰🇾 cayman_islands 🇰🇿 kazakhstan 🇱🇦 laos 🇱🇧 lebanon 🇱🇨 st_lucia 🇱🇮 liechtenstein 🇱🇰 sri_lanka 🇱🇷 liberia 🇱🇸 lesotho 🇱🇹 lithuania 🇱🇺 luxembourg 🇱🇻 latvia 🇱🇾 libya 🇲🇦 morocco 🇲🇨 monaco 🇲🇩 moldova 🇲🇪 montenegro 🇲🇬 madagascar 🇲🇭 marshall_islands 🇲🇰 macedonia 🇲🇱 mali 🇲🇲 myanmar 🇲🇳 mongolia 🇲🇴 macau 🇲🇵 northern_mariana_islands 🇲🇶 martinique 🇲🇷 mauritania 🇲🇸 montserrat 🇲🇹 malta 🇲🇺 mauritius 🇲🇻 maldives 🇲🇼 malawi 🇲🇽 mexico 🇲🇾 malaysia 🇲🇿 mozambique 🇳🇦 namibia 🇳🇨 new_caledonia 🇳🇪 niger 🇳🇫 norfolk_island 🇳🇬 nigeria 🇳🇮 nicaragua 🇳🇱 netherlands 🇳🇴 norway 🇳🇵 nepal 🇳🇷 nauru 🇳🇺 niue 🇳🇿 new_zealand 🇴🇲 oman 🇵🇦 panama 🇵🇪 peru 🇵🇫 french_polynesia 🇵🇬 papua_new_guinea 🇵🇭 philippines 🇵🇰 pakistan 🇵🇱 poland 🇵🇲 st_pierre_miquelon 🇵🇳 pitcairn_islands 🇵🇷 puerto_rico 🇵🇸 palestinian_territories 🇵🇹 portugal 🇵🇼 palau 🇵🇾 paraguay 🇶🇦 qatar 🇷🇪 reunion 🇷🇴 romania 🇷🇸 serbia 🇷🇺 ru 🇷🇼 rwanda 🇸🇦 saudi_arabia 🇸🇧 solomon_islands 🇸🇨 seychelles 🇸🇩 sudan 🇸🇪 sweden 🇸🇬 singapore 🇸🇭 st_helena 🇸🇮 slovenia 🇸🇰 slovakia 🇸🇱 sierra_leone 🇸🇲 san_marino 🇸🇳 senegal 🇸🇴 somalia 🇸🇷 suriname 🇸🇸 south_sudan 🇸🇹 sao_tome_principe 🇸🇻 el_salvador 🇸🇽 sint_maarten 🇸🇾 syria 🇸🇿 swaziland 🇹🇨 turks_caicos_islands 🇹🇩 chad 🇹🇫 french_southern_territories 🇹🇬 togo 🇹🇭 thailand 🇹🇯 tajikistan 🇹🇰 tokelau 🇹🇱 timor_leste 🇹🇲 turkmenistan 🇹🇳 tunisia 🇹🇴 tonga 🇹🇷 tr 🇹🇹 trinidad_tobago 🇹🇻 tuvalu 🇹🇼 taiwan 🇹🇿 tanzania 🇺🇦 ukraine 🇺🇬 uganda 🇺🇸 us 🇺🇾 uruguay 🇺🇿 uzbekistan 🇻🇦 vatican_city 🇻🇨 st_vincent_grenadines 🇻🇪 venezuela 🇻🇬 british_virgin_islands 🇻🇮 us_virgin_islands 🇻🇳 vietnam 🇻🇺 vanuatu 🇼🇫 wallis_futuna 🇼🇸 samoa 🇽🇰 kosovo 🇾🇪 yemen 🇾🇹 mayotte 🇿🇦 south_africa 🇿🇲 zambia 🇿🇼 zimbabwe ","date":"2019-10-01","objectID":"/emoji-support/:9:2","tags":["emoji"],"title":"Emoji 支持","uri":"/emoji-support/"},{"categories":null,"content":" Cubes's Blog 这是一个分享IT技术的小站。 ","date":"0001-01-01","objectID":"/links/:0:0","tags":null,"title":"友链","uri":"/links/"}]